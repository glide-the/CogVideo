{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1ef6588-7b23-4b76-b3e2-a099beeb6908",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...:  40%|█████████████████▏                         | 2/5 [00:00<00:00,  3.47it/s]\n",
      "Loading checkpoint shards:   0%|                                                        | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Loading checkpoint shards:  50%|████████████████████████                        | 1/2 [00:00<00:00,  1.94it/s]\u001b[A\n",
      "Loading checkpoint shards: 100%|████████████████████████████████████████████████| 2/2 [00:00<00:00,  2.06it/s]\u001b[A\n",
      "Loading pipeline components...: 100%|███████████████████████████████████████████| 5/5 [00:01<00:00,  2.71it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'CogVideoXPipeline' object has no attribute 'load_lora_weights'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \n\u001b[1;32m      7\u001b[0m pipe \u001b[38;5;241m=\u001b[39m CogVideoXPipeline\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/mnt/ceph/develop/jiawei/model_checkpoint/CogVideoX-2b-base\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 9\u001b[0m \u001b[43mpipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_lora_weights\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/mnt/ceph/develop/jiawei/model_checkpoint/export_hf_lora_weights\u001b[39m\u001b[38;5;124m\"\u001b[39m,  weight_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpytorch_lora_weights.safetensors\u001b[39m\u001b[38;5;124m\"\u001b[39m, adapter_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m pipe\u001b[38;5;241m.\u001b[39mfuse_lora(lora_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m)\n\u001b[1;32m     13\u001b[0m pipe\u001b[38;5;241m.\u001b[39mscheduler \u001b[38;5;241m=\u001b[39m CogVideoXDPMScheduler\u001b[38;5;241m.\u001b[39mfrom_config(pipe\u001b[38;5;241m.\u001b[39mscheduler\u001b[38;5;241m.\u001b[39mconfig, timestep_spacing\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrailing\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/diffusers/configuration_utils.py:143\u001b[0m, in \u001b[0;36mConfigMixin.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    140\u001b[0m     deprecate(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdirect config name access\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1.0.0\u001b[39m\u001b[38;5;124m\"\u001b[39m, deprecation_message, standard_warn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_dict[name]\n\u001b[0;32m--> 143\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'CogVideoXPipeline' object has no attribute 'load_lora_weights'"
     ]
    }
   ],
   "source": [
    "\n",
    "from diffusers import CogVideoXPipeline, CogVideoXDDIMScheduler, CogVideoXDPMScheduler\n",
    "import os\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" \n",
    "\n",
    "pipe = CogVideoXPipeline.from_pretrained(\"/mnt/ceph/develop/jiawei/model_checkpoint/CogVideoX-2b-base\", torch_dtype=torch.bfloat16).to(device)\n",
    " \n",
    "pipe.load_lora_weights(\"/mnt/ceph/develop/jiawei/model_checkpoint/export_hf_lora_weights\",  weight_name=\"pytorch_lora_weights.safetensors\", adapter_name=\"test_1\")\n",
    "pipe.fuse_lora(lora_scale=0.7)\n",
    "\n",
    "\n",
    "pipe.scheduler = CogVideoXDPMScheduler.from_config(pipe.scheduler.config, timestep_spacing=\"trailing\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6da883de-9474-481d-8fbc-d5c24f074919",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math \n",
    "import random \n",
    "import time\n",
    "\n",
    "\n",
    "os.makedirs(\"./output\", exist_ok=True)\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8112bb8-6a92-4ee1-a987-2fc56f0ff7d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 50/50 [02:11<00:00,  2.64s/it]\n"
     ]
    }
   ],
   "source": [
    "prompt=\"\"\"In the heart of a bustling city, a young woman with long, flowing brown hair and a radiant smile stands out. She's donned in a cozy white beanie adorned with playful animal ears, adding a touch of whimsy to her appearance. Her eyes sparkle with joy as she looks directly into the camera, her expression inviting and warm. The background is a blur of activity, with indistinct figures moving about, suggesting a lively public space. The lighting is soft and diffused, casting a gentle glow on her face and highlighting her features. The overall mood is cheerful and vibrant, capturing a moment of happiness in the midst of urban life.\n",
    "\"\"\"\n",
    "latents = pipe(\n",
    "    prompt=prompt,\n",
    "    num_videos_per_prompt=1,\n",
    "    num_inference_steps=50,\n",
    "    num_frames=49,\n",
    "    use_dynamic_cfg=True,\n",
    "    output_type=\"pt\",\n",
    "    guidance_scale=3.0,\n",
    "    generator=torch.Generator(device=\"cpu\").manual_seed(42),\n",
    ").frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18d35dae-9ac4-423c-b7cb-1f3791302b2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./output/20240911_114547.mp4'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from diffusers.utils import export_to_video\n",
    "from diffusers.image_processor import VaeImageProcessor\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "batch_size = latents.shape[0]\n",
    "batch_video_frames = []\n",
    "for batch_idx in range(batch_size):\n",
    "    pt_image = latents[batch_idx]\n",
    "    pt_image = torch.stack([pt_image[i] for i in range(pt_image.shape[0])])\n",
    "\n",
    "    image_np = VaeImageProcessor.pt_to_numpy(pt_image)\n",
    "    image_pil = VaeImageProcessor.numpy_to_pil(image_np)\n",
    "    batch_video_frames.append(image_pil)\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "video_path = f\"./output/{timestamp}.mp4\"\n",
    "os.makedirs(os.path.dirname(video_path), exist_ok=True)\n",
    "tensor = batch_video_frames[0]\n",
    "fps=math.ceil((len(batch_video_frames[0]) - 1) / 6)\n",
    "\n",
    "export_to_video(tensor, video_path, fps=fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68ef55c-5aa4-4b4e-a7b1-f1b96eee0687",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
