RUN on nm04-a800-node083, CUDA_VISIBLE_DEVICES=0
torchrun --standalone --nproc_per_node=1 train_video.py --base configs/cogvideox_2b_lora.yaml configs/sft.yaml --seed 42
[2024-09-09 13:15:21,041] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
[93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, input, weight, bias=None):
/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/kornia/feature/lightglue.py:44: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)
no module 'xformers'. Processing without...
no module 'xformers'. Processing without...
[2024-09-09 13:15:38,912] [INFO] using world size: 1
[2024-09-09 13:15:38,912] [INFO] Will override arguments with manually specified deepspeed_config!
[2024-09-09 13:15:38,918] [INFO] [RANK 0] > initializing model parallel with size 1
[2024-09-09 13:15:38,919] [INFO] [comm.py:637:init_distributed] cdb=None
Namespace(base=['configs/cogvideox_2b_lora.yaml', 'configs/sft.yaml'], model_parallel_size=1, force_pretrain=False, device=0, debug=False, log_image=True, output_dir='samples', input_dir=None, input_type='cli', input_file='input.txt', final_size=2048, sdedit=False, grid_num_rows=1, force_inference=False, lcm_steps=None, sampling_num_frames=32, sampling_fps=8, only_save_latents=False, only_log_video_latents=False, latent_channels=32, image2video=False, experiment_name='lora-disney', train_iters=1000, batch_size=2, lr=0.0005, mode='finetune', seed=42, zero_stage=0, checkpoint_activations=True, checkpoint_num_layers=1, checkpoint_skip_layers=0, fp16=True, bf16=False, gradient_accumulation_steps=1, profiling=-1, epochs=None, log_interval=20, summary_dir='', save_args=False, lr_decay_iters=None, lr_decay_style='linear', lr_decay_ratio=0.1, warmup=0.01, weight_decay=5e-06, save='/mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora', load='/mnt/ceph/develop/jiawei/model_checkpoint/CogVideoX-2b-sat/transformer', force_train=True, save_interval=500, no_save_rng=False, no_load_rng=True, resume_dataloader=False, distributed_backend='nccl', local_rank=0, exit_interval=None, wandb=False, wandb_project_name='default_project', eval_batch_size=1, eval_iters=1, eval_interval=100, strict_eval=False, train_data=['/mnt/ceph/develop/jiawei/lora_dataset/cogvideo_lora_dataset'], train_data_weights=None, iterable_dataset=False, iterable_dataset_eval='', batch_from_same_dataset=False, valid_data=['/mnt/ceph/develop/jiawei/lora_dataset/cogvideo_lora_dataset'], test_data=None, split='1,0,0', num_workers=8, block_size=10000, prefetch_factor=4, deepspeed=True, deepspeed_config={'train_micro_batch_size_per_gpu': 2, 'gradient_accumulation_steps': 1, 'steps_per_print': 50, 'gradient_clipping': 0.1, 'zero_optimization': {'stage': 2, 'cpu_offload': False, 'contiguous_gradients': False, 'overlap_comm': True, 'reduce_scatter': True, 'reduce_bucket_size': 1000000000, 'allgather_bucket_size': 1000000000, 'load_from_fp32_weights': False}, 'zero_allow_untested_optimizer': True, 'bf16': {'enabled': False}, 'fp16': {'enabled': True}, 'loss_scale': 0, 'loss_scale_window': 400, 'hysteresis': 2, 'min_loss_scale': 1, 'optimizer': {'type': 'sat.ops.FusedEmaAdam', 'params': {'lr': 0.0005, 'betas': [0.9, 0.95], 'eps': 1e-08, 'weight_decay': 5e-06}}, 'activation_checkpointing': {'partition_activations': False, 'contiguous_memory_optimization': False}, 'wall_clock_breakdown': False}, deepscale=False, deepscale_config=None, model_config={'scale_factor': 1.15258426, 'disable_first_stage_autocast': True, 'not_trainable_prefixes': ['all'], 'log_keys': ['txt'], 'denoiser_config': {'target': 'sgm.modules.diffusionmodules.denoiser.DiscreteDenoiser', 'params': {'num_idx': 1000, 'quantize_c_noise': False, 'weighting_config': {'target': 'sgm.modules.diffusionmodules.denoiser_weighting.EpsWeighting'}, 'scaling_config': {'target': 'sgm.modules.diffusionmodules.denoiser_scaling.VideoScaling'}, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 3.0}}}}, 'network_config': {'target': 'dit_video_concat.DiffusionTransformer', 'params': {'time_embed_dim': 512, 'elementwise_affine': True, 'num_frames': 49, 'time_compressed_rate': 4, 'latent_width': 90, 'latent_height': 60, 'num_layers': 30, 'patch_size': 2, 'in_channels': 16, 'out_channels': 16, 'hidden_size': 1920, 'adm_in_channels': 256, 'num_attention_heads': 30, 'transformer_args': {'checkpoint_activations': True, 'vocab_size': 1, 'max_sequence_length': 64, 'layernorm_order': 'pre', 'skip_init': False, 'model_parallel_size': 1, 'is_decoder': False}, 'modules': {'pos_embed_config': {'target': 'dit_video_concat.Basic3DPositionEmbeddingMixin', 'params': {'text_length': 226, 'height_interpolation': 1.875, 'width_interpolation': 1.875}}, 'lora_config': {'target': 'sat.model.finetune.lora2.LoraMixin', 'params': {'r': 128}}, 'patch_embed_config': {'target': 'dit_video_concat.ImagePatchEmbeddingMixin', 'params': {'text_hidden_size': 4096}}, 'adaln_layer_config': {'target': 'dit_video_concat.AdaLNMixin', 'params': {'qk_ln': True}}, 'final_layer_config': {'target': 'dit_video_concat.FinalLayerMixin'}}}}, 'conditioner_config': {'target': 'sgm.modules.GeneralConditioner', 'params': {'emb_models': [{'is_trainable': False, 'input_key': 'txt', 'ucg_rate': 0.1, 'target': 'sgm.modules.encoders.modules.FrozenT5Embedder', 'params': {'model_dir': '/mnt/ceph/develop/jiawei/model_checkpoint/t5-v1_1-xxl', 'max_length': 226}}]}}, 'first_stage_config': {'target': 'vae_modules.autoencoder.VideoAutoencoderInferenceWrapper', 'params': {'cp_size': 1, 'ckpt_path': '/mnt/ceph/develop/jiawei/model_checkpoint/CogVideoX-2b-sat/vae/3d-vae.pt', 'ignore_keys': ['loss'], 'loss_config': {'target': 'torch.nn.Identity'}, 'regularizer_config': {'target': 'vae_modules.regularizers.DiagonalGaussianRegularizer'}, 'encoder_config': {'target': 'vae_modules.cp_enc_dec.ContextParallelEncoder3D', 'params': {'double_z': True, 'z_channels': 16, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 2, 4], 'attn_resolutions': [], 'num_res_blocks': 3, 'dropout': 0.0, 'gather_norm': True}}, 'decoder_config': {'target': 'vae_modules.cp_enc_dec.ContextParallelDecoder3D', 'params': {'double_z': True, 'z_channels': 16, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 2, 4], 'attn_resolutions': [], 'num_res_blocks': 3, 'dropout': 0.0, 'gather_norm': False}}}}, 'loss_fn_config': {'target': 'sgm.modules.diffusionmodules.loss.VideoDiffusionLoss', 'params': {'offset_noise_level': 0, 'sigma_sampler_config': {'target': 'sgm.modules.diffusionmodules.sigma_sampling.DiscreteSampling', 'params': {'uniform_sampling': True, 'num_idx': 1000, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 3.0}}}}}}, 'sampler_config': {'target': 'sgm.modules.diffusionmodules.sampling.VPSDEDPMPP2MSampler', 'params': {'num_steps': 50, 'verbose': True, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 3.0}}, 'guider_config': {'target': 'sgm.modules.diffusionmodules.guiders.DynamicCFG', 'params': {'scale': 6, 'exp': 5, 'num_steps': 50}}}}}, data_config={'target': 'data_video.SFTDataset', 'params': {'video_size': [480, 720], 'fps': 8, 'max_num_frames': 49, 'skip_frms_num': 3.0}}, cuda=True, rank=0, world_size=1, deepspeed_activation_checkpointing=True, master_ip='nm04-a800-node083', master_port='26645', log_config=[{'model': {'scale_factor': 1.15258426, 'disable_first_stage_autocast': True, 'not_trainable_prefixes': ['all'], 'log_keys': ['txt'], 'denoiser_config': {'target': 'sgm.modules.diffusionmodules.denoiser.DiscreteDenoiser', 'params': {'num_idx': 1000, 'quantize_c_noise': False, 'weighting_config': {'target': 'sgm.modules.diffusionmodules.denoiser_weighting.EpsWeighting'}, 'scaling_config': {'target': 'sgm.modules.diffusionmodules.denoiser_scaling.VideoScaling'}, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 3.0}}}}, 'network_config': {'target': 'dit_video_concat.DiffusionTransformer', 'params': {'time_embed_dim': 512, 'elementwise_affine': True, 'num_frames': 49, 'time_compressed_rate': 4, 'latent_width': 90, 'latent_height': 60, 'num_layers': 30, 'patch_size': 2, 'in_channels': 16, 'out_channels': 16, 'hidden_size': 1920, 'adm_in_channels': 256, 'num_attention_heads': 30, 'transformer_args': {'checkpoint_activations': True, 'vocab_size': 1, 'max_sequence_length': 64, 'layernorm_order': 'pre', 'skip_init': False, 'model_parallel_size': 1, 'is_decoder': False}, 'modules': {'pos_embed_config': {'target': 'dit_video_concat.Basic3DPositionEmbeddingMixin', 'params': {'text_length': 226, 'height_interpolation': 1.875, 'width_interpolation': 1.875}}, 'lora_config': {'target': 'sat.model.finetune.lora2.LoraMixin', 'params': {'r': 128}}, 'patch_embed_config': {'target': 'dit_video_concat.ImagePatchEmbeddingMixin', 'params': {'text_hidden_size': 4096}}, 'adaln_layer_config': {'target': 'dit_video_concat.AdaLNMixin', 'params': {'qk_ln': True}}, 'final_layer_config': {'target': 'dit_video_concat.FinalLayerMixin'}}}}, 'conditioner_config': {'target': 'sgm.modules.GeneralConditioner', 'params': {'emb_models': [{'is_trainable': False, 'input_key': 'txt', 'ucg_rate': 0.1, 'target': 'sgm.modules.encoders.modules.FrozenT5Embedder', 'params': {'model_dir': '/mnt/ceph/develop/jiawei/model_checkpoint/t5-v1_1-xxl', 'max_length': 226}}]}}, 'first_stage_config': {'target': 'vae_modules.autoencoder.VideoAutoencoderInferenceWrapper', 'params': {'cp_size': 1, 'ckpt_path': '/mnt/ceph/develop/jiawei/model_checkpoint/CogVideoX-2b-sat/vae/3d-vae.pt', 'ignore_keys': ['loss'], 'loss_config': {'target': 'torch.nn.Identity'}, 'regularizer_config': {'target': 'vae_modules.regularizers.DiagonalGaussianRegularizer'}, 'encoder_config': {'target': 'vae_modules.cp_enc_dec.ContextParallelEncoder3D', 'params': {'double_z': True, 'z_channels': 16, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 2, 4], 'attn_resolutions': [], 'num_res_blocks': 3, 'dropout': 0.0, 'gather_norm': True}}, 'decoder_config': {'target': 'vae_modules.cp_enc_dec.ContextParallelDecoder3D', 'params': {'double_z': True, 'z_channels': 16, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 2, 4], 'attn_resolutions': [], 'num_res_blocks': 3, 'dropout': 0.0, 'gather_norm': False}}}}, 'loss_fn_config': {'target': 'sgm.modules.diffusionmodules.loss.VideoDiffusionLoss', 'params': {'offset_noise_level': 0, 'sigma_sampler_config': {'target': 'sgm.modules.diffusionmodules.sigma_sampling.DiscreteSampling', 'params': {'uniform_sampling': True, 'num_idx': 1000, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 3.0}}}}}}, 'sampler_config': {'target': 'sgm.modules.diffusionmodules.sampling.VPSDEDPMPP2MSampler', 'params': {'num_steps': 50, 'verbose': True, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 3.0}}, 'guider_config': {'target': 'sgm.modules.diffusionmodules.guiders.DynamicCFG', 'params': {'scale': 6, 'exp': 5, 'num_steps': 50}}}}}}, {'args': {'checkpoint_activations': True, 'model_parallel_size': 1, 'experiment_name': 'lora-disney', 'mode': 'finetune', 'load': '/mnt/ceph/develop/jiawei/model_checkpoint/CogVideoX-2b-sat/transformer', 'no_load_rng': True, 'train_iters': 1000, 'eval_iters': 1, 'eval_interval': 100, 'eval_batch_size': 1, 'save': '/mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora', 'save_interval': 500, 'log_interval': 20, 'train_data': ['/mnt/ceph/develop/jiawei/lora_dataset/cogvideo_lora_dataset'], 'valid_data': ['/mnt/ceph/develop/jiawei/lora_dataset/cogvideo_lora_dataset'], 'split': '1,0,0', 'num_workers': 8, 'force_train': True, 'only_log_video_latents': False}, 'data': {'target': 'data_video.SFTDataset', 'params': {'video_size': [480, 720], 'fps': 8, 'max_num_frames': 49, 'skip_frms_num': 3.0}}, 'deepspeed': {'train_micro_batch_size_per_gpu': 2, 'gradient_accumulation_steps': 1, 'steps_per_print': 50, 'gradient_clipping': 0.1, 'zero_optimization': {'stage': 2, 'cpu_offload': False, 'contiguous_gradients': False, 'overlap_comm': True, 'reduce_scatter': True, 'reduce_bucket_size': 1000000000, 'allgather_bucket_size': 1000000000, 'load_from_fp32_weights': False}, 'zero_allow_untested_optimizer': True, 'bf16': {'enabled': False}, 'fp16': {'enabled': True}, 'loss_scale': 0, 'loss_scale_window': 400, 'hysteresis': 2, 'min_loss_scale': 1, 'optimizer': {'type': 'sat.ops.FusedEmaAdam', 'params': {'lr': '5E-4', 'betas': [0.9, 0.95], 'eps': '1e-8', 'weight_decay': '5E-6'}}, 'activation_checkpointing': {'partition_activations': False, 'contiguous_memory_optimization': False}, 'wall_clock_breakdown': False}}])
[2024-09-09 13:15:40,067] [INFO] [RANK 0] building SATVideoDiffusionEngine model ...
[2024-09-09 13:15:50,411] [WARNING] [RANK 0] Failed to load bitsandbytes:No module named 'bitsandbytes'
[2024-09-09 13:15:50,412] [INFO] [RANK 0] replacing layer 0 attention with lora
[2024-09-09 13:15:50,438] [INFO] [RANK 0] replacing layer 1 attention with lora
[2024-09-09 13:15:50,460] [INFO] [RANK 0] replacing layer 2 attention with lora
[2024-09-09 13:15:50,483] [INFO] [RANK 0] replacing layer 3 attention with lora
[2024-09-09 13:15:50,507] [INFO] [RANK 0] replacing layer 4 attention with lora
[2024-09-09 13:15:50,530] [INFO] [RANK 0] replacing layer 5 attention with lora
[2024-09-09 13:15:50,552] [INFO] [RANK 0] replacing layer 6 attention with lora
[2024-09-09 13:15:50,576] [INFO] [RANK 0] replacing layer 7 attention with lora
[2024-09-09 13:15:50,599] [INFO] [RANK 0] replacing layer 8 attention with lora
[2024-09-09 13:15:50,621] [INFO] [RANK 0] replacing layer 9 attention with lora
[2024-09-09 13:15:50,643] [INFO] [RANK 0] replacing layer 10 attention with lora
[2024-09-09 13:15:50,666] [INFO] [RANK 0] replacing layer 11 attention with lora
[2024-09-09 13:15:50,689] [INFO] [RANK 0] replacing layer 12 attention with lora
[2024-09-09 13:15:50,711] [INFO] [RANK 0] replacing layer 13 attention with lora
[2024-09-09 13:15:50,733] [INFO] [RANK 0] replacing layer 14 attention with lora
[2024-09-09 13:15:50,756] [INFO] [RANK 0] replacing layer 15 attention with lora
[2024-09-09 13:15:50,778] [INFO] [RANK 0] replacing layer 16 attention with lora
[2024-09-09 13:15:50,800] [INFO] [RANK 0] replacing layer 17 attention with lora
[2024-09-09 13:15:50,823] [INFO] [RANK 0] replacing layer 18 attention with lora
[2024-09-09 13:15:50,845] [INFO] [RANK 0] replacing layer 19 attention with lora
[2024-09-09 13:15:50,867] [INFO] [RANK 0] replacing layer 20 attention with lora
[2024-09-09 13:15:50,889] [INFO] [RANK 0] replacing layer 21 attention with lora
[2024-09-09 13:15:50,911] [INFO] [RANK 0] replacing layer 22 attention with lora
[2024-09-09 13:15:50,933] [INFO] [RANK 0] replacing layer 23 attention with lora
[2024-09-09 13:15:50,956] [INFO] [RANK 0] replacing layer 24 attention with lora
[2024-09-09 13:15:50,979] [INFO] [RANK 0] replacing layer 25 attention with lora
[2024-09-09 13:15:51,000] [INFO] [RANK 0] replacing layer 26 attention with lora
[2024-09-09 13:15:51,020] [INFO] [RANK 0] replacing layer 27 attention with lora
[2024-09-09 13:15:51,040] [INFO] [RANK 0] replacing layer 28 attention with lora
[2024-09-09 13:15:51,056] [INFO] [RANK 0] replacing layer 29 attention with lora
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  1.43it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.49it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.48it/s]
Initialized embedder #0: FrozenT5Embedder with 4762310656 params. Trainable: False
Working with z of shape (1, 16, 32, 32) = 16384 dimensions.
/mnt/ceph/develop/jiawei/CogVideo/sat/vae_modules/autoencoder.py:565: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  sd = torch.load(path, map_location="cpu")["state_dict"]
Deleting key loss.logvar from state_dict.
Deleting key loss.perceptual_loss.scaling_layer.shift from state_dict.
Deleting key loss.perceptual_loss.scaling_layer.scale from state_dict.
Deleting key loss.perceptual_loss.net.slice1.0.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice1.0.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice1.2.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice1.2.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice2.5.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice2.5.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice2.7.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice2.7.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice3.10.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice3.10.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice3.12.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice3.12.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice3.14.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice3.14.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice4.17.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice4.17.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice4.19.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice4.19.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice4.21.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice4.21.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice5.24.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice5.24.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice5.26.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice5.26.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice5.28.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice5.28.bias from state_dict.
Deleting key loss.perceptual_loss.lin0.model.1.weight from state_dict.
Deleting key loss.perceptual_loss.lin1.model.1.weight from state_dict.
Deleting key loss.perceptual_loss.lin2.model.1.weight from state_dict.
Deleting key loss.perceptual_loss.lin3.model.1.weight from state_dict.
Deleting key loss.perceptual_loss.lin4.model.1.weight from state_dict.
Deleting key loss.discriminator.blocks.0.downsample_res.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.0.downsample_res.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.0.net.0.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.0.net.0.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.0.net.2.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.0.net.2.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.0.downsample.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.0.downsample.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.1.downsample_res.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.1.downsample_res.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.1.net.0.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.1.net.0.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.1.net.2.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.1.net.2.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.1.downsample.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.1.downsample.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.2.downsample_res.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.2.downsample_res.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.2.net.0.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.2.net.0.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.2.net.2.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.2.net.2.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.2.downsample.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.2.downsample.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.3.downsample_res.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.3.downsample_res.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.3.net.0.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.3.net.0.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.3.net.2.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.3.net.2.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.3.downsample.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.3.downsample.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.4.0.conv_res.weight from state_dict.
Deleting key loss.discriminator.blocks.4.0.conv_res.bias from state_dict.
Deleting key loss.discriminator.blocks.4.0.net.0.weight from state_dict.
Deleting key loss.discriminator.blocks.4.0.net.0.bias from state_dict.
Deleting key loss.discriminator.blocks.4.0.net.2.weight from state_dict.
Deleting key loss.discriminator.blocks.4.0.net.2.bias from state_dict.
Deleting key loss.discriminator.blocks.4.0.downsample.1.weight from state_dict.
Deleting key loss.discriminator.blocks.4.0.downsample.1.bias from state_dict.
Deleting key loss.discriminator.blocks.4.1.0.fn.norm.gamma from state_dict.
Deleting key loss.discriminator.blocks.4.1.0.fn.attn.to_q.0.weight from state_dict.
Deleting key loss.discriminator.blocks.4.1.0.fn.attn.to_kv.0.weight from state_dict.
Deleting key loss.discriminator.blocks.4.1.0.fn.attn.to_out.0.weight from state_dict.
Deleting key loss.discriminator.blocks.4.1.1.fn.norm.gamma from state_dict.
Deleting key loss.discriminator.blocks.4.1.1.fn.net.0.weight from state_dict.
Deleting key loss.discriminator.blocks.4.1.1.fn.net.0.bias from state_dict.
Deleting key loss.discriminator.blocks.4.1.1.fn.net.2.weight from state_dict.
Deleting key loss.discriminator.blocks.4.1.1.fn.net.2.bias from state_dict.
Deleting key loss.discriminator.blocks.5.0.conv_res.weight from state_dict.
Deleting key loss.discriminator.blocks.5.0.conv_res.bias from state_dict.
Deleting key loss.discriminator.blocks.5.0.net.0.weight from state_dict.
Deleting key loss.discriminator.blocks.5.0.net.0.bias from state_dict.
Deleting key loss.discriminator.blocks.5.0.net.2.weight from state_dict.
Deleting key loss.discriminator.blocks.5.0.net.2.bias from state_dict.
Deleting key loss.discriminator.blocks.5.0.downsample.1.weight from state_dict.
Deleting key loss.discriminator.blocks.5.0.downsample.1.bias from state_dict.
Deleting key loss.discriminator.blocks.5.1.0.fn.norm.gamma from state_dict.
Deleting key loss.discriminator.blocks.5.1.0.fn.attn.to_q.0.weight from state_dict.
Deleting key loss.discriminator.blocks.5.1.0.fn.attn.to_kv.0.weight from state_dict.
Deleting key loss.discriminator.blocks.5.1.0.fn.attn.to_out.0.weight from state_dict.
Deleting key loss.discriminator.blocks.5.1.1.fn.norm.gamma from state_dict.
Deleting key loss.discriminator.blocks.5.1.1.fn.net.0.weight from state_dict.
Deleting key loss.discriminator.blocks.5.1.1.fn.net.0.bias from state_dict.
Deleting key loss.discriminator.blocks.5.1.1.fn.net.2.weight from state_dict.
Deleting key loss.discriminator.blocks.5.1.1.fn.net.2.bias from state_dict.
Deleting key loss.discriminator.blocks.6.0.conv_res.weight from state_dict.
Deleting key loss.discriminator.blocks.6.0.conv_res.bias from state_dict.
Deleting key loss.discriminator.blocks.6.0.net.0.weight from state_dict.
Deleting key loss.discriminator.blocks.6.0.net.0.bias from state_dict.
Deleting key loss.discriminator.blocks.6.0.net.2.weight from state_dict.
Deleting key loss.discriminator.blocks.6.0.net.2.bias from state_dict.
Deleting key loss.discriminator.blocks.6.1.0.fn.norm.gamma from state_dict.
Deleting key loss.discriminator.blocks.6.1.0.fn.attn.to_q.0.weight from state_dict.
Deleting key loss.discriminator.blocks.6.1.0.fn.attn.to_kv.0.weight from state_dict.
Deleting key loss.discriminator.blocks.6.1.0.fn.attn.to_out.0.weight from state_dict.
Deleting key loss.discriminator.blocks.6.1.1.fn.norm.gamma from state_dict.
Deleting key loss.discriminator.blocks.6.1.1.fn.net.0.weight from state_dict.
Deleting key loss.discriminator.blocks.6.1.1.fn.net.0.bias from state_dict.
Deleting key loss.discriminator.blocks.6.1.1.fn.net.2.weight from state_dict.
Deleting key loss.discriminator.blocks.6.1.1.fn.net.2.bias from state_dict.
Deleting key loss.discriminator.to_logits.0.weight from state_dict.
Deleting key loss.discriminator.to_logits.0.bias from state_dict.
Deleting key loss.discriminator.to_logits.3.weight from state_dict.
Deleting key loss.discriminator.to_logits.3.bias from state_dict.
Missing keys:  []
Unexpected keys:  []
Restored from /mnt/ceph/develop/jiawei/model_checkpoint/CogVideoX-2b-sat/vae/3d-vae.pt
[2024-09-09 13:15:55,330] [INFO] [RANK 0]  > number of parameters on model parallel rank 0: 6764790755
[2024-09-09 13:16:02,432] [INFO] [RANK 0] global rank 0 is loading checkpoint /mnt/ceph/develop/jiawei/model_checkpoint/CogVideoX-2b-sat/transformer/1000/mp_rank_00_model_states.pt
/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/sat/training/model_io.py:286: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  sd = torch.load(checkpoint_name, map_location='cpu')
[2024-09-09 13:16:04,604] [INFO] [RANK 0] > successfully loaded /mnt/ceph/develop/jiawei/model_checkpoint/CogVideoX-2b-sat/transformer/1000/mp_rank_00_model_states.pt
[2024-09-09 13:16:05,021] [INFO] [RANK 0] ***** Total trainable parameters: 58982400 *****
[2024-09-09 13:16:05,021] [INFO] [RANK 0] [<class 'sat.ops.layernorm.LayerNorm'>, <class 'torch.nn.modules.normalization.LayerNorm'>, <class 'sat.ops.layernorm.RMSNorm'>] is set to no_weight_decay
[2024-09-09 13:16:05,027] [INFO] [RANK 0] Syncing initialized parameters...
[2024-09-09 13:16:05,090] [INFO] [RANK 0] Finished syncing initialized parameters.
[2024-09-09 13:16:05,090] [INFO] [RANK 0] Using optimizer sat.ops.FusedEmaAdam from sat.
[2024-09-09 13:16:05,094] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.4, git-hash=unknown, git-branch=unknown
[2024-09-09 13:16:05,094] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
[2024-09-09 13:16:05,181] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /root/.cache/torch_extensions/py310_cu121/fused_ema_adam/build.ninja...
/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1965: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module fused_ema_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_ema_adam...
Time to load fused_ema_adam op: 3.3338820934295654 seconds
[2024-09-09 13:16:08,545] [INFO] [logging.py:96:log_dist] [Rank 0] Using client callable to create basic optimizer
[2024-09-09 13:16:08,549] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-09-09 13:16:08,575] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedEmaAdam
[2024-09-09 13:16:08,575] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedEmaAdam type=<class 'sat.ops.fused_ema_adam.FusedEmaAdam'>
[2024-09-09 13:16:08,575] [WARNING] [engine.py:1179:_do_optimizer_sanity_check] **** You are using ZeRO with an untested optimizer, proceed with caution *****
[2024-09-09 13:16:08,575] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2024-09-09 13:16:08,577] [INFO] [stage_1_and_2.py:148:__init__] Reduce bucket size 1000000000
[2024-09-09 13:16:08,577] [INFO] [stage_1_and_2.py:149:__init__] Allgather bucket size 1000000000
[2024-09-09 13:16:08,577] [INFO] [stage_1_and_2.py:150:__init__] CPU Offload: False
[2024-09-09 13:16:08,577] [INFO] [stage_1_and_2.py:151:__init__] Round robin gradient partitioning: False
[2024-09-09 13:16:08,843] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2024-09-09 13:16:08,843] [INFO] [utils.py:782:see_memory_usage] MA 12.86 GB         Max_MA 12.97 GB         CA 13.23 GB         Max_CA 13 GB 
[2024-09-09 13:16:08,845] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 47.83 GB, percent = 2.4%
[2024-09-09 13:16:08,985] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2024-09-09 13:16:08,986] [INFO] [utils.py:782:see_memory_usage] MA 12.86 GB         Max_MA 13.08 GB         CA 13.45 GB         Max_CA 13 GB 
[2024-09-09 13:16:08,989] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 47.83 GB, percent = 2.4%
[2024-09-09 13:16:08,991] [INFO] [stage_1_and_2.py:543:__init__] optimizer state initialized
[2024-09-09 13:16:09,126] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2024-09-09 13:16:09,127] [INFO] [utils.py:782:see_memory_usage] MA 12.86 GB         Max_MA 12.86 GB         CA 13.45 GB         Max_CA 13 GB 
[2024-09-09 13:16:09,130] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 47.83 GB, percent = 2.4%
[2024-09-09 13:16:09,134] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer
[2024-09-09 13:16:09,134] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2024-09-09 13:16:09,134] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2024-09-09 13:16:09,134] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[1.0], mom=[[0.9, 0.95]]
[2024-09-09 13:16:09,137] [INFO] [config.py:997:print] DeepSpeedEngine configuration:
[2024-09-09 13:16:09,141] [INFO] [config.py:1001:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-09-09 13:16:09,143] [INFO] [config.py:1001:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-09-09 13:16:09,143] [INFO] [config.py:1001:print]   amp_enabled .................. False
[2024-09-09 13:16:09,144] [INFO] [config.py:1001:print]   amp_params ................... False
[2024-09-09 13:16:09,144] [INFO] [config.py:1001:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-09-09 13:16:09,147] [INFO] [config.py:1001:print]   bfloat16_enabled ............. False
[2024-09-09 13:16:09,147] [INFO] [config.py:1001:print]   bfloat16_immediate_grad_update  False
[2024-09-09 13:16:09,147] [INFO] [config.py:1001:print]   checkpoint_parallel_write_pipeline  False
[2024-09-09 13:16:09,147] [INFO] [config.py:1001:print]   checkpoint_tag_validation_enabled  True
[2024-09-09 13:16:09,147] [INFO] [config.py:1001:print]   checkpoint_tag_validation_fail  False
[2024-09-09 13:16:09,147] [INFO] [config.py:1001:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fbd74163ac0>
[2024-09-09 13:16:09,147] [INFO] [config.py:1001:print]   communication_data_type ...... None
[2024-09-09 13:16:09,148] [INFO] [config.py:1001:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-09-09 13:16:09,150] [INFO] [config.py:1001:print]   curriculum_enabled_legacy .... False
[2024-09-09 13:16:09,150] [INFO] [config.py:1001:print]   curriculum_params_legacy ..... False
[2024-09-09 13:16:09,150] [INFO] [config.py:1001:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-09-09 13:16:09,150] [INFO] [config.py:1001:print]   data_efficiency_enabled ...... False
[2024-09-09 13:16:09,150] [INFO] [config.py:1001:print]   dataloader_drop_last ......... False
[2024-09-09 13:16:09,152] [INFO] [config.py:1001:print]   disable_allgather ............ False
[2024-09-09 13:16:09,152] [INFO] [config.py:1001:print]   dump_state ................... False
[2024-09-09 13:16:09,152] [INFO] [config.py:1001:print]   dynamic_loss_scale_args ...... None
[2024-09-09 13:16:09,152] [INFO] [config.py:1001:print]   eigenvalue_enabled ........... False
[2024-09-09 13:16:09,152] [INFO] [config.py:1001:print]   eigenvalue_gas_boundary_resolution  1
[2024-09-09 13:16:09,152] [INFO] [config.py:1001:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-09-09 13:16:09,152] [INFO] [config.py:1001:print]   eigenvalue_layer_num ......... 0
[2024-09-09 13:16:09,152] [INFO] [config.py:1001:print]   eigenvalue_max_iter .......... 100
[2024-09-09 13:16:09,152] [INFO] [config.py:1001:print]   eigenvalue_stability ......... 1e-06
[2024-09-09 13:16:09,152] [INFO] [config.py:1001:print]   eigenvalue_tol ............... 0.01
[2024-09-09 13:16:09,152] [INFO] [config.py:1001:print]   eigenvalue_verbose ........... False
[2024-09-09 13:16:09,152] [INFO] [config.py:1001:print]   elasticity_enabled ........... False
[2024-09-09 13:16:09,153] [INFO] [config.py:1001:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-09-09 13:16:09,155] [INFO] [config.py:1001:print]   fp16_auto_cast ............... False
[2024-09-09 13:16:09,155] [INFO] [config.py:1001:print]   fp16_enabled ................. True
[2024-09-09 13:16:09,155] [INFO] [config.py:1001:print]   fp16_master_weights_and_gradients  False
[2024-09-09 13:16:09,155] [INFO] [config.py:1001:print]   global_rank .................. 0
[2024-09-09 13:16:09,155] [INFO] [config.py:1001:print]   grad_accum_dtype ............. None
[2024-09-09 13:16:09,155] [INFO] [config.py:1001:print]   gradient_accumulation_steps .. 1
[2024-09-09 13:16:09,155] [INFO] [config.py:1001:print]   gradient_clipping ............ 0.1
[2024-09-09 13:16:09,155] [INFO] [config.py:1001:print]   gradient_predivide_factor .... 1.0
[2024-09-09 13:16:09,155] [INFO] [config.py:1001:print]   graph_harvesting ............. False
[2024-09-09 13:16:09,155] [INFO] [config.py:1001:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-09-09 13:16:09,155] [INFO] [config.py:1001:print]   initial_dynamic_scale ........ 65536
[2024-09-09 13:16:09,155] [INFO] [config.py:1001:print]   load_universal_checkpoint .... False
[2024-09-09 13:16:09,158] [INFO] [config.py:1001:print]   loss_scale ................... 0
[2024-09-09 13:16:09,160] [INFO] [config.py:1001:print]   memory_breakdown ............. False
[2024-09-09 13:16:09,162] [INFO] [config.py:1001:print]   mics_hierarchial_params_gather  False
[2024-09-09 13:16:09,162] [INFO] [config.py:1001:print]   mics_shard_size .............. -1
[2024-09-09 13:16:09,162] [INFO] [config.py:1001:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-09-09 13:16:09,163] [INFO] [config.py:1001:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-09-09 13:16:09,164] [INFO] [config.py:1001:print]   optimizer_legacy_fusion ...... False
[2024-09-09 13:16:09,164] [INFO] [config.py:1001:print]   optimizer_name ............... None
[2024-09-09 13:16:09,164] [INFO] [config.py:1001:print]   optimizer_params ............. None
[2024-09-09 13:16:09,165] [INFO] [config.py:1001:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-09-09 13:16:09,165] [INFO] [config.py:1001:print]   pld_enabled .................. False
[2024-09-09 13:16:09,165] [INFO] [config.py:1001:print]   pld_params ................... False
[2024-09-09 13:16:09,165] [INFO] [config.py:1001:print]   prescale_gradients ........... False
[2024-09-09 13:16:09,165] [INFO] [config.py:1001:print]   scheduler_name ............... None
[2024-09-09 13:16:09,165] [INFO] [config.py:1001:print]   scheduler_params ............. None
[2024-09-09 13:16:09,165] [INFO] [config.py:1001:print]   seq_parallel_communication_data_type  torch.float32
[2024-09-09 13:16:09,165] [INFO] [config.py:1001:print]   sparse_attention ............. None
[2024-09-09 13:16:09,165] [INFO] [config.py:1001:print]   sparse_gradients_enabled ..... False
[2024-09-09 13:16:09,165] [INFO] [config.py:1001:print]   steps_per_print .............. 50
[2024-09-09 13:16:09,165] [INFO] [config.py:1001:print]   timers_config ................ enabled=True synchronized=True
[2024-09-09 13:16:09,167] [INFO] [config.py:1001:print]   train_batch_size ............. 2
[2024-09-09 13:16:09,167] [INFO] [config.py:1001:print]   train_micro_batch_size_per_gpu  2
[2024-09-09 13:16:09,167] [INFO] [config.py:1001:print]   use_data_before_expert_parallel_  False
[2024-09-09 13:16:09,170] [INFO] [config.py:1001:print]   use_node_local_storage ....... False
[2024-09-09 13:16:09,172] [INFO] [config.py:1001:print]   wall_clock_breakdown ......... False
[2024-09-09 13:16:09,175] [INFO] [config.py:1001:print]   weight_quantization_config ... None
[2024-09-09 13:16:09,175] [INFO] [config.py:1001:print]   world_size ................... 1
[2024-09-09 13:16:09,175] [INFO] [config.py:1001:print]   zero_allow_untested_optimizer  True
[2024-09-09 13:16:09,175] [INFO] [config.py:1001:print]   zero_config .................. stage=2 contiguous_gradients=False reduce_scatter=True reduce_bucket_size=1000000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=1000000000 overlap_comm=True load_from_fp32_weights=False elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-09-09 13:16:09,175] [INFO] [config.py:1001:print]   zero_enabled ................. True
[2024-09-09 13:16:09,175] [INFO] [config.py:1001:print]   zero_force_ds_cpu_optimizer .. True
[2024-09-09 13:16:09,175] [INFO] [config.py:1001:print]   zero_optimization_stage ...... 2
[2024-09-09 13:16:09,175] [INFO] [config.py:987:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 2, 
    "gradient_accumulation_steps": 1, 
    "steps_per_print": 50, 
    "gradient_clipping": 0.1, 
    "zero_optimization": {
        "stage": 2, 
        "cpu_offload": false, 
        "contiguous_gradients": false, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 1.000000e+09, 
        "allgather_bucket_size": 1.000000e+09, 
        "load_from_fp32_weights": false
    }, 
    "zero_allow_untested_optimizer": true, 
    "bf16": {
        "enabled": false
    }, 
    "fp16": {
        "enabled": true
    }, 
    "loss_scale": 0, 
    "loss_scale_window": 400, 
    "hysteresis": 2, 
    "min_loss_scale": 1, 
    "activation_checkpointing": {
        "partition_activations": false, 
        "contiguous_memory_optimization": false
    }, 
    "wall_clock_breakdown": false
}
[2024-09-09 13:16:09,175] [INFO] [RANK 0] learning rate decaying style linear, ratio 10.0
[2024-09-09 13:16:09,177] [INFO] [RANK 0] Finetuning Model...
[2024-09-09 13:16:09,177] [INFO] [RANK 0] arguments:
[2024-09-09 13:16:09,177] [INFO] [RANK 0]   base ......................... ['configs/cogvideox_2b_lora.yaml', 'configs/sft.yaml']
[2024-09-09 13:16:09,179] [INFO] [RANK 0]   model_parallel_size .......... 1
[2024-09-09 13:16:09,179] [INFO] [RANK 0]   force_pretrain ............... False
[2024-09-09 13:16:09,179] [INFO] [RANK 0]   device ....................... 0
[2024-09-09 13:16:09,179] [INFO] [RANK 0]   debug ........................ False
[2024-09-09 13:16:09,181] [INFO] [RANK 0]   log_image .................... True
[2024-09-09 13:16:09,181] [INFO] [RANK 0]   output_dir ................... samples
[2024-09-09 13:16:09,182] [INFO] [RANK 0]   input_dir .................... None
[2024-09-09 13:16:09,182] [INFO] [RANK 0]   input_type ................... cli
[2024-09-09 13:16:09,184] [INFO] [RANK 0]   input_file ................... input.txt
[2024-09-09 13:16:09,184] [INFO] [RANK 0]   final_size ................... 2048
[2024-09-09 13:16:09,184] [INFO] [RANK 0]   sdedit ....................... False
[2024-09-09 13:16:09,184] [INFO] [RANK 0]   grid_num_rows ................ 1
[2024-09-09 13:16:09,184] [INFO] [RANK 0]   force_inference .............. False
[2024-09-09 13:16:09,184] [INFO] [RANK 0]   lcm_steps .................... None
[2024-09-09 13:16:09,184] [INFO] [RANK 0]   sampling_num_frames .......... 32
[2024-09-09 13:16:09,184] [INFO] [RANK 0]   sampling_fps ................. 8
[2024-09-09 13:16:09,184] [INFO] [RANK 0]   only_save_latents ............ False
[2024-09-09 13:16:09,184] [INFO] [RANK 0]   only_log_video_latents ....... False
[2024-09-09 13:16:09,184] [INFO] [RANK 0]   latent_channels .............. 32
[2024-09-09 13:16:09,184] [INFO] [RANK 0]   image2video .................. False
[2024-09-09 13:16:09,184] [INFO] [RANK 0]   experiment_name .............. lora-disney-09-09-13-15
[2024-09-09 13:16:09,186] [INFO] [RANK 0]   train_iters .................. 1000
[2024-09-09 13:16:09,189] [INFO] [RANK 0]   batch_size ................... 2
[2024-09-09 13:16:09,191] [INFO] [RANK 0]   lr ........................... 0.0005
[2024-09-09 13:16:09,191] [INFO] [RANK 0]   mode ......................... finetune
[2024-09-09 13:16:09,191] [INFO] [RANK 0]   seed ......................... 42
[2024-09-09 13:16:09,193] [INFO] [RANK 0]   zero_stage ................... 0
[2024-09-09 13:16:09,193] [INFO] [RANK 0]   checkpoint_activations ....... True
[2024-09-09 13:16:09,193] [INFO] [RANK 0]   checkpoint_num_layers ........ 1
[2024-09-09 13:16:09,193] [INFO] [RANK 0]   checkpoint_skip_layers ....... 0
[2024-09-09 13:16:09,193] [INFO] [RANK 0]   fp16 ......................... True
[2024-09-09 13:16:09,193] [INFO] [RANK 0]   bf16 ......................... False
[2024-09-09 13:16:09,193] [INFO] [RANK 0]   gradient_accumulation_steps .. 1
[2024-09-09 13:16:09,195] [INFO] [RANK 0]   profiling .................... -1
[2024-09-09 13:16:09,195] [INFO] [RANK 0]   epochs ....................... None
[2024-09-09 13:16:09,196] [INFO] [RANK 0]   log_interval ................. 20
[2024-09-09 13:16:09,196] [INFO] [RANK 0]   summary_dir .................. 
[2024-09-09 13:16:09,198] [INFO] [RANK 0]   save_args .................... False
[2024-09-09 13:16:09,201] [INFO] [RANK 0]   lr_decay_iters ............... None
[2024-09-09 13:16:09,203] [INFO] [RANK 0]   lr_decay_style ............... linear
[2024-09-09 13:16:09,203] [INFO] [RANK 0]   lr_decay_ratio ............... 0.1
[2024-09-09 13:16:09,203] [INFO] [RANK 0]   warmup ....................... 0.01
[2024-09-09 13:16:09,208] [INFO] [RANK 0]   weight_decay ................. 5e-06
[2024-09-09 13:16:09,208] [INFO] [RANK 0]   save ......................... /mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora/lora-disney-09-09-13-15
[2024-09-09 13:16:09,208] [INFO] [RANK 0]   load ......................... /mnt/ceph/develop/jiawei/model_checkpoint/CogVideoX-2b-sat/transformer
[2024-09-09 13:16:09,208] [INFO] [RANK 0]   force_train .................. True
[2024-09-09 13:16:09,208] [INFO] [RANK 0]   save_interval ................ 500
[2024-09-09 13:16:09,208] [INFO] [RANK 0]   no_save_rng .................. False
[2024-09-09 13:16:09,208] [INFO] [RANK 0]   no_load_rng .................. True
[2024-09-09 13:16:09,211] [INFO] [RANK 0]   resume_dataloader ............ False
[2024-09-09 13:16:09,214] [INFO] [RANK 0]   distributed_backend .......... nccl
[2024-09-09 13:16:09,217] [INFO] [RANK 0]   local_rank ................... 0
[2024-09-09 13:16:09,217] [INFO] [RANK 0]   exit_interval ................ None
[2024-09-09 13:16:09,217] [INFO] [RANK 0]   wandb ........................ False
[2024-09-09 13:16:09,217] [INFO] [RANK 0]   wandb_project_name ........... default_project
[2024-09-09 13:16:09,217] [INFO] [RANK 0]   eval_batch_size .............. 1
[2024-09-09 13:16:09,217] [INFO] [RANK 0]   eval_iters ................... 1
[2024-09-09 13:16:09,217] [INFO] [RANK 0]   eval_interval ................ 100
[2024-09-09 13:16:09,217] [INFO] [RANK 0]   strict_eval .................. False
[2024-09-09 13:16:09,217] [INFO] [RANK 0]   train_data ................... ['/mnt/ceph/develop/jiawei/lora_dataset/cogvideo_lora_dataset']
[2024-09-09 13:16:09,217] [INFO] [RANK 0]   train_data_weights ........... None
[2024-09-09 13:16:09,219] [INFO] [RANK 0]   iterable_dataset ............. False
[2024-09-09 13:16:09,219] [INFO] [RANK 0]   iterable_dataset_eval ........ 
[2024-09-09 13:16:09,219] [INFO] [RANK 0]   batch_from_same_dataset ...... False
[2024-09-09 13:16:09,222] [INFO] [RANK 0]   valid_data ................... ['/mnt/ceph/develop/jiawei/lora_dataset/cogvideo_lora_dataset']
[2024-09-09 13:16:09,224] [INFO] [RANK 0]   test_data .................... None
[2024-09-09 13:16:09,226] [INFO] [RANK 0]   split ........................ 1,0,0
[2024-09-09 13:16:09,226] [INFO] [RANK 0]   num_workers .................. 8
[2024-09-09 13:16:09,226] [INFO] [RANK 0]   block_size ................... 10000
[2024-09-09 13:16:09,226] [INFO] [RANK 0]   prefetch_factor .............. 4
[2024-09-09 13:16:09,226] [INFO] [RANK 0]   deepspeed .................... True
[2024-09-09 13:16:09,226] [INFO] [RANK 0]   deepspeed_config ............. {'train_micro_batch_size_per_gpu': 2, 'gradient_accumulation_steps': 1, 'steps_per_print': 50, 'gradient_clipping': 0.1, 'zero_optimization': {'stage': 2, 'cpu_offload': False, 'contiguous_gradients': False, 'overlap_comm': True, 'reduce_scatter': True, 'reduce_bucket_size': 1000000000, 'allgather_bucket_size': 1000000000, 'load_from_fp32_weights': False}, 'zero_allow_untested_optimizer': True, 'bf16': {'enabled': False}, 'fp16': {'enabled': True}, 'loss_scale': 0, 'loss_scale_window': 400, 'hysteresis': 2, 'min_loss_scale': 1, 'activation_checkpointing': {'partition_activations': False, 'contiguous_memory_optimization': False}, 'wall_clock_breakdown': False}
[2024-09-09 13:16:09,226] [INFO] [RANK 0]   deepscale .................... False
[2024-09-09 13:16:09,226] [INFO] [RANK 0]   deepscale_config ............. None
[2024-09-09 13:16:09,227] [INFO] [RANK 0]   model_config ................. {'scale_factor': 1.15258426, 'disable_first_stage_autocast': True, 'not_trainable_prefixes': ['all'], 'log_keys': ['txt'], 'denoiser_config': {'target': 'sgm.modules.diffusionmodules.denoiser.DiscreteDenoiser', 'params': {'num_idx': 1000, 'quantize_c_noise': False, 'weighting_config': {'target': 'sgm.modules.diffusionmodules.denoiser_weighting.EpsWeighting'}, 'scaling_config': {'target': 'sgm.modules.diffusionmodules.denoiser_scaling.VideoScaling'}, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 3.0}}}}, 'network_config': {'target': 'dit_video_concat.DiffusionTransformer', 'params': {'time_embed_dim': 512, 'elementwise_affine': True, 'num_frames': 49, 'time_compressed_rate': 4, 'latent_width': 90, 'latent_height': 60, 'num_layers': 30, 'patch_size': 2, 'in_channels': 16, 'out_channels': 16, 'hidden_size': 1920, 'adm_in_channels': 256, 'num_attention_heads': 30, 'transformer_args': {'checkpoint_activations': True, 'vocab_size': 1, 'max_sequence_length': 64, 'layernorm_order': 'pre', 'skip_init': False, 'model_parallel_size': 1, 'is_decoder': False, 'num_layers': 30, 'hidden_size': 1920, 'num_attention_heads': 30, 'parallel_output': True}, 'modules': {'pos_embed_config': {'target': 'dit_video_concat.Basic3DPositionEmbeddingMixin', 'params': {'text_length': 226, 'height_interpolation': 1.875, 'width_interpolation': 1.875}}, 'lora_config': {'target': 'sat.model.finetune.lora2.LoraMixin', 'params': {'r': 128}}, 'patch_embed_config': {'target': 'dit_video_concat.ImagePatchEmbeddingMixin', 'params': {'text_hidden_size': 4096}}, 'adaln_layer_config': {'target': 'dit_video_concat.AdaLNMixin', 'params': {'qk_ln': True}}, 'final_layer_config': {'target': 'dit_video_concat.FinalLayerMixin'}}, 'dtype': 'fp16'}}, 'conditioner_config': {'target': 'sgm.modules.GeneralConditioner', 'params': {'emb_models': [{'is_trainable': False, 'input_key': 'txt', 'ucg_rate': 0.1, 'target': 'sgm.modules.encoders.modules.FrozenT5Embedder', 'params': {'model_dir': '/mnt/ceph/develop/jiawei/model_checkpoint/t5-v1_1-xxl', 'max_length': 226}}]}}, 'first_stage_config': {'target': 'vae_modules.autoencoder.VideoAutoencoderInferenceWrapper', 'params': {'cp_size': 1, 'ckpt_path': '/mnt/ceph/develop/jiawei/model_checkpoint/CogVideoX-2b-sat/vae/3d-vae.pt', 'ignore_keys': ['loss'], 'loss_config': {'target': 'torch.nn.Identity'}, 'regularizer_config': {'target': 'vae_modules.regularizers.DiagonalGaussianRegularizer'}, 'encoder_config': {'target': 'vae_modules.cp_enc_dec.ContextParallelEncoder3D', 'params': {'double_z': True, 'z_channels': 16, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 2, 4], 'attn_resolutions': [], 'num_res_blocks': 3, 'dropout': 0.0, 'gather_norm': True}}, 'decoder_config': {'target': 'vae_modules.cp_enc_dec.ContextParallelDecoder3D', 'params': {'double_z': True, 'z_channels': 16, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 2, 4], 'attn_resolutions': [], 'num_res_blocks': 3, 'dropout': 0.0, 'gather_norm': False}}}}, 'loss_fn_config': {'target': 'sgm.modules.diffusionmodules.loss.VideoDiffusionLoss', 'params': {'offset_noise_level': 0, 'sigma_sampler_config': {'target': 'sgm.modules.diffusionmodules.sigma_sampling.DiscreteSampling', 'params': {'uniform_sampling': True, 'num_idx': 1000, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 3.0}}}}}}, 'sampler_config': {'target': 'sgm.modules.diffusionmodules.sampling.VPSDEDPMPP2MSampler', 'params': {'num_steps': 50, 'verbose': True, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 3.0}}, 'guider_config': {'target': 'sgm.modules.diffusionmodules.guiders.DynamicCFG', 'params': {'scale': 6, 'exp': 5, 'num_steps': 50}}}}}
[2024-09-09 13:16:09,229] [INFO] [RANK 0]   data_config .................. {'target': 'data_video.SFTDataset', 'params': {'video_size': [480, 720], 'fps': 8, 'max_num_frames': 49, 'skip_frms_num': 3.0}}
[2024-09-09 13:16:09,229] [INFO] [RANK 0]   cuda ......................... True
[2024-09-09 13:16:09,229] [INFO] [RANK 0]   rank ......................... 0
[2024-09-09 13:16:09,229] [INFO] [RANK 0]   world_size ................... 1
[2024-09-09 13:16:09,229] [INFO] [RANK 0]   deepspeed_activation_checkpointing  True
[2024-09-09 13:16:09,229] [INFO] [RANK 0]   master_ip .................... nm04-a800-node083
[2024-09-09 13:16:09,229] [INFO] [RANK 0]   master_port .................. 26645
[2024-09-09 13:16:09,229] [INFO] [RANK 0]   log_config ................... [{'model': {'scale_factor': 1.15258426, 'disable_first_stage_autocast': True, 'not_trainable_prefixes': ['all'], 'log_keys': ['txt'], 'denoiser_config': {'target': 'sgm.modules.diffusionmodules.denoiser.DiscreteDenoiser', 'params': {'num_idx': 1000, 'quantize_c_noise': False, 'weighting_config': {'target': 'sgm.modules.diffusionmodules.denoiser_weighting.EpsWeighting'}, 'scaling_config': {'target': 'sgm.modules.diffusionmodules.denoiser_scaling.VideoScaling'}, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 3.0}}}}, 'network_config': {'target': 'dit_video_concat.DiffusionTransformer', 'params': {'time_embed_dim': 512, 'elementwise_affine': True, 'num_frames': 49, 'time_compressed_rate': 4, 'latent_width': 90, 'latent_height': 60, 'num_layers': 30, 'patch_size': 2, 'in_channels': 16, 'out_channels': 16, 'hidden_size': 1920, 'adm_in_channels': 256, 'num_attention_heads': 30, 'transformer_args': {'checkpoint_activations': True, 'vocab_size': 1, 'max_sequence_length': 64, 'layernorm_order': 'pre', 'skip_init': False, 'model_parallel_size': 1, 'is_decoder': False}, 'modules': {'pos_embed_config': {'target': 'dit_video_concat.Basic3DPositionEmbeddingMixin', 'params': {'text_length': 226, 'height_interpolation': 1.875, 'width_interpolation': 1.875}}, 'lora_config': {'target': 'sat.model.finetune.lora2.LoraMixin', 'params': {'r': 128}}, 'patch_embed_config': {'target': 'dit_video_concat.ImagePatchEmbeddingMixin', 'params': {'text_hidden_size': 4096}}, 'adaln_layer_config': {'target': 'dit_video_concat.AdaLNMixin', 'params': {'qk_ln': True}}, 'final_layer_config': {'target': 'dit_video_concat.FinalLayerMixin'}}}}, 'conditioner_config': {'target': 'sgm.modules.GeneralConditioner', 'params': {'emb_models': [{'is_trainable': False, 'input_key': 'txt', 'ucg_rate': 0.1, 'target': 'sgm.modules.encoders.modules.FrozenT5Embedder', 'params': {'model_dir': '/mnt/ceph/develop/jiawei/model_checkpoint/t5-v1_1-xxl', 'max_length': 226}}]}}, 'first_stage_config': {'target': 'vae_modules.autoencoder.VideoAutoencoderInferenceWrapper', 'params': {'cp_size': 1, 'ckpt_path': '/mnt/ceph/develop/jiawei/model_checkpoint/CogVideoX-2b-sat/vae/3d-vae.pt', 'ignore_keys': ['loss'], 'loss_config': {'target': 'torch.nn.Identity'}, 'regularizer_config': {'target': 'vae_modules.regularizers.DiagonalGaussianRegularizer'}, 'encoder_config': {'target': 'vae_modules.cp_enc_dec.ContextParallelEncoder3D', 'params': {'double_z': True, 'z_channels': 16, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 2, 4], 'attn_resolutions': [], 'num_res_blocks': 3, 'dropout': 0.0, 'gather_norm': True}}, 'decoder_config': {'target': 'vae_modules.cp_enc_dec.ContextParallelDecoder3D', 'params': {'double_z': True, 'z_channels': 16, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 2, 4], 'attn_resolutions': [], 'num_res_blocks': 3, 'dropout': 0.0, 'gather_norm': False}}}}, 'loss_fn_config': {'target': 'sgm.modules.diffusionmodules.loss.VideoDiffusionLoss', 'params': {'offset_noise_level': 0, 'sigma_sampler_config': {'target': 'sgm.modules.diffusionmodules.sigma_sampling.DiscreteSampling', 'params': {'uniform_sampling': True, 'num_idx': 1000, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 3.0}}}}}}, 'sampler_config': {'target': 'sgm.modules.diffusionmodules.sampling.VPSDEDPMPP2MSampler', 'params': {'num_steps': 50, 'verbose': True, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 3.0}}, 'guider_config': {'target': 'sgm.modules.diffusionmodules.guiders.DynamicCFG', 'params': {'scale': 6, 'exp': 5, 'num_steps': 50}}}}}}, {'args': {'checkpoint_activations': True, 'model_parallel_size': 1, 'experiment_name': 'lora-disney', 'mode': 'finetune', 'load': '/mnt/ceph/develop/jiawei/model_checkpoint/CogVideoX-2b-sat/transformer', 'no_load_rng': True, 'train_iters': 1000, 'eval_iters': 1, 'eval_interval': 100, 'eval_batch_size': 1, 'save': '/mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora', 'save_interval': 500, 'log_interval': 20, 'train_data': ['/mnt/ceph/develop/jiawei/lora_dataset/cogvideo_lora_dataset'], 'valid_data': ['/mnt/ceph/develop/jiawei/lora_dataset/cogvideo_lora_dataset'], 'split': '1,0,0', 'num_workers': 8, 'force_train': True, 'only_log_video_latents': False}, 'data': {'target': 'data_video.SFTDataset', 'params': {'video_size': [480, 720], 'fps': 8, 'max_num_frames': 49, 'skip_frms_num': 3.0}}, 'deepspeed': {'train_micro_batch_size_per_gpu': 2, 'gradient_accumulation_steps': 1, 'steps_per_print': 50, 'gradient_clipping': 0.1, 'zero_optimization': {'stage': 2, 'cpu_offload': False, 'contiguous_gradients': False, 'overlap_comm': True, 'reduce_scatter': True, 'reduce_bucket_size': 1000000000, 'allgather_bucket_size': 1000000000, 'load_from_fp32_weights': False}, 'zero_allow_untested_optimizer': True, 'bf16': {'enabled': False}, 'fp16': {'enabled': True}, 'loss_scale': 0, 'loss_scale_window': 400, 'hysteresis': 2, 'min_loss_scale': 1, 'optimizer': {'type': 'sat.ops.FusedEmaAdam', 'params': {'lr': '5E-4', 'betas': [0.9, 0.95], 'eps': '1e-8', 'weight_decay': '5E-6'}}, 'activation_checkpointing': {'partition_activations': False, 'contiguous_memory_optimization': False}, 'wall_clock_breakdown': False}}]
[2024-09-09 13:16:09,231] [INFO] [RANK 0]   do_train ..................... True
[2024-09-09 13:16:09,231] [INFO] [RANK 0]   val_last_shape ............... []
[2024-09-09 13:16:09,231] [INFO] [RANK 0]   val_drop_number .............. 0
[2024-09-09 13:16:09,231] [INFO] [RANK 0]   do_valid ..................... True
[2024-09-09 13:16:09,231] [INFO] [RANK 0]   do_test ...................... False
[2024-09-09 13:16:09,231] [INFO] [RANK 0]   iteration .................... 0
[2024-09-09 13:16:39,600] [INFO] [checkpointing.py:541:forward] Activation Checkpointing Information
[2024-09-09 13:16:39,600] [INFO] [checkpointing.py:542:forward] ----Partition Activations False, CPU CHECKPOINTING False
[2024-09-09 13:16:39,600] [INFO] [checkpointing.py:543:forward] ----contiguous Memory Checkpointing False with None total layers
[2024-09-09 13:16:39,600] [INFO] [checkpointing.py:545:forward] ----Synchronization False
[2024-09-09 13:16:39,604] [INFO] [checkpointing.py:546:forward] ----Profiling time in checkpointing False
[2024-09-09 13:16:46,103] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4294967296, reducing to 2147483648
[2024-09-09 13:17:01,073] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2147483648, reducing to 1073741824
[2024-09-09 13:17:16,254] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1073741824, reducing to 536870912
[2024-09-09 13:17:46,884] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 536870912, reducing to 268435456
[2024-09-09 13:19:32,339] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 268435456, reducing to 134217728
[2024-09-09 13:21:33,217] [INFO] [RANK 0]  iteration       20/    1000 | elapsed time per iteration (ms): 16131.0 | learning rate 2.500E-05 | total loss 2.303817E-01 | loss 2.303817E-01 | loss scale 134217728.0 |speed 7.44 samples/(min*GPU)
[2024-09-09 13:21:33,218] [INFO] [RANK 0] after 20 iterations memory (MB) | allocated: 13975.08984375 | max allocated: 64454.34912109375 | cached: 22772.0 | max cached: 77966.0
[2024-09-09 13:21:33,221] [INFO] [RANK 0] time (ms) | forward: 11238.17 | backward: 4866.39 | allreduce: 0.00 | optimizer: 25.30 | data loader: 158.22
[2024-09-09 13:21:48,390] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 134217728, reducing to 67108864
[2024-09-09 13:23:33,824] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 67108864, reducing to 33554432
[2024-09-09 13:25:34,438] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 33554432, reducing to 16777216
[2024-09-09 13:26:34,941] [INFO] [RANK 0]  iteration       40/    1000 | elapsed time per iteration (ms): 15086.2 | learning rate 2.500E-05 | total loss 1.918419E-01 | loss 1.918419E-01 | loss scale 16777216.0 |speed 7.95 samples/(min*GPU)
[2024-09-09 13:26:34,943] [INFO] [RANK 0] time (ms) | forward: 10190.52 | backward: 4868.34 | allreduce: 0.00 | optimizer: 26.07 | data loader: 0.29
[2024-09-09 13:29:05,664] [INFO] [logging.py:96:log_dist] [Rank 0] step=50, skipped=8, lr=[2.5e-05], mom=[[0.9, 0.95]]
E0909 13:29:45.458000 140039056193344 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: -9) local_rank: 0 (pid: 1766484) of binary: /mnt/ceph/develop/jiawei/conda_env/cogvideo/bin/python
Traceback (most recent call last):
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
========================================================
train_video.py FAILED
--------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
--------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-09-09_13:29:45
  host      : nm04-a800-node083
  rank      : 0 (local_rank: 0)
  exitcode  : -9 (pid: 1766484)
  error_file: <N/A>
  traceback : Signal 9 (SIGKILL) received by PID 1766484
========================================================
DONE on nm04-a800-node083
RUN on nm04-a800-node083, CUDA_VISIBLE_DEVICES=0
torchrun --standalone --nproc_per_node=1 train_video.py --base configs/cogvideox_2b_lora.yaml configs/sft.yaml --seed 42
[2024-09-09 13:34:43,710] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
[93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, input, weight, bias=None):
/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/kornia/feature/lightglue.py:44: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)
no module 'xformers'. Processing without...
no module 'xformers'. Processing without...
Traceback (most recent call last):
  File "/mnt/ceph/develop/jiawei/CogVideo/sat/train_video.py", line 208, in <module>
    args = get_args(args_list)
  File "/mnt/ceph/develop/jiawei/CogVideo/sat/arguments.py", line 71, in get_args
    args = process_config_to_args(args)
  File "/mnt/ceph/develop/jiawei/CogVideo/sat/arguments.py", line 259, in process_config_to_args
    configs = [OmegaConf.load(cfg) for cfg in args.base]
  File "/mnt/ceph/develop/jiawei/CogVideo/sat/arguments.py", line 259, in <listcomp>
    configs = [OmegaConf.load(cfg) for cfg in args.base]
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/omegaconf/omegaconf.py", line 190, in load
    obj = yaml.load(f, Loader=get_yaml_loader())
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/yaml/__init__.py", line 81, in load
    return loader.get_single_data()
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/yaml/constructor.py", line 51, in get_single_data
    return self.construct_document(node)
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/yaml/constructor.py", line 60, in construct_document
    for dummy in generator:
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/yaml/constructor.py", line 413, in construct_yaml_map
    value = self.construct_mapping(node)
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/omegaconf/_utils.py", line 144, in construct_mapping
    raise yaml.constructor.ConstructorError(
yaml.constructor.ConstructorError: while constructing a mapping
  in "/mnt/ceph/develop/jiawei/CogVideo/sat/configs/sft.yaml", line 2, column 3
found duplicate key experiment_name
  in "/mnt/ceph/develop/jiawei/CogVideo/sat/configs/sft.yaml", line 23, column 3
E0909 13:35:03.512000 140627001079616 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 0 (pid: 1779211) of binary: /mnt/ceph/develop/jiawei/conda_env/cogvideo/bin/python
Traceback (most recent call last):
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train_video.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-09-09_13:35:03
  host      : nm04-a800-node083
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 1779211)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
DONE on nm04-a800-node083
RUN on nm04-a800-node083, CUDA_VISIBLE_DEVICES=0
torchrun --standalone --nproc_per_node=1 train_video.py --base configs/cogvideox_2b_lora.yaml configs/sft.yaml --seed 42
[2024-09-09 13:37:02,751] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
[93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, input, weight, bias=None):
/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/kornia/feature/lightglue.py:44: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)
no module 'xformers'. Processing without...
no module 'xformers'. Processing without...
Traceback (most recent call last):
  File "/mnt/ceph/develop/jiawei/CogVideo/sat/train_video.py", line 208, in <module>
    args = get_args(args_list)
  File "/mnt/ceph/develop/jiawei/CogVideo/sat/arguments.py", line 71, in get_args
    args = process_config_to_args(args)
  File "/mnt/ceph/develop/jiawei/CogVideo/sat/arguments.py", line 259, in process_config_to_args
    configs = [OmegaConf.load(cfg) for cfg in args.base]
  File "/mnt/ceph/develop/jiawei/CogVideo/sat/arguments.py", line 259, in <listcomp>
    configs = [OmegaConf.load(cfg) for cfg in args.base]
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/omegaconf/omegaconf.py", line 190, in load
    obj = yaml.load(f, Loader=get_yaml_loader())
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/yaml/__init__.py", line 81, in load
    return loader.get_single_data()
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/yaml/constructor.py", line 51, in get_single_data
    return self.construct_document(node)
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/yaml/constructor.py", line 60, in construct_document
    for dummy in generator:
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/yaml/constructor.py", line 413, in construct_yaml_map
    value = self.construct_mapping(node)
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/omegaconf/_utils.py", line 144, in construct_mapping
    raise yaml.constructor.ConstructorError(
yaml.constructor.ConstructorError: while constructing a mapping
  in "/mnt/ceph/develop/jiawei/CogVideo/sat/configs/sft.yaml", line 2, column 3
found duplicate key experiment_name
  in "/mnt/ceph/develop/jiawei/CogVideo/sat/configs/sft.yaml", line 23, column 3
E0909 13:37:21.611000 140668403439424 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 0 (pid: 1780442) of binary: /mnt/ceph/develop/jiawei/conda_env/cogvideo/bin/python
Traceback (most recent call last):
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train_video.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-09-09_13:37:21
  host      : nm04-a800-node083
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 1780442)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
DONE on nm04-a800-node083
RUN on nm04-a800-node083, CUDA_VISIBLE_DEVICES=0
torchrun --standalone --nproc_per_node=1 train_video.py --base configs/cogvideox_2b_lora.yaml configs/sft.yaml --seed 42
[2024-09-09 13:37:56,084] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
[93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, input, weight, bias=None):
/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/kornia/feature/lightglue.py:44: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)
no module 'xformers'. Processing without...
no module 'xformers'. Processing without...
Traceback (most recent call last):
  File "/mnt/ceph/develop/jiawei/CogVideo/sat/train_video.py", line 208, in <module>
    args = get_args(args_list)
  File "/mnt/ceph/develop/jiawei/CogVideo/sat/arguments.py", line 71, in get_args
    args = process_config_to_args(args)
  File "/mnt/ceph/develop/jiawei/CogVideo/sat/arguments.py", line 259, in process_config_to_args
    configs = [OmegaConf.load(cfg) for cfg in args.base]
  File "/mnt/ceph/develop/jiawei/CogVideo/sat/arguments.py", line 259, in <listcomp>
    configs = [OmegaConf.load(cfg) for cfg in args.base]
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/omegaconf/omegaconf.py", line 190, in load
    obj = yaml.load(f, Loader=get_yaml_loader())
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/yaml/__init__.py", line 81, in load
    return loader.get_single_data()
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/yaml/constructor.py", line 51, in get_single_data
    return self.construct_document(node)
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/yaml/constructor.py", line 60, in construct_document
    for dummy in generator:
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/yaml/constructor.py", line 413, in construct_yaml_map
    value = self.construct_mapping(node)
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/omegaconf/_utils.py", line 144, in construct_mapping
    raise yaml.constructor.ConstructorError(
yaml.constructor.ConstructorError: while constructing a mapping
  in "/mnt/ceph/develop/jiawei/CogVideo/sat/configs/sft.yaml", line 2, column 3
found duplicate key experiment_name
  in "/mnt/ceph/develop/jiawei/CogVideo/sat/configs/sft.yaml", line 23, column 3
E0909 13:38:11.096000 140546216519488 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 0 (pid: 1781088) of binary: /mnt/ceph/develop/jiawei/conda_env/cogvideo/bin/python
Traceback (most recent call last):
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train_video.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-09-09_13:38:11
  host      : nm04-a800-node083
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 1781088)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
DONE on nm04-a800-node083
RUN on nm04-a800-node083, CUDA_VISIBLE_DEVICES=0
torchrun --standalone --nproc_per_node=1 train_video.py --base configs/cogvideox_2b_lora.yaml configs/sft.yaml --seed 42
[2024-09-09 13:38:24,424] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
[93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, input, weight, bias=None):
/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/kornia/feature/lightglue.py:44: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)
no module 'xformers'. Processing without...
no module 'xformers'. Processing without...
Traceback (most recent call last):
  File "/mnt/ceph/develop/jiawei/CogVideo/sat/train_video.py", line 208, in <module>
    args = get_args(args_list)
  File "/mnt/ceph/develop/jiawei/CogVideo/sat/arguments.py", line 71, in get_args
    args = process_config_to_args(args)
  File "/mnt/ceph/develop/jiawei/CogVideo/sat/arguments.py", line 259, in process_config_to_args
    configs = [OmegaConf.load(cfg) for cfg in args.base]
  File "/mnt/ceph/develop/jiawei/CogVideo/sat/arguments.py", line 259, in <listcomp>
    configs = [OmegaConf.load(cfg) for cfg in args.base]
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/omegaconf/omegaconf.py", line 190, in load
    obj = yaml.load(f, Loader=get_yaml_loader())
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/yaml/__init__.py", line 81, in load
    return loader.get_single_data()
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/yaml/constructor.py", line 51, in get_single_data
    return self.construct_document(node)
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/yaml/constructor.py", line 60, in construct_document
    for dummy in generator:
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/yaml/constructor.py", line 413, in construct_yaml_map
    value = self.construct_mapping(node)
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/omegaconf/_utils.py", line 144, in construct_mapping
    raise yaml.constructor.ConstructorError(
yaml.constructor.ConstructorError: while constructing a mapping
  in "/mnt/ceph/develop/jiawei/CogVideo/sat/configs/sft.yaml", line 2, column 3
found duplicate key experiment_name
  in "/mnt/ceph/develop/jiawei/CogVideo/sat/configs/sft.yaml", line 23, column 3
E0909 13:38:42.178000 140571980937024 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 0 (pid: 1781967) of binary: /mnt/ceph/develop/jiawei/conda_env/cogvideo/bin/python
Traceback (most recent call last):
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train_video.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-09-09_13:38:42
  host      : nm04-a800-node083
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 1781967)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
DONE on nm04-a800-node083
RUN on nm04-a800-node083, CUDA_VISIBLE_DEVICES=0
torchrun --standalone --nproc_per_node=1 train_video.py --base configs/cogvideox_2b_lora.yaml configs/sft.yaml --seed 42
[2024-09-09 13:39:32,616] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
[93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, input, weight, bias=None):
/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/kornia/feature/lightglue.py:44: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)
no module 'xformers'. Processing without...
no module 'xformers'. Processing without...
[2024-09-09 13:39:50,532] [INFO] using world size: 1
[2024-09-09 13:39:50,532] [INFO] Will override arguments with manually specified deepspeed_config!
[2024-09-09 13:39:50,539] [INFO] [RANK 0] > initializing model parallel with size 1
[2024-09-09 13:39:50,540] [INFO] [comm.py:637:init_distributed] cdb=None
Namespace(base=['configs/cogvideox_2b_lora.yaml', 'configs/sft.yaml'], model_parallel_size=1, force_pretrain=False, device=0, debug=False, log_image=True, output_dir='samples', input_dir=None, input_type='cli', input_file='input.txt', final_size=2048, sdedit=False, grid_num_rows=1, force_inference=False, lcm_steps=None, sampling_num_frames=32, sampling_fps=8, only_save_latents=False, only_log_video_latents=False, latent_channels=32, image2video=False, experiment_name='lora-disney', train_iters=1000, batch_size=2, lr=0.0005, mode='finetune', seed=42, zero_stage=0, checkpoint_activations=True, checkpoint_num_layers=1, checkpoint_skip_layers=0, fp16=True, bf16=False, gradient_accumulation_steps=1, profiling=-1, epochs=None, log_interval=20, summary_dir='/mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora', save_args=False, lr_decay_iters=None, lr_decay_style='linear', lr_decay_ratio=0.1, warmup=0.01, weight_decay=5e-06, save='/mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora', load='/mnt/ceph/develop/jiawei/model_checkpoint/CogVideoX-2b-sat/transformer', force_train=True, save_interval=500, no_save_rng=False, no_load_rng=True, resume_dataloader=False, distributed_backend='nccl', local_rank=0, exit_interval=None, wandb=True, wandb_project_name='cogvideox_ckpts_2b_lora', eval_batch_size=1, eval_iters=1, eval_interval=100, strict_eval=False, train_data=['/mnt/ceph/develop/jiawei/lora_dataset/cogvideo_lora_dataset'], train_data_weights=None, iterable_dataset=False, iterable_dataset_eval='', batch_from_same_dataset=False, valid_data=['/mnt/ceph/develop/jiawei/lora_dataset/cogvideo_lora_dataset'], test_data=None, split='1,0,0', num_workers=8, block_size=10000, prefetch_factor=4, deepspeed=True, deepspeed_config={'train_micro_batch_size_per_gpu': 2, 'gradient_accumulation_steps': 1, 'steps_per_print': 50, 'gradient_clipping': 0.1, 'zero_optimization': {'stage': 2, 'cpu_offload': False, 'contiguous_gradients': False, 'overlap_comm': True, 'reduce_scatter': True, 'reduce_bucket_size': 1000000000, 'allgather_bucket_size': 1000000000, 'load_from_fp32_weights': False}, 'zero_allow_untested_optimizer': True, 'bf16': {'enabled': False}, 'fp16': {'enabled': True}, 'loss_scale': 0, 'loss_scale_window': 400, 'hysteresis': 2, 'min_loss_scale': 1, 'optimizer': {'type': 'sat.ops.FusedEmaAdam', 'params': {'lr': 0.0005, 'betas': [0.9, 0.95], 'eps': 1e-08, 'weight_decay': 5e-06}}, 'activation_checkpointing': {'partition_activations': False, 'contiguous_memory_optimization': False}, 'wall_clock_breakdown': False}, deepscale=False, deepscale_config=None, model_config={'scale_factor': 1.15258426, 'disable_first_stage_autocast': True, 'not_trainable_prefixes': ['all'], 'log_keys': ['txt'], 'denoiser_config': {'target': 'sgm.modules.diffusionmodules.denoiser.DiscreteDenoiser', 'params': {'num_idx': 1000, 'quantize_c_noise': False, 'weighting_config': {'target': 'sgm.modules.diffusionmodules.denoiser_weighting.EpsWeighting'}, 'scaling_config': {'target': 'sgm.modules.diffusionmodules.denoiser_scaling.VideoScaling'}, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 3.0}}}}, 'network_config': {'target': 'dit_video_concat.DiffusionTransformer', 'params': {'time_embed_dim': 512, 'elementwise_affine': True, 'num_frames': 49, 'time_compressed_rate': 4, 'latent_width': 90, 'latent_height': 60, 'num_layers': 30, 'patch_size': 2, 'in_channels': 16, 'out_channels': 16, 'hidden_size': 1920, 'adm_in_channels': 256, 'num_attention_heads': 30, 'transformer_args': {'checkpoint_activations': True, 'vocab_size': 1, 'max_sequence_length': 64, 'layernorm_order': 'pre', 'skip_init': False, 'model_parallel_size': 1, 'is_decoder': False}, 'modules': {'pos_embed_config': {'target': 'dit_video_concat.Basic3DPositionEmbeddingMixin', 'params': {'text_length': 226, 'height_interpolation': 1.875, 'width_interpolation': 1.875}}, 'lora_config': {'target': 'sat.model.finetune.lora2.LoraMixin', 'params': {'r': 128}}, 'patch_embed_config': {'target': 'dit_video_concat.ImagePatchEmbeddingMixin', 'params': {'text_hidden_size': 4096}}, 'adaln_layer_config': {'target': 'dit_video_concat.AdaLNMixin', 'params': {'qk_ln': True}}, 'final_layer_config': {'target': 'dit_video_concat.FinalLayerMixin'}}}}, 'conditioner_config': {'target': 'sgm.modules.GeneralConditioner', 'params': {'emb_models': [{'is_trainable': False, 'input_key': 'txt', 'ucg_rate': 0.1, 'target': 'sgm.modules.encoders.modules.FrozenT5Embedder', 'params': {'model_dir': '/mnt/ceph/develop/jiawei/model_checkpoint/t5-v1_1-xxl', 'max_length': 226}}]}}, 'first_stage_config': {'target': 'vae_modules.autoencoder.VideoAutoencoderInferenceWrapper', 'params': {'cp_size': 1, 'ckpt_path': '/mnt/ceph/develop/jiawei/model_checkpoint/CogVideoX-2b-sat/vae/3d-vae.pt', 'ignore_keys': ['loss'], 'loss_config': {'target': 'torch.nn.Identity'}, 'regularizer_config': {'target': 'vae_modules.regularizers.DiagonalGaussianRegularizer'}, 'encoder_config': {'target': 'vae_modules.cp_enc_dec.ContextParallelEncoder3D', 'params': {'double_z': True, 'z_channels': 16, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 2, 4], 'attn_resolutions': [], 'num_res_blocks': 3, 'dropout': 0.0, 'gather_norm': True}}, 'decoder_config': {'target': 'vae_modules.cp_enc_dec.ContextParallelDecoder3D', 'params': {'double_z': True, 'z_channels': 16, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 2, 4], 'attn_resolutions': [], 'num_res_blocks': 3, 'dropout': 0.0, 'gather_norm': False}}}}, 'loss_fn_config': {'target': 'sgm.modules.diffusionmodules.loss.VideoDiffusionLoss', 'params': {'offset_noise_level': 0, 'sigma_sampler_config': {'target': 'sgm.modules.diffusionmodules.sigma_sampling.DiscreteSampling', 'params': {'uniform_sampling': True, 'num_idx': 1000, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 3.0}}}}}}, 'sampler_config': {'target': 'sgm.modules.diffusionmodules.sampling.VPSDEDPMPP2MSampler', 'params': {'num_steps': 50, 'verbose': True, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 3.0}}, 'guider_config': {'target': 'sgm.modules.diffusionmodules.guiders.DynamicCFG', 'params': {'scale': 6, 'exp': 5, 'num_steps': 50}}}}}, data_config={'target': 'data_video.SFTDataset', 'params': {'video_size': [480, 720], 'fps': 8, 'max_num_frames': 49, 'skip_frms_num': 3.0}}, cuda=True, rank=0, world_size=1, deepspeed_activation_checkpointing=True, master_ip='nm04-a800-node083', master_port='50933', log_config=[{'model': {'scale_factor': 1.15258426, 'disable_first_stage_autocast': True, 'not_trainable_prefixes': ['all'], 'log_keys': ['txt'], 'denoiser_config': {'target': 'sgm.modules.diffusionmodules.denoiser.DiscreteDenoiser', 'params': {'num_idx': 1000, 'quantize_c_noise': False, 'weighting_config': {'target': 'sgm.modules.diffusionmodules.denoiser_weighting.EpsWeighting'}, 'scaling_config': {'target': 'sgm.modules.diffusionmodules.denoiser_scaling.VideoScaling'}, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 3.0}}}}, 'network_config': {'target': 'dit_video_concat.DiffusionTransformer', 'params': {'time_embed_dim': 512, 'elementwise_affine': True, 'num_frames': 49, 'time_compressed_rate': 4, 'latent_width': 90, 'latent_height': 60, 'num_layers': 30, 'patch_size': 2, 'in_channels': 16, 'out_channels': 16, 'hidden_size': 1920, 'adm_in_channels': 256, 'num_attention_heads': 30, 'transformer_args': {'checkpoint_activations': True, 'vocab_size': 1, 'max_sequence_length': 64, 'layernorm_order': 'pre', 'skip_init': False, 'model_parallel_size': 1, 'is_decoder': False}, 'modules': {'pos_embed_config': {'target': 'dit_video_concat.Basic3DPositionEmbeddingMixin', 'params': {'text_length': 226, 'height_interpolation': 1.875, 'width_interpolation': 1.875}}, 'lora_config': {'target': 'sat.model.finetune.lora2.LoraMixin', 'params': {'r': 128}}, 'patch_embed_config': {'target': 'dit_video_concat.ImagePatchEmbeddingMixin', 'params': {'text_hidden_size': 4096}}, 'adaln_layer_config': {'target': 'dit_video_concat.AdaLNMixin', 'params': {'qk_ln': True}}, 'final_layer_config': {'target': 'dit_video_concat.FinalLayerMixin'}}}}, 'conditioner_config': {'target': 'sgm.modules.GeneralConditioner', 'params': {'emb_models': [{'is_trainable': False, 'input_key': 'txt', 'ucg_rate': 0.1, 'target': 'sgm.modules.encoders.modules.FrozenT5Embedder', 'params': {'model_dir': '/mnt/ceph/develop/jiawei/model_checkpoint/t5-v1_1-xxl', 'max_length': 226}}]}}, 'first_stage_config': {'target': 'vae_modules.autoencoder.VideoAutoencoderInferenceWrapper', 'params': {'cp_size': 1, 'ckpt_path': '/mnt/ceph/develop/jiawei/model_checkpoint/CogVideoX-2b-sat/vae/3d-vae.pt', 'ignore_keys': ['loss'], 'loss_config': {'target': 'torch.nn.Identity'}, 'regularizer_config': {'target': 'vae_modules.regularizers.DiagonalGaussianRegularizer'}, 'encoder_config': {'target': 'vae_modules.cp_enc_dec.ContextParallelEncoder3D', 'params': {'double_z': True, 'z_channels': 16, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 2, 4], 'attn_resolutions': [], 'num_res_blocks': 3, 'dropout': 0.0, 'gather_norm': True}}, 'decoder_config': {'target': 'vae_modules.cp_enc_dec.ContextParallelDecoder3D', 'params': {'double_z': True, 'z_channels': 16, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 2, 4], 'attn_resolutions': [], 'num_res_blocks': 3, 'dropout': 0.0, 'gather_norm': False}}}}, 'loss_fn_config': {'target': 'sgm.modules.diffusionmodules.loss.VideoDiffusionLoss', 'params': {'offset_noise_level': 0, 'sigma_sampler_config': {'target': 'sgm.modules.diffusionmodules.sigma_sampling.DiscreteSampling', 'params': {'uniform_sampling': True, 'num_idx': 1000, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 3.0}}}}}}, 'sampler_config': {'target': 'sgm.modules.diffusionmodules.sampling.VPSDEDPMPP2MSampler', 'params': {'num_steps': 50, 'verbose': True, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 3.0}}, 'guider_config': {'target': 'sgm.modules.diffusionmodules.guiders.DynamicCFG', 'params': {'scale': 6, 'exp': 5, 'num_steps': 50}}}}}}, {'args': {'checkpoint_activations': True, 'model_parallel_size': 1, 'experiment_name': 'lora-disney', 'mode': 'finetune', 'load': '/mnt/ceph/develop/jiawei/model_checkpoint/CogVideoX-2b-sat/transformer', 'no_load_rng': True, 'train_iters': 1000, 'eval_iters': 1, 'eval_interval': 100, 'eval_batch_size': 1, 'save': '/mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora', 'save_interval': 500, 'log_interval': 20, 'train_data': ['/mnt/ceph/develop/jiawei/lora_dataset/cogvideo_lora_dataset'], 'valid_data': ['/mnt/ceph/develop/jiawei/lora_dataset/cogvideo_lora_dataset'], 'split': '1,0,0', 'num_workers': 8, 'force_train': True, 'only_log_video_latents': False, 'wandb': True, 'wandb_project_name': 'cogvideox_ckpts_2b_lora', 'summary_dir': '/mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora'}, 'data': {'target': 'data_video.SFTDataset', 'params': {'video_size': [480, 720], 'fps': 8, 'max_num_frames': 49, 'skip_frms_num': 3.0}}, 'deepspeed': {'train_micro_batch_size_per_gpu': 2, 'gradient_accumulation_steps': 1, 'steps_per_print': 50, 'gradient_clipping': 0.1, 'zero_optimization': {'stage': 2, 'cpu_offload': False, 'contiguous_gradients': False, 'overlap_comm': True, 'reduce_scatter': True, 'reduce_bucket_size': 1000000000, 'allgather_bucket_size': 1000000000, 'load_from_fp32_weights': False}, 'zero_allow_untested_optimizer': True, 'bf16': {'enabled': False}, 'fp16': {'enabled': True}, 'loss_scale': 0, 'loss_scale_window': 400, 'hysteresis': 2, 'min_loss_scale': 1, 'optimizer': {'type': 'sat.ops.FusedEmaAdam', 'params': {'lr': '5E-4', 'betas': [0.9, 0.95], 'eps': '1e-8', 'weight_decay': '5E-6'}}, 'activation_checkpointing': {'partition_activations': False, 'contiguous_memory_optimization': False}, 'wall_clock_breakdown': False}}])
[2024-09-09 13:39:51,713] [INFO] [RANK 0] building SATVideoDiffusionEngine model ...
[2024-09-09 13:40:02,030] [WARNING] [RANK 0] Failed to load bitsandbytes:No module named 'bitsandbytes'
[2024-09-09 13:40:02,030] [INFO] [RANK 0] replacing layer 0 attention with lora
[2024-09-09 13:40:02,055] [INFO] [RANK 0] replacing layer 1 attention with lora
[2024-09-09 13:40:02,079] [INFO] [RANK 0] replacing layer 2 attention with lora
[2024-09-09 13:40:02,097] [INFO] [RANK 0] replacing layer 3 attention with lora
[2024-09-09 13:40:02,115] [INFO] [RANK 0] replacing layer 4 attention with lora
[2024-09-09 13:40:02,133] [INFO] [RANK 0] replacing layer 5 attention with lora
[2024-09-09 13:40:02,151] [INFO] [RANK 0] replacing layer 6 attention with lora
[2024-09-09 13:40:02,170] [INFO] [RANK 0] replacing layer 7 attention with lora
[2024-09-09 13:40:02,188] [INFO] [RANK 0] replacing layer 8 attention with lora
[2024-09-09 13:40:02,208] [INFO] [RANK 0] replacing layer 9 attention with lora
[2024-09-09 13:40:02,228] [INFO] [RANK 0] replacing layer 10 attention with lora
[2024-09-09 13:40:02,247] [INFO] [RANK 0] replacing layer 11 attention with lora
[2024-09-09 13:40:02,264] [INFO] [RANK 0] replacing layer 12 attention with lora
[2024-09-09 13:40:02,283] [INFO] [RANK 0] replacing layer 13 attention with lora
[2024-09-09 13:40:02,301] [INFO] [RANK 0] replacing layer 14 attention with lora
[2024-09-09 13:40:02,319] [INFO] [RANK 0] replacing layer 15 attention with lora
[2024-09-09 13:40:02,339] [INFO] [RANK 0] replacing layer 16 attention with lora
[2024-09-09 13:40:02,354] [INFO] [RANK 0] replacing layer 17 attention with lora
[2024-09-09 13:40:02,372] [INFO] [RANK 0] replacing layer 18 attention with lora
[2024-09-09 13:40:02,390] [INFO] [RANK 0] replacing layer 19 attention with lora
[2024-09-09 13:40:02,408] [INFO] [RANK 0] replacing layer 20 attention with lora
[2024-09-09 13:40:02,427] [INFO] [RANK 0] replacing layer 21 attention with lora
[2024-09-09 13:40:02,445] [INFO] [RANK 0] replacing layer 22 attention with lora
[2024-09-09 13:40:02,464] [INFO] [RANK 0] replacing layer 23 attention with lora
[2024-09-09 13:40:02,483] [INFO] [RANK 0] replacing layer 24 attention with lora
[2024-09-09 13:40:02,501] [INFO] [RANK 0] replacing layer 25 attention with lora
[2024-09-09 13:40:02,519] [INFO] [RANK 0] replacing layer 26 attention with lora
[2024-09-09 13:40:02,537] [INFO] [RANK 0] replacing layer 27 attention with lora
[2024-09-09 13:40:02,555] [INFO] [RANK 0] replacing layer 28 attention with lora
[2024-09-09 13:40:02,771] [INFO] [RANK 0] replacing layer 29 attention with lora
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  1.63it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.70it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.69it/s]
Initialized embedder #0: FrozenT5Embedder with 4762310656 params. Trainable: False
Working with z of shape (1, 16, 32, 32) = 16384 dimensions.
/mnt/ceph/develop/jiawei/CogVideo/sat/vae_modules/autoencoder.py:565: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  sd = torch.load(path, map_location="cpu")["state_dict"]
Deleting key loss.logvar from state_dict.
Deleting key loss.perceptual_loss.scaling_layer.shift from state_dict.
Deleting key loss.perceptual_loss.scaling_layer.scale from state_dict.
Deleting key loss.perceptual_loss.net.slice1.0.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice1.0.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice1.2.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice1.2.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice2.5.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice2.5.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice2.7.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice2.7.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice3.10.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice3.10.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice3.12.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice3.12.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice3.14.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice3.14.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice4.17.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice4.17.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice4.19.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice4.19.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice4.21.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice4.21.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice5.24.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice5.24.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice5.26.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice5.26.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice5.28.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice5.28.bias from state_dict.
Deleting key loss.perceptual_loss.lin0.model.1.weight from state_dict.
Deleting key loss.perceptual_loss.lin1.model.1.weight from state_dict.
Deleting key loss.perceptual_loss.lin2.model.1.weight from state_dict.
Deleting key loss.perceptual_loss.lin3.model.1.weight from state_dict.
Deleting key loss.perceptual_loss.lin4.model.1.weight from state_dict.
Deleting key loss.discriminator.blocks.0.downsample_res.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.0.downsample_res.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.0.net.0.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.0.net.0.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.0.net.2.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.0.net.2.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.0.downsample.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.0.downsample.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.1.downsample_res.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.1.downsample_res.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.1.net.0.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.1.net.0.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.1.net.2.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.1.net.2.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.1.downsample.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.1.downsample.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.2.downsample_res.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.2.downsample_res.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.2.net.0.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.2.net.0.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.2.net.2.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.2.net.2.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.2.downsample.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.2.downsample.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.3.downsample_res.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.3.downsample_res.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.3.net.0.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.3.net.0.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.3.net.2.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.3.net.2.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.3.downsample.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.3.downsample.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.4.0.conv_res.weight from state_dict.
Deleting key loss.discriminator.blocks.4.0.conv_res.bias from state_dict.
Deleting key loss.discriminator.blocks.4.0.net.0.weight from state_dict.
Deleting key loss.discriminator.blocks.4.0.net.0.bias from state_dict.
Deleting key loss.discriminator.blocks.4.0.net.2.weight from state_dict.
Deleting key loss.discriminator.blocks.4.0.net.2.bias from state_dict.
Deleting key loss.discriminator.blocks.4.0.downsample.1.weight from state_dict.
Deleting key loss.discriminator.blocks.4.0.downsample.1.bias from state_dict.
Deleting key loss.discriminator.blocks.4.1.0.fn.norm.gamma from state_dict.
Deleting key loss.discriminator.blocks.4.1.0.fn.attn.to_q.0.weight from state_dict.
Deleting key loss.discriminator.blocks.4.1.0.fn.attn.to_kv.0.weight from state_dict.
Deleting key loss.discriminator.blocks.4.1.0.fn.attn.to_out.0.weight from state_dict.
Deleting key loss.discriminator.blocks.4.1.1.fn.norm.gamma from state_dict.
Deleting key loss.discriminator.blocks.4.1.1.fn.net.0.weight from state_dict.
Deleting key loss.discriminator.blocks.4.1.1.fn.net.0.bias from state_dict.
Deleting key loss.discriminator.blocks.4.1.1.fn.net.2.weight from state_dict.
Deleting key loss.discriminator.blocks.4.1.1.fn.net.2.bias from state_dict.
Deleting key loss.discriminator.blocks.5.0.conv_res.weight from state_dict.
Deleting key loss.discriminator.blocks.5.0.conv_res.bias from state_dict.
Deleting key loss.discriminator.blocks.5.0.net.0.weight from state_dict.
Deleting key loss.discriminator.blocks.5.0.net.0.bias from state_dict.
Deleting key loss.discriminator.blocks.5.0.net.2.weight from state_dict.
Deleting key loss.discriminator.blocks.5.0.net.2.bias from state_dict.
Deleting key loss.discriminator.blocks.5.0.downsample.1.weight from state_dict.
Deleting key loss.discriminator.blocks.5.0.downsample.1.bias from state_dict.
Deleting key loss.discriminator.blocks.5.1.0.fn.norm.gamma from state_dict.
Deleting key loss.discriminator.blocks.5.1.0.fn.attn.to_q.0.weight from state_dict.
Deleting key loss.discriminator.blocks.5.1.0.fn.attn.to_kv.0.weight from state_dict.
Deleting key loss.discriminator.blocks.5.1.0.fn.attn.to_out.0.weight from state_dict.
Deleting key loss.discriminator.blocks.5.1.1.fn.norm.gamma from state_dict.
Deleting key loss.discriminator.blocks.5.1.1.fn.net.0.weight from state_dict.
Deleting key loss.discriminator.blocks.5.1.1.fn.net.0.bias from state_dict.
Deleting key loss.discriminator.blocks.5.1.1.fn.net.2.weight from state_dict.
Deleting key loss.discriminator.blocks.5.1.1.fn.net.2.bias from state_dict.
Deleting key loss.discriminator.blocks.6.0.conv_res.weight from state_dict.
Deleting key loss.discriminator.blocks.6.0.conv_res.bias from state_dict.
Deleting key loss.discriminator.blocks.6.0.net.0.weight from state_dict.
Deleting key loss.discriminator.blocks.6.0.net.0.bias from state_dict.
Deleting key loss.discriminator.blocks.6.0.net.2.weight from state_dict.
Deleting key loss.discriminator.blocks.6.0.net.2.bias from state_dict.
Deleting key loss.discriminator.blocks.6.1.0.fn.norm.gamma from state_dict.
Deleting key loss.discriminator.blocks.6.1.0.fn.attn.to_q.0.weight from state_dict.
Deleting key loss.discriminator.blocks.6.1.0.fn.attn.to_kv.0.weight from state_dict.
Deleting key loss.discriminator.blocks.6.1.0.fn.attn.to_out.0.weight from state_dict.
Deleting key loss.discriminator.blocks.6.1.1.fn.norm.gamma from state_dict.
Deleting key loss.discriminator.blocks.6.1.1.fn.net.0.weight from state_dict.
Deleting key loss.discriminator.blocks.6.1.1.fn.net.0.bias from state_dict.
Deleting key loss.discriminator.blocks.6.1.1.fn.net.2.weight from state_dict.
Deleting key loss.discriminator.blocks.6.1.1.fn.net.2.bias from state_dict.
Deleting key loss.discriminator.to_logits.0.weight from state_dict.
Deleting key loss.discriminator.to_logits.0.bias from state_dict.
Deleting key loss.discriminator.to_logits.3.weight from state_dict.
Deleting key loss.discriminator.to_logits.3.bias from state_dict.
Missing keys:  []
Unexpected keys:  []
Restored from /mnt/ceph/develop/jiawei/model_checkpoint/CogVideoX-2b-sat/vae/3d-vae.pt
[2024-09-09 13:40:06,854] [INFO] [RANK 0]  > number of parameters on model parallel rank 0: 6764790755
[2024-09-09 13:40:12,867] [INFO] [RANK 0] global rank 0 is loading checkpoint /mnt/ceph/develop/jiawei/model_checkpoint/CogVideoX-2b-sat/transformer/1000/mp_rank_00_model_states.pt
/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/sat/training/model_io.py:286: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  sd = torch.load(checkpoint_name, map_location='cpu')
[2024-09-09 13:40:14,935] [INFO] [RANK 0] > successfully loaded /mnt/ceph/develop/jiawei/model_checkpoint/CogVideoX-2b-sat/transformer/1000/mp_rank_00_model_states.pt
[2024-09-09 13:40:15,314] [INFO] [RANK 0] ***** Total trainable parameters: 58982400 *****
[2024-09-09 13:40:15,314] [INFO] [RANK 0] [<class 'sat.ops.layernorm.LayerNorm'>, <class 'torch.nn.modules.normalization.LayerNorm'>, <class 'sat.ops.layernorm.RMSNorm'>] is set to no_weight_decay
[2024-09-09 13:40:15,322] [INFO] [RANK 0] Syncing initialized parameters...
[2024-09-09 13:40:15,384] [INFO] [RANK 0] Finished syncing initialized parameters.
[2024-09-09 13:40:15,384] [INFO] [RANK 0] Using optimizer sat.ops.FusedEmaAdam from sat.
[2024-09-09 13:40:15,387] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.4, git-hash=unknown, git-branch=unknown
[2024-09-09 13:40:15,388] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
[2024-09-09 13:40:15,472] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /root/.cache/torch_extensions/py310_cu121/fused_ema_adam/build.ninja...
/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1965: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module fused_ema_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_ema_adam...
Time to load fused_ema_adam op: 3.4361090660095215 seconds
[2024-09-09 13:40:18,945] [INFO] [logging.py:96:log_dist] [Rank 0] Using client callable to create basic optimizer
[2024-09-09 13:40:18,948] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-09-09 13:40:18,975] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedEmaAdam
[2024-09-09 13:40:18,975] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedEmaAdam type=<class 'sat.ops.fused_ema_adam.FusedEmaAdam'>
[2024-09-09 13:40:18,978] [WARNING] [engine.py:1179:_do_optimizer_sanity_check] **** You are using ZeRO with an untested optimizer, proceed with caution *****
[2024-09-09 13:40:18,978] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2024-09-09 13:40:18,979] [INFO] [stage_1_and_2.py:148:__init__] Reduce bucket size 1000000000
[2024-09-09 13:40:18,979] [INFO] [stage_1_and_2.py:149:__init__] Allgather bucket size 1000000000
[2024-09-09 13:40:18,979] [INFO] [stage_1_and_2.py:150:__init__] CPU Offload: False
[2024-09-09 13:40:18,979] [INFO] [stage_1_and_2.py:151:__init__] Round robin gradient partitioning: False
[2024-09-09 13:40:19,231] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2024-09-09 13:40:19,231] [INFO] [utils.py:782:see_memory_usage] MA 12.86 GB         Max_MA 12.97 GB         CA 13.23 GB         Max_CA 13 GB 
[2024-09-09 13:40:19,234] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 47.66 GB, percent = 2.4%
[2024-09-09 13:40:19,377] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2024-09-09 13:40:19,378] [INFO] [utils.py:782:see_memory_usage] MA 12.86 GB         Max_MA 13.08 GB         CA 13.45 GB         Max_CA 13 GB 
[2024-09-09 13:40:19,380] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 47.67 GB, percent = 2.4%
[2024-09-09 13:40:19,382] [INFO] [stage_1_and_2.py:543:__init__] optimizer state initialized
[2024-09-09 13:40:19,517] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2024-09-09 13:40:19,517] [INFO] [utils.py:782:see_memory_usage] MA 12.86 GB         Max_MA 12.86 GB         CA 13.45 GB         Max_CA 13 GB 
[2024-09-09 13:40:19,520] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 47.67 GB, percent = 2.4%
[2024-09-09 13:40:19,528] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer
[2024-09-09 13:40:19,530] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2024-09-09 13:40:19,531] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2024-09-09 13:40:19,531] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[1.0], mom=[[0.9, 0.95]]
[2024-09-09 13:40:19,533] [INFO] [config.py:997:print] DeepSpeedEngine configuration:
[2024-09-09 13:40:19,533] [INFO] [config.py:1001:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-09-09 13:40:19,533] [INFO] [config.py:1001:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-09-09 13:40:19,533] [INFO] [config.py:1001:print]   amp_enabled .................. False
[2024-09-09 13:40:19,533] [INFO] [config.py:1001:print]   amp_params ................... False
[2024-09-09 13:40:19,536] [INFO] [config.py:1001:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-09-09 13:40:19,539] [INFO] [config.py:1001:print]   bfloat16_enabled ............. False
[2024-09-09 13:40:19,539] [INFO] [config.py:1001:print]   bfloat16_immediate_grad_update  False
[2024-09-09 13:40:19,539] [INFO] [config.py:1001:print]   checkpoint_parallel_write_pipeline  False
[2024-09-09 13:40:19,539] [INFO] [config.py:1001:print]   checkpoint_tag_validation_enabled  True
[2024-09-09 13:40:19,539] [INFO] [config.py:1001:print]   checkpoint_tag_validation_fail  False
[2024-09-09 13:40:19,541] [INFO] [config.py:1001:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f831db6bb20>
[2024-09-09 13:40:19,541] [INFO] [config.py:1001:print]   communication_data_type ...... None
[2024-09-09 13:40:19,541] [INFO] [config.py:1001:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-09-09 13:40:19,541] [INFO] [config.py:1001:print]   curriculum_enabled_legacy .... False
[2024-09-09 13:40:19,544] [INFO] [config.py:1001:print]   curriculum_params_legacy ..... False
[2024-09-09 13:40:19,544] [INFO] [config.py:1001:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-09-09 13:40:19,544] [INFO] [config.py:1001:print]   data_efficiency_enabled ...... False
[2024-09-09 13:40:19,544] [INFO] [config.py:1001:print]   dataloader_drop_last ......... False
[2024-09-09 13:40:19,544] [INFO] [config.py:1001:print]   disable_allgather ............ False
[2024-09-09 13:40:19,544] [INFO] [config.py:1001:print]   dump_state ................... False
[2024-09-09 13:40:19,544] [INFO] [config.py:1001:print]   dynamic_loss_scale_args ...... None
[2024-09-09 13:40:19,544] [INFO] [config.py:1001:print]   eigenvalue_enabled ........... False
[2024-09-09 13:40:19,544] [INFO] [config.py:1001:print]   eigenvalue_gas_boundary_resolution  1
[2024-09-09 13:40:19,547] [INFO] [config.py:1001:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-09-09 13:40:19,547] [INFO] [config.py:1001:print]   eigenvalue_layer_num ......... 0
[2024-09-09 13:40:19,547] [INFO] [config.py:1001:print]   eigenvalue_max_iter .......... 100
[2024-09-09 13:40:19,547] [INFO] [config.py:1001:print]   eigenvalue_stability ......... 1e-06
[2024-09-09 13:40:19,547] [INFO] [config.py:1001:print]   eigenvalue_tol ............... 0.01
[2024-09-09 13:40:19,547] [INFO] [config.py:1001:print]   eigenvalue_verbose ........... False
[2024-09-09 13:40:19,549] [INFO] [config.py:1001:print]   elasticity_enabled ........... False
[2024-09-09 13:40:19,549] [INFO] [config.py:1001:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-09-09 13:40:19,549] [INFO] [config.py:1001:print]   fp16_auto_cast ............... False
[2024-09-09 13:40:19,549] [INFO] [config.py:1001:print]   fp16_enabled ................. True
[2024-09-09 13:40:19,549] [INFO] [config.py:1001:print]   fp16_master_weights_and_gradients  False
[2024-09-09 13:40:19,549] [INFO] [config.py:1001:print]   global_rank .................. 0
[2024-09-09 13:40:19,549] [INFO] [config.py:1001:print]   grad_accum_dtype ............. None
[2024-09-09 13:40:19,549] [INFO] [config.py:1001:print]   gradient_accumulation_steps .. 1
[2024-09-09 13:40:19,549] [INFO] [config.py:1001:print]   gradient_clipping ............ 0.1
[2024-09-09 13:40:19,550] [INFO] [config.py:1001:print]   gradient_predivide_factor .... 1.0
[2024-09-09 13:40:19,551] [INFO] [config.py:1001:print]   graph_harvesting ............. False
[2024-09-09 13:40:19,552] [INFO] [config.py:1001:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-09-09 13:40:19,552] [INFO] [config.py:1001:print]   initial_dynamic_scale ........ 65536
[2024-09-09 13:40:19,552] [INFO] [config.py:1001:print]   load_universal_checkpoint .... False
[2024-09-09 13:40:19,552] [INFO] [config.py:1001:print]   loss_scale ................... 0
[2024-09-09 13:40:19,552] [INFO] [config.py:1001:print]   memory_breakdown ............. False
[2024-09-09 13:40:19,554] [INFO] [config.py:1001:print]   mics_hierarchial_params_gather  False
[2024-09-09 13:40:19,554] [INFO] [config.py:1001:print]   mics_shard_size .............. -1
[2024-09-09 13:40:19,554] [INFO] [config.py:1001:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-09-09 13:40:19,554] [INFO] [config.py:1001:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-09-09 13:40:19,554] [INFO] [config.py:1001:print]   optimizer_legacy_fusion ...... False
[2024-09-09 13:40:19,554] [INFO] [config.py:1001:print]   optimizer_name ............... None
[2024-09-09 13:40:19,554] [INFO] [config.py:1001:print]   optimizer_params ............. None
[2024-09-09 13:40:19,557] [INFO] [config.py:1001:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-09-09 13:40:19,557] [INFO] [config.py:1001:print]   pld_enabled .................. False
[2024-09-09 13:40:19,557] [INFO] [config.py:1001:print]   pld_params ................... False
[2024-09-09 13:40:19,557] [INFO] [config.py:1001:print]   prescale_gradients ........... False
[2024-09-09 13:40:19,557] [INFO] [config.py:1001:print]   scheduler_name ............... None
[2024-09-09 13:40:19,557] [INFO] [config.py:1001:print]   scheduler_params ............. None
[2024-09-09 13:40:19,557] [INFO] [config.py:1001:print]   seq_parallel_communication_data_type  torch.float32
[2024-09-09 13:40:19,557] [INFO] [config.py:1001:print]   sparse_attention ............. None
[2024-09-09 13:40:19,557] [INFO] [config.py:1001:print]   sparse_gradients_enabled ..... False
[2024-09-09 13:40:19,557] [INFO] [config.py:1001:print]   steps_per_print .............. 50
[2024-09-09 13:40:19,557] [INFO] [config.py:1001:print]   timers_config ................ enabled=True synchronized=True
[2024-09-09 13:40:19,559] [INFO] [config.py:1001:print]   train_batch_size ............. 2
[2024-09-09 13:40:19,559] [INFO] [config.py:1001:print]   train_micro_batch_size_per_gpu  2
[2024-09-09 13:40:19,559] [INFO] [config.py:1001:print]   use_data_before_expert_parallel_  False
[2024-09-09 13:40:19,560] [INFO] [config.py:1001:print]   use_node_local_storage ....... False
[2024-09-09 13:40:19,560] [INFO] [config.py:1001:print]   wall_clock_breakdown ......... False
[2024-09-09 13:40:19,560] [INFO] [config.py:1001:print]   weight_quantization_config ... None
[2024-09-09 13:40:19,560] [INFO] [config.py:1001:print]   world_size ................... 1
[2024-09-09 13:40:19,560] [INFO] [config.py:1001:print]   zero_allow_untested_optimizer  True
[2024-09-09 13:40:19,560] [INFO] [config.py:1001:print]   zero_config .................. stage=2 contiguous_gradients=False reduce_scatter=True reduce_bucket_size=1000000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=1000000000 overlap_comm=True load_from_fp32_weights=False elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-09-09 13:40:19,560] [INFO] [config.py:1001:print]   zero_enabled ................. True
[2024-09-09 13:40:19,560] [INFO] [config.py:1001:print]   zero_force_ds_cpu_optimizer .. True
[2024-09-09 13:40:19,560] [INFO] [config.py:1001:print]   zero_optimization_stage ...... 2
[2024-09-09 13:40:19,560] [INFO] [config.py:987:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 2, 
    "gradient_accumulation_steps": 1, 
    "steps_per_print": 50, 
    "gradient_clipping": 0.1, 
    "zero_optimization": {
        "stage": 2, 
        "cpu_offload": false, 
        "contiguous_gradients": false, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 1.000000e+09, 
        "allgather_bucket_size": 1.000000e+09, 
        "load_from_fp32_weights": false
    }, 
    "zero_allow_untested_optimizer": true, 
    "bf16": {
        "enabled": false
    }, 
    "fp16": {
        "enabled": true
    }, 
    "loss_scale": 0, 
    "loss_scale_window": 400, 
    "hysteresis": 2, 
    "min_loss_scale": 1, 
    "activation_checkpointing": {
        "partition_activations": false, 
        "contiguous_memory_optimization": false
    }, 
    "wall_clock_breakdown": false
}
[2024-09-09 13:40:19,560] [INFO] [RANK 0] learning rate decaying style linear, ratio 10.0
[2024-09-09 13:40:19,560] [INFO] [RANK 0] Finetuning Model...
[2024-09-09 13:40:19,560] [INFO] [RANK 0] arguments:
[2024-09-09 13:40:19,560] [INFO] [RANK 0]   base ......................... ['configs/cogvideox_2b_lora.yaml', 'configs/sft.yaml']
[2024-09-09 13:40:19,560] [INFO] [RANK 0]   model_parallel_size .......... 1
[2024-09-09 13:40:19,560] [INFO] [RANK 0]   force_pretrain ............... False
[2024-09-09 13:40:19,561] [INFO] [RANK 0]   device ....................... 0
[2024-09-09 13:40:19,561] [INFO] [RANK 0]   debug ........................ False
[2024-09-09 13:40:19,561] [INFO] [RANK 0]   log_image .................... True
[2024-09-09 13:40:19,561] [INFO] [RANK 0]   output_dir ................... samples
[2024-09-09 13:40:19,561] [INFO] [RANK 0]   input_dir .................... None
[2024-09-09 13:40:19,561] [INFO] [RANK 0]   input_type ................... cli
[2024-09-09 13:40:19,561] [INFO] [RANK 0]   input_file ................... input.txt
[2024-09-09 13:40:19,561] [INFO] [RANK 0]   final_size ................... 2048
[2024-09-09 13:40:19,561] [INFO] [RANK 0]   sdedit ....................... False
[2024-09-09 13:40:19,563] [INFO] [RANK 0]   grid_num_rows ................ 1
[2024-09-09 13:40:19,563] [INFO] [RANK 0]   force_inference .............. False
[2024-09-09 13:40:19,563] [INFO] [RANK 0]   lcm_steps .................... None
[2024-09-09 13:40:19,563] [INFO] [RANK 0]   sampling_num_frames .......... 32
[2024-09-09 13:40:19,563] [INFO] [RANK 0]   sampling_fps ................. 8
[2024-09-09 13:40:19,563] [INFO] [RANK 0]   only_save_latents ............ False
[2024-09-09 13:40:19,563] [INFO] [RANK 0]   only_log_video_latents ....... False
[2024-09-09 13:40:19,563] [INFO] [RANK 0]   latent_channels .............. 32
[2024-09-09 13:40:19,563] [INFO] [RANK 0]   image2video .................. False
[2024-09-09 13:40:19,563] [INFO] [RANK 0]   experiment_name .............. lora-disney-09-09-13-39
[2024-09-09 13:40:19,563] [INFO] [RANK 0]   train_iters .................. 1000
[2024-09-09 13:40:19,563] [INFO] [RANK 0]   batch_size ................... 2
[2024-09-09 13:40:19,563] [INFO] [RANK 0]   lr ........................... 0.0005
[2024-09-09 13:40:19,563] [INFO] [RANK 0]   mode ......................... finetune
[2024-09-09 13:40:19,563] [INFO] [RANK 0]   seed ......................... 42
[2024-09-09 13:40:19,563] [INFO] [RANK 0]   zero_stage ................... 0
[2024-09-09 13:40:19,563] [INFO] [RANK 0]   checkpoint_activations ....... True
[2024-09-09 13:40:19,563] [INFO] [RANK 0]   checkpoint_num_layers ........ 1
[2024-09-09 13:40:19,563] [INFO] [RANK 0]   checkpoint_skip_layers ....... 0
[2024-09-09 13:40:19,563] [INFO] [RANK 0]   fp16 ......................... True
[2024-09-09 13:40:19,563] [INFO] [RANK 0]   bf16 ......................... False
[2024-09-09 13:40:19,563] [INFO] [RANK 0]   gradient_accumulation_steps .. 1
[2024-09-09 13:40:19,566] [INFO] [RANK 0]   profiling .................... -1
[2024-09-09 13:40:19,566] [INFO] [RANK 0]   epochs ....................... None
[2024-09-09 13:40:19,566] [INFO] [RANK 0]   log_interval ................. 20
[2024-09-09 13:40:19,566] [INFO] [RANK 0]   summary_dir .................. /mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora
[2024-09-09 13:40:19,566] [INFO] [RANK 0]   save_args .................... False
[2024-09-09 13:40:19,566] [INFO] [RANK 0]   lr_decay_iters ............... None
[2024-09-09 13:40:19,566] [INFO] [RANK 0]   lr_decay_style ............... linear
[2024-09-09 13:40:19,566] [INFO] [RANK 0]   lr_decay_ratio ............... 0.1
[2024-09-09 13:40:19,566] [INFO] [RANK 0]   warmup ....................... 0.01
[2024-09-09 13:40:19,566] [INFO] [RANK 0]   weight_decay ................. 5e-06
[2024-09-09 13:40:19,566] [INFO] [RANK 0]   save ......................... /mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora/lora-disney-09-09-13-39
[2024-09-09 13:40:19,566] [INFO] [RANK 0]   load ......................... /mnt/ceph/develop/jiawei/model_checkpoint/CogVideoX-2b-sat/transformer
[2024-09-09 13:40:19,569] [INFO] [RANK 0]   force_train .................. True
[2024-09-09 13:40:19,569] [INFO] [RANK 0]   save_interval ................ 500
[2024-09-09 13:40:19,569] [INFO] [RANK 0]   no_save_rng .................. False
[2024-09-09 13:40:19,569] [INFO] [RANK 0]   no_load_rng .................. True
[2024-09-09 13:40:19,572] [INFO] [RANK 0]   resume_dataloader ............ False
[2024-09-09 13:40:19,572] [INFO] [RANK 0]   distributed_backend .......... nccl
[2024-09-09 13:40:19,572] [INFO] [RANK 0]   local_rank ................... 0
[2024-09-09 13:40:19,572] [INFO] [RANK 0]   exit_interval ................ None
[2024-09-09 13:40:19,572] [INFO] [RANK 0]   wandb ........................ True
[2024-09-09 13:40:19,572] [INFO] [RANK 0]   wandb_project_name ........... cogvideox_ckpts_2b_lora
[2024-09-09 13:40:19,572] [INFO] [RANK 0]   eval_batch_size .............. 1
[2024-09-09 13:40:19,572] [INFO] [RANK 0]   eval_iters ................... 1
[2024-09-09 13:40:19,572] [INFO] [RANK 0]   eval_interval ................ 100
[2024-09-09 13:40:19,572] [INFO] [RANK 0]   strict_eval .................. False
[2024-09-09 13:40:19,572] [INFO] [RANK 0]   train_data ................... ['/mnt/ceph/develop/jiawei/lora_dataset/cogvideo_lora_dataset']
[2024-09-09 13:40:19,575] [INFO] [RANK 0]   train_data_weights ........... None
[2024-09-09 13:40:19,575] [INFO] [RANK 0]   iterable_dataset ............. False
[2024-09-09 13:40:19,575] [INFO] [RANK 0]   iterable_dataset_eval ........ 
[2024-09-09 13:40:19,575] [INFO] [RANK 0]   batch_from_same_dataset ...... False
[2024-09-09 13:40:19,575] [INFO] [RANK 0]   valid_data ................... ['/mnt/ceph/develop/jiawei/lora_dataset/cogvideo_lora_dataset']
[2024-09-09 13:40:19,575] [INFO] [RANK 0]   test_data .................... None
[2024-09-09 13:40:19,575] [INFO] [RANK 0]   split ........................ 1,0,0
[2024-09-09 13:40:19,575] [INFO] [RANK 0]   num_workers .................. 8
[2024-09-09 13:40:19,575] [INFO] [RANK 0]   block_size ................... 10000
[2024-09-09 13:40:19,575] [INFO] [RANK 0]   prefetch_factor .............. 4
[2024-09-09 13:40:19,575] [INFO] [RANK 0]   deepspeed .................... True
[2024-09-09 13:40:19,575] [INFO] [RANK 0]   deepspeed_config ............. {'train_micro_batch_size_per_gpu': 2, 'gradient_accumulation_steps': 1, 'steps_per_print': 50, 'gradient_clipping': 0.1, 'zero_optimization': {'stage': 2, 'cpu_offload': False, 'contiguous_gradients': False, 'overlap_comm': True, 'reduce_scatter': True, 'reduce_bucket_size': 1000000000, 'allgather_bucket_size': 1000000000, 'load_from_fp32_weights': False}, 'zero_allow_untested_optimizer': True, 'bf16': {'enabled': False}, 'fp16': {'enabled': True}, 'loss_scale': 0, 'loss_scale_window': 400, 'hysteresis': 2, 'min_loss_scale': 1, 'activation_checkpointing': {'partition_activations': False, 'contiguous_memory_optimization': False}, 'wall_clock_breakdown': False}
[2024-09-09 13:40:19,575] [INFO] [RANK 0]   deepscale .................... False
[2024-09-09 13:40:19,575] [INFO] [RANK 0]   deepscale_config ............. None
[2024-09-09 13:40:19,578] [INFO] [RANK 0]   model_config ................. {'scale_factor': 1.15258426, 'disable_first_stage_autocast': True, 'not_trainable_prefixes': ['all'], 'log_keys': ['txt'], 'denoiser_config': {'target': 'sgm.modules.diffusionmodules.denoiser.DiscreteDenoiser', 'params': {'num_idx': 1000, 'quantize_c_noise': False, 'weighting_config': {'target': 'sgm.modules.diffusionmodules.denoiser_weighting.EpsWeighting'}, 'scaling_config': {'target': 'sgm.modules.diffusionmodules.denoiser_scaling.VideoScaling'}, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 3.0}}}}, 'network_config': {'target': 'dit_video_concat.DiffusionTransformer', 'params': {'time_embed_dim': 512, 'elementwise_affine': True, 'num_frames': 49, 'time_compressed_rate': 4, 'latent_width': 90, 'latent_height': 60, 'num_layers': 30, 'patch_size': 2, 'in_channels': 16, 'out_channels': 16, 'hidden_size': 1920, 'adm_in_channels': 256, 'num_attention_heads': 30, 'transformer_args': {'checkpoint_activations': True, 'vocab_size': 1, 'max_sequence_length': 64, 'layernorm_order': 'pre', 'skip_init': False, 'model_parallel_size': 1, 'is_decoder': False, 'num_layers': 30, 'hidden_size': 1920, 'num_attention_heads': 30, 'parallel_output': True}, 'modules': {'pos_embed_config': {'target': 'dit_video_concat.Basic3DPositionEmbeddingMixin', 'params': {'text_length': 226, 'height_interpolation': 1.875, 'width_interpolation': 1.875}}, 'lora_config': {'target': 'sat.model.finetune.lora2.LoraMixin', 'params': {'r': 128}}, 'patch_embed_config': {'target': 'dit_video_concat.ImagePatchEmbeddingMixin', 'params': {'text_hidden_size': 4096}}, 'adaln_layer_config': {'target': 'dit_video_concat.AdaLNMixin', 'params': {'qk_ln': True}}, 'final_layer_config': {'target': 'dit_video_concat.FinalLayerMixin'}}, 'dtype': 'fp16'}}, 'conditioner_config': {'target': 'sgm.modules.GeneralConditioner', 'params': {'emb_models': [{'is_trainable': False, 'input_key': 'txt', 'ucg_rate': 0.1, 'target': 'sgm.modules.encoders.modules.FrozenT5Embedder', 'params': {'model_dir': '/mnt/ceph/develop/jiawei/model_checkpoint/t5-v1_1-xxl', 'max_length': 226}}]}}, 'first_stage_config': {'target': 'vae_modules.autoencoder.VideoAutoencoderInferenceWrapper', 'params': {'cp_size': 1, 'ckpt_path': '/mnt/ceph/develop/jiawei/model_checkpoint/CogVideoX-2b-sat/vae/3d-vae.pt', 'ignore_keys': ['loss'], 'loss_config': {'target': 'torch.nn.Identity'}, 'regularizer_config': {'target': 'vae_modules.regularizers.DiagonalGaussianRegularizer'}, 'encoder_config': {'target': 'vae_modules.cp_enc_dec.ContextParallelEncoder3D', 'params': {'double_z': True, 'z_channels': 16, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 2, 4], 'attn_resolutions': [], 'num_res_blocks': 3, 'dropout': 0.0, 'gather_norm': True}}, 'decoder_config': {'target': 'vae_modules.cp_enc_dec.ContextParallelDecoder3D', 'params': {'double_z': True, 'z_channels': 16, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 2, 4], 'attn_resolutions': [], 'num_res_blocks': 3, 'dropout': 0.0, 'gather_norm': False}}}}, 'loss_fn_config': {'target': 'sgm.modules.diffusionmodules.loss.VideoDiffusionLoss', 'params': {'offset_noise_level': 0, 'sigma_sampler_config': {'target': 'sgm.modules.diffusionmodules.sigma_sampling.DiscreteSampling', 'params': {'uniform_sampling': True, 'num_idx': 1000, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 3.0}}}}}}, 'sampler_config': {'target': 'sgm.modules.diffusionmodules.sampling.VPSDEDPMPP2MSampler', 'params': {'num_steps': 50, 'verbose': True, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 3.0}}, 'guider_config': {'target': 'sgm.modules.diffusionmodules.guiders.DynamicCFG', 'params': {'scale': 6, 'exp': 5, 'num_steps': 50}}}}}
[2024-09-09 13:40:19,578] [INFO] [RANK 0]   data_config .................. {'target': 'data_video.SFTDataset', 'params': {'video_size': [480, 720], 'fps': 8, 'max_num_frames': 49, 'skip_frms_num': 3.0}}
[2024-09-09 13:40:19,578] [INFO] [RANK 0]   cuda ......................... True
[2024-09-09 13:40:19,578] [INFO] [RANK 0]   rank ......................... 0
[2024-09-09 13:40:19,580] [INFO] [RANK 0]   world_size ................... 1
[2024-09-09 13:40:19,580] [INFO] [RANK 0]   deepspeed_activation_checkpointing  True
[2024-09-09 13:40:19,580] [INFO] [RANK 0]   master_ip .................... nm04-a800-node083
[2024-09-09 13:40:19,580] [INFO] [RANK 0]   master_port .................. 50933
[2024-09-09 13:40:19,581] [INFO] [RANK 0]   log_config ................... [{'model': {'scale_factor': 1.15258426, 'disable_first_stage_autocast': True, 'not_trainable_prefixes': ['all'], 'log_keys': ['txt'], 'denoiser_config': {'target': 'sgm.modules.diffusionmodules.denoiser.DiscreteDenoiser', 'params': {'num_idx': 1000, 'quantize_c_noise': False, 'weighting_config': {'target': 'sgm.modules.diffusionmodules.denoiser_weighting.EpsWeighting'}, 'scaling_config': {'target': 'sgm.modules.diffusionmodules.denoiser_scaling.VideoScaling'}, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 3.0}}}}, 'network_config': {'target': 'dit_video_concat.DiffusionTransformer', 'params': {'time_embed_dim': 512, 'elementwise_affine': True, 'num_frames': 49, 'time_compressed_rate': 4, 'latent_width': 90, 'latent_height': 60, 'num_layers': 30, 'patch_size': 2, 'in_channels': 16, 'out_channels': 16, 'hidden_size': 1920, 'adm_in_channels': 256, 'num_attention_heads': 30, 'transformer_args': {'checkpoint_activations': True, 'vocab_size': 1, 'max_sequence_length': 64, 'layernorm_order': 'pre', 'skip_init': False, 'model_parallel_size': 1, 'is_decoder': False}, 'modules': {'pos_embed_config': {'target': 'dit_video_concat.Basic3DPositionEmbeddingMixin', 'params': {'text_length': 226, 'height_interpolation': 1.875, 'width_interpolation': 1.875}}, 'lora_config': {'target': 'sat.model.finetune.lora2.LoraMixin', 'params': {'r': 128}}, 'patch_embed_config': {'target': 'dit_video_concat.ImagePatchEmbeddingMixin', 'params': {'text_hidden_size': 4096}}, 'adaln_layer_config': {'target': 'dit_video_concat.AdaLNMixin', 'params': {'qk_ln': True}}, 'final_layer_config': {'target': 'dit_video_concat.FinalLayerMixin'}}}}, 'conditioner_config': {'target': 'sgm.modules.GeneralConditioner', 'params': {'emb_models': [{'is_trainable': False, 'input_key': 'txt', 'ucg_rate': 0.1, 'target': 'sgm.modules.encoders.modules.FrozenT5Embedder', 'params': {'model_dir': '/mnt/ceph/develop/jiawei/model_checkpoint/t5-v1_1-xxl', 'max_length': 226}}]}}, 'first_stage_config': {'target': 'vae_modules.autoencoder.VideoAutoencoderInferenceWrapper', 'params': {'cp_size': 1, 'ckpt_path': '/mnt/ceph/develop/jiawei/model_checkpoint/CogVideoX-2b-sat/vae/3d-vae.pt', 'ignore_keys': ['loss'], 'loss_config': {'target': 'torch.nn.Identity'}, 'regularizer_config': {'target': 'vae_modules.regularizers.DiagonalGaussianRegularizer'}, 'encoder_config': {'target': 'vae_modules.cp_enc_dec.ContextParallelEncoder3D', 'params': {'double_z': True, 'z_channels': 16, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 2, 4], 'attn_resolutions': [], 'num_res_blocks': 3, 'dropout': 0.0, 'gather_norm': True}}, 'decoder_config': {'target': 'vae_modules.cp_enc_dec.ContextParallelDecoder3D', 'params': {'double_z': True, 'z_channels': 16, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 2, 4], 'attn_resolutions': [], 'num_res_blocks': 3, 'dropout': 0.0, 'gather_norm': False}}}}, 'loss_fn_config': {'target': 'sgm.modules.diffusionmodules.loss.VideoDiffusionLoss', 'params': {'offset_noise_level': 0, 'sigma_sampler_config': {'target': 'sgm.modules.diffusionmodules.sigma_sampling.DiscreteSampling', 'params': {'uniform_sampling': True, 'num_idx': 1000, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 3.0}}}}}}, 'sampler_config': {'target': 'sgm.modules.diffusionmodules.sampling.VPSDEDPMPP2MSampler', 'params': {'num_steps': 50, 'verbose': True, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 3.0}}, 'guider_config': {'target': 'sgm.modules.diffusionmodules.guiders.DynamicCFG', 'params': {'scale': 6, 'exp': 5, 'num_steps': 50}}}}}}, {'args': {'checkpoint_activations': True, 'model_parallel_size': 1, 'experiment_name': 'lora-disney', 'mode': 'finetune', 'load': '/mnt/ceph/develop/jiawei/model_checkpoint/CogVideoX-2b-sat/transformer', 'no_load_rng': True, 'train_iters': 1000, 'eval_iters': 1, 'eval_interval': 100, 'eval_batch_size': 1, 'save': '/mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora', 'save_interval': 500, 'log_interval': 20, 'train_data': ['/mnt/ceph/develop/jiawei/lora_dataset/cogvideo_lora_dataset'], 'valid_data': ['/mnt/ceph/develop/jiawei/lora_dataset/cogvideo_lora_dataset'], 'split': '1,0,0', 'num_workers': 8, 'force_train': True, 'only_log_video_latents': False, 'wandb': True, 'wandb_project_name': 'cogvideox_ckpts_2b_lora', 'summary_dir': '/mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora'}, 'data': {'target': 'data_video.SFTDataset', 'params': {'video_size': [480, 720], 'fps': 8, 'max_num_frames': 49, 'skip_frms_num': 3.0}}, 'deepspeed': {'train_micro_batch_size_per_gpu': 2, 'gradient_accumulation_steps': 1, 'steps_per_print': 50, 'gradient_clipping': 0.1, 'zero_optimization': {'stage': 2, 'cpu_offload': False, 'contiguous_gradients': False, 'overlap_comm': True, 'reduce_scatter': True, 'reduce_bucket_size': 1000000000, 'allgather_bucket_size': 1000000000, 'load_from_fp32_weights': False}, 'zero_allow_untested_optimizer': True, 'bf16': {'enabled': False}, 'fp16': {'enabled': True}, 'loss_scale': 0, 'loss_scale_window': 400, 'hysteresis': 2, 'min_loss_scale': 1, 'optimizer': {'type': 'sat.ops.FusedEmaAdam', 'params': {'lr': '5E-4', 'betas': [0.9, 0.95], 'eps': '1e-8', 'weight_decay': '5E-6'}}, 'activation_checkpointing': {'partition_activations': False, 'contiguous_memory_optimization': False}, 'wall_clock_breakdown': False}}]
[2024-09-09 13:40:19,581] [INFO] [RANK 0]   do_train ..................... True
[2024-09-09 13:40:19,581] [INFO] [RANK 0]   val_last_shape ............... []
[2024-09-09 13:40:19,581] [INFO] [RANK 0]   val_drop_number .............. 0
[2024-09-09 13:40:19,581] [INFO] [RANK 0]   do_valid ..................... True
[2024-09-09 13:40:19,584] [INFO] [RANK 0]   do_test ...................... False
[2024-09-09 13:40:19,584] [INFO] [RANK 0]   iteration .................... 0
wandb: Currently logged in as: dmeck. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora/wandb/run-20240909_134023-zcligh48
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lora-disney-09-09-13-39
wandb: â­ï¸ View project at https://wandb.ai/dmeck/cogvideox_ckpts_2b_lora
wandb: ðŸš€ View run at https://wandb.ai/dmeck/cogvideox_ckpts_2b_lora/runs/zcligh48
[2024-09-09 13:40:58,981] [INFO] [checkpointing.py:541:forward] Activation Checkpointing Information
[2024-09-09 13:40:58,981] [INFO] [checkpointing.py:542:forward] ----Partition Activations False, CPU CHECKPOINTING False
[2024-09-09 13:40:58,985] [INFO] [checkpointing.py:543:forward] ----contiguous Memory Checkpointing False with None total layers
[2024-09-09 13:40:58,985] [INFO] [checkpointing.py:545:forward] ----Synchronization False
[2024-09-09 13:40:58,985] [INFO] [checkpointing.py:546:forward] ----Profiling time in checkpointing False
[2024-09-09 13:41:05,481] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4294967296, reducing to 2147483648
[2024-09-09 13:41:20,454] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2147483648, reducing to 1073741824
[2024-09-09 13:41:35,629] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1073741824, reducing to 536870912
[2024-09-09 13:42:06,180] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 536870912, reducing to 268435456
[2024-09-09 13:43:51,595] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 268435456, reducing to 134217728
[2024-09-09 13:45:53,225] [INFO] [RANK 0]  iteration       20/    1000 | elapsed time per iteration (ms): 16163.8 | learning rate 2.500E-05 | total loss 2.303816E-01 | loss 2.303816E-01 | loss scale 134217728.0 |speed 7.42 samples/(min*GPU)
[2024-09-09 13:45:53,227] [INFO] [RANK 0] after 20 iterations memory (MB) | allocated: 13975.08984375 | max allocated: 64454.34912109375 | cached: 22772.0 | max cached: 77966.0
[2024-09-09 13:45:53,230] [INFO] [RANK 0] time (ms) | forward: 11235.47 | backward: 4902.12 | allreduce: 0.00 | optimizer: 25.16 | data loader: 107.58
[2024-09-09 13:46:38,591] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 134217728, reducing to 67108864
[2024-09-09 13:47:54,409] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 67108864, reducing to 33554432
[2024-09-09 13:49:55,329] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 33554432, reducing to 16777216
[2024-09-09 13:50:55,958] [INFO] [RANK 0]  iteration       40/    1000 | elapsed time per iteration (ms): 15136.7 | learning rate 2.500E-05 | total loss 1.918435E-01 | loss 1.918435E-01 | loss scale 16777216.0 |speed 7.93 samples/(min*GPU)
[2024-09-09 13:50:55,961] [INFO] [RANK 0] time (ms) | forward: 10241.91 | backward: 4867.12 | allreduce: 0.00 | optimizer: 26.27 | data loader: 0.36
[2024-09-09 13:53:27,731] [INFO] [logging.py:96:log_dist] [Rank 0] step=50, skipped=8, lr=[2.5e-05], mom=[[0.9, 0.95]]
[2024-09-09 13:55:59,769] [INFO] [RANK 0]  iteration       60/    1000 | elapsed time per iteration (ms): 15190.5 | learning rate 2.500E-05 | total loss 1.934066E-01 | loss 1.934066E-01 | loss scale 16777216.0 |speed 7.90 samples/(min*GPU)
[2024-09-09 13:55:59,771] [INFO] [RANK 0] time (ms) | forward: 10279.55 | backward: 4881.88 | allreduce: 0.00 | optimizer: 27.89 | data loader: 0.31
[2024-09-09 13:58:01,534] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16777216, reducing to 8388608
[2024-09-09 14:01:04,075] [INFO] [RANK 0]  iteration       80/    1000 | elapsed time per iteration (ms): 15215.3 | learning rate 2.500E-05 | total loss 2.473808E-01 | loss 2.473808E-01 | loss scale 8388608.0 |speed 7.89 samples/(min*GPU)
[2024-09-09 14:01:04,076] [INFO] [RANK 0] time (ms) | forward: 10313.96 | backward: 4870.68 | allreduce: 0.00 | optimizer: 29.19 | data loader: 0.36
[2024-09-09 14:06:07,276] [INFO] [logging.py:96:log_dist] [Rank 0] step=100, skipped=9, lr=[2.5e-05], mom=[[0.9, 0.95]]
[2024-09-09 14:06:07,277] [INFO] [RANK 0]  iteration      100/    1000 | elapsed time per iteration (ms): 15160.1 | learning rate 2.500E-05 | total loss 2.256814E-01 | loss 2.256814E-01 | loss scale 8388608.0 |speed 7.92 samples/(min*GPU)
[2024-09-09 14:06:07,278] [INFO] [RANK 0] time (ms) | forward: 10259.15 | backward: 4871.01 | allreduce: 0.00 | optimizer: 28.66 | data loader: 0.31
/mnt/ceph/develop/jiawei/CogVideo/sat/train_video.py:67: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at ../torch/csrc/autograd/init.cpp:733.)
  "dtype": torch.get_autocast_gpu_dtype(),
/mnt/ceph/develop/jiawei/CogVideo/sat/train_video.py:70: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.no_grad(), torch.cuda.amp.autocast(**gpu_autocast_kwargs):
##############################  Sampling setting  ##############################
Sampler: VPSDEDPMPP2MSampler
Discretization: ZeroSNRDDPMDiscretization
Guider: DynamicCFG
Sampling with VPSDEDPMPP2MSampler for 51 steps:   0%|          | 0/51 [00:00<?, ?it/s]Sampling with VPSDEDPMPP2MSampler for 51 steps:   2%|â–         | 1/51 [00:01<01:20,  1.61s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:   4%|â–         | 2/51 [00:03<01:21,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:   6%|â–Œ         | 3/51 [00:04<01:19,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:   8%|â–Š         | 4/51 [00:06<01:17,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  10%|â–‰         | 5/51 [00:08<01:16,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  12%|â–ˆâ–        | 6/51 [00:09<01:14,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  14%|â–ˆâ–Ž        | 7/51 [00:11<01:12,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  16%|â–ˆâ–Œ        | 8/51 [00:13<01:11,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  18%|â–ˆâ–Š        | 9/51 [00:14<01:09,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  20%|â–ˆâ–‰        | 10/51 [00:16<01:07,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  22%|â–ˆâ–ˆâ–       | 11/51 [00:18<01:06,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  24%|â–ˆâ–ˆâ–Ž       | 12/51 [00:19<01:04,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  25%|â–ˆâ–ˆâ–Œ       | 13/51 [00:21<01:02,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  27%|â–ˆâ–ˆâ–‹       | 14/51 [00:23<01:01,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  29%|â–ˆâ–ˆâ–‰       | 15/51 [00:24<00:59,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  31%|â–ˆâ–ˆâ–ˆâ–      | 16/51 [00:26<00:58,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 17/51 [00:28<00:56,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 18/51 [00:29<00:54,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 19/51 [00:31<00:53,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 20/51 [00:33<00:51,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 21/51 [00:34<00:49,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 22/51 [00:36<00:48,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 23/51 [00:38<00:46,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 24/51 [00:39<00:44,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 25/51 [00:41<00:43,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 26/51 [00:43<00:41,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 27/51 [00:44<00:39,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 28/51 [00:46<00:38,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 29/51 [00:48<00:36,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 30/51 [00:49<00:34,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 31/51 [00:51<00:33,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 32/51 [00:53<00:31,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33/51 [00:54<00:29,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 34/51 [00:56<00:28,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 35/51 [00:57<00:26,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 36/51 [00:59<00:24,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 37/51 [01:01<00:23,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/51 [01:02<00:21,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 39/51 [01:04<00:19,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 40/51 [01:06<00:18,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 41/51 [01:07<00:16,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 42/51 [01:09<00:14,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 43/51 [01:11<00:13,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 44/51 [01:12<00:11,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 45/51 [01:14<00:09,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 46/51 [01:16<00:08,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 47/51 [01:17<00:06,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 48/51 [01:19<00:04,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 49/51 [01:21<00:03,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 50/51 [01:22<00:01,  1.65s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 50/51 [01:22<00:01,  1.66s/it]
[2024-09-09 14:08:04,947] [INFO] [RANK 0] ----------------------------------------------------------------------------------------------------
[2024-09-09 14:08:04,947] [INFO] [RANK 0] -----------------------------------------------------------------------------------------------
[2024-09-09 14:08:04,951] [INFO] [RANK 0]  validation loss at iteration 100 | loss: 3.070267E-01 | PPL: 1.359377E+00 loss 3.070267E-01 |
[2024-09-09 14:08:04,951] [INFO] [RANK 0] -----------------------------------------------------------------------------------------------
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 100 is less than current step: 101. Dropping entry: {'Train/valid_ppl': 1.359377282109275, 'Train/valid_loss': 0.3070267140865326, '_timestamp': 1725862084.951632}).
[2024-09-09 14:11:48,814] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8388608, reducing to 4194304
E0909 14:12:25.943000 140234156545856 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: -9) local_rank: 0 (pid: 1782727) of binary: /mnt/ceph/develop/jiawei/conda_env/cogvideo/bin/python
Traceback (most recent call last):
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
========================================================
train_video.py FAILED
--------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
--------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-09-09_14:12:25
  host      : nm04-a800-node083
  rank      : 0 (local_rank: 0)
  exitcode  : -9 (pid: 1782727)
  error_file: <N/A>
  traceback : Signal 9 (SIGKILL) received by PID 1782727
========================================================
DONE on nm04-a800-node083
RUN on nm04-a800-node083, CUDA_VISIBLE_DEVICES=0
torchrun --standalone --nproc_per_node=1 train_video.py --base configs/cogvideox_2b_lora.yaml configs/sft.yaml --seed 42
[2024-09-09 14:14:56,358] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
[93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, input, weight, bias=None):
/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/kornia/feature/lightglue.py:44: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)
no module 'xformers'. Processing without...
no module 'xformers'. Processing without...
Traceback (most recent call last):
  File "/mnt/ceph/develop/jiawei/CogVideo/sat/train_video.py", line 208, in <module>
    args = get_args(args_list)
  File "/mnt/ceph/develop/jiawei/CogVideo/sat/arguments.py", line 71, in get_args
    args = process_config_to_args(args)
  File "/mnt/ceph/develop/jiawei/CogVideo/sat/arguments.py", line 259, in process_config_to_args
    configs = [OmegaConf.load(cfg) for cfg in args.base]
  File "/mnt/ceph/develop/jiawei/CogVideo/sat/arguments.py", line 259, in <listcomp>
    configs = [OmegaConf.load(cfg) for cfg in args.base]
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/omegaconf/omegaconf.py", line 190, in load
    obj = yaml.load(f, Loader=get_yaml_loader())
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/yaml/__init__.py", line 81, in load
    return loader.get_single_data()
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/yaml/constructor.py", line 51, in get_single_data
    return self.construct_document(node)
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/yaml/constructor.py", line 60, in construct_document
    for dummy in generator:
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/yaml/constructor.py", line 413, in construct_yaml_map
    value = self.construct_mapping(node)
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/omegaconf/_utils.py", line 144, in construct_mapping
    raise yaml.constructor.ConstructorError(
yaml.constructor.ConstructorError: while constructing a mapping
  in "/mnt/ceph/develop/jiawei/CogVideo/sat/configs/sft.yaml", line 2, column 3
found duplicate key experiment_name
  in "/mnt/ceph/develop/jiawei/CogVideo/sat/configs/sft.yaml", line 23, column 3
E0909 14:15:18.550000 139793904850752 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 0 (pid: 1848264) of binary: /mnt/ceph/develop/jiawei/conda_env/cogvideo/bin/python
Traceback (most recent call last):
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train_video.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-09-09_14:15:18
  host      : nm04-a800-node083
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 1848264)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
DONE on nm04-a800-node083
RUN on nm04-a800-node083, CUDA_VISIBLE_DEVICES=0
torchrun --standalone --nproc_per_node=1 train_video.py --base configs/cogvideox_2b_lora.yaml configs/sft.yaml --seed 42
[2024-09-09 14:16:21,705] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
[93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, input, weight, bias=None):
/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/kornia/feature/lightglue.py:44: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)
no module 'xformers'. Processing without...
no module 'xformers'. Processing without...
[2024-09-09 14:16:38,069] [INFO] using world size: 1
[2024-09-09 14:16:38,070] [INFO] Will override arguments with manually specified deepspeed_config!
[2024-09-09 14:16:38,076] [INFO] [RANK 0] > initializing model parallel with size 1
[2024-09-09 14:16:38,077] [INFO] [comm.py:637:init_distributed] cdb=None
Namespace(base=['configs/cogvideox_2b_lora.yaml', 'configs/sft.yaml'], model_parallel_size=1, force_pretrain=False, device=0, debug=False, log_image=True, output_dir='samples', input_dir=None, input_type='cli', input_file='input.txt', final_size=2048, sdedit=False, grid_num_rows=1, force_inference=False, lcm_steps=None, sampling_num_frames=32, sampling_fps=8, only_save_latents=False, only_log_video_latents=False, latent_channels=32, image2video=False, experiment_name='lora-disney', train_iters=1000, batch_size=2, lr=0.001, mode='finetune', seed=42, zero_stage=0, checkpoint_activations=True, checkpoint_num_layers=1, checkpoint_skip_layers=0, fp16=True, bf16=False, gradient_accumulation_steps=1, profiling=-1, epochs=None, log_interval=20, summary_dir='/mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora', save_args=False, lr_decay_iters=None, lr_decay_style='linear', lr_decay_ratio=0.1, warmup=0.01, weight_decay=0.0001, save='/mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora', load='/mnt/ceph/develop/jiawei/model_checkpoint/CogVideoX-2b-sat/transformer', force_train=True, save_interval=500, no_save_rng=False, no_load_rng=True, resume_dataloader=False, distributed_backend='nccl', local_rank=0, exit_interval=None, wandb=True, wandb_project_name='cogvideox_ckpts_2b_lora', eval_batch_size=1, eval_iters=1, eval_interval=100, strict_eval=False, train_data=['/mnt/ceph/develop/jiawei/lora_dataset/cogvideo_lora_dataset'], train_data_weights=None, iterable_dataset=False, iterable_dataset_eval='', batch_from_same_dataset=False, valid_data=['/mnt/ceph/develop/jiawei/lora_dataset/cogvideo_lora_dataset'], test_data=None, split='1,0,0', num_workers=8, block_size=10000, prefetch_factor=4, deepspeed=True, deepspeed_config={'train_micro_batch_size_per_gpu': 2, 'gradient_accumulation_steps': 1, 'steps_per_print': 50, 'gradient_clipping': 0.1, 'zero_optimization': {'stage': 2, 'cpu_offload': False, 'contiguous_gradients': False, 'overlap_comm': True, 'reduce_scatter': True, 'reduce_bucket_size': 1000000000, 'allgather_bucket_size': 1000000000, 'load_from_fp32_weights': False}, 'zero_allow_untested_optimizer': True, 'bf16': {'enabled': False}, 'fp16': {'enabled': True}, 'loss_scale': 0, 'loss_scale_window': 400, 'hysteresis': 2, 'min_loss_scale': 1, 'optimizer': {'type': 'sat.ops.FusedEmaAdam', 'params': {'lr': 0.001, 'betas': [0.9, 0.95], 'eps': 1e-08, 'weight_decay': 0.0001}}, 'activation_checkpointing': {'partition_activations': False, 'contiguous_memory_optimization': False}, 'wall_clock_breakdown': False}, deepscale=False, deepscale_config=None, model_config={'scale_factor': 1.15258426, 'disable_first_stage_autocast': True, 'not_trainable_prefixes': ['all'], 'log_keys': ['txt'], 'denoiser_config': {'target': 'sgm.modules.diffusionmodules.denoiser.DiscreteDenoiser', 'params': {'num_idx': 1000, 'quantize_c_noise': False, 'weighting_config': {'target': 'sgm.modules.diffusionmodules.denoiser_weighting.EpsWeighting'}, 'scaling_config': {'target': 'sgm.modules.diffusionmodules.denoiser_scaling.VideoScaling'}, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 3.0}}}}, 'network_config': {'target': 'dit_video_concat.DiffusionTransformer', 'params': {'time_embed_dim': 512, 'elementwise_affine': True, 'num_frames': 49, 'time_compressed_rate': 4, 'latent_width': 90, 'latent_height': 60, 'num_layers': 30, 'patch_size': 2, 'in_channels': 16, 'out_channels': 16, 'hidden_size': 1920, 'adm_in_channels': 256, 'num_attention_heads': 30, 'transformer_args': {'checkpoint_activations': True, 'vocab_size': 1, 'max_sequence_length': 64, 'layernorm_order': 'pre', 'skip_init': False, 'model_parallel_size': 1, 'is_decoder': False}, 'modules': {'pos_embed_config': {'target': 'dit_video_concat.Basic3DPositionEmbeddingMixin', 'params': {'text_length': 226, 'height_interpolation': 1.875, 'width_interpolation': 1.875}}, 'lora_config': {'target': 'sat.model.finetune.lora2.LoraMixin', 'params': {'r': 128}}, 'patch_embed_config': {'target': 'dit_video_concat.ImagePatchEmbeddingMixin', 'params': {'text_hidden_size': 4096}}, 'adaln_layer_config': {'target': 'dit_video_concat.AdaLNMixin', 'params': {'qk_ln': True}}, 'final_layer_config': {'target': 'dit_video_concat.FinalLayerMixin'}}}}, 'conditioner_config': {'target': 'sgm.modules.GeneralConditioner', 'params': {'emb_models': [{'is_trainable': False, 'input_key': 'txt', 'ucg_rate': 0.1, 'target': 'sgm.modules.encoders.modules.FrozenT5Embedder', 'params': {'model_dir': '/mnt/ceph/develop/jiawei/model_checkpoint/t5-v1_1-xxl', 'max_length': 226}}]}}, 'first_stage_config': {'target': 'vae_modules.autoencoder.VideoAutoencoderInferenceWrapper', 'params': {'cp_size': 1, 'ckpt_path': '/mnt/ceph/develop/jiawei/model_checkpoint/CogVideoX-2b-sat/vae/3d-vae.pt', 'ignore_keys': ['loss'], 'loss_config': {'target': 'torch.nn.Identity'}, 'regularizer_config': {'target': 'vae_modules.regularizers.DiagonalGaussianRegularizer'}, 'encoder_config': {'target': 'vae_modules.cp_enc_dec.ContextParallelEncoder3D', 'params': {'double_z': True, 'z_channels': 16, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 2, 4], 'attn_resolutions': [], 'num_res_blocks': 3, 'dropout': 0.0, 'gather_norm': True}}, 'decoder_config': {'target': 'vae_modules.cp_enc_dec.ContextParallelDecoder3D', 'params': {'double_z': True, 'z_channels': 16, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 2, 4], 'attn_resolutions': [], 'num_res_blocks': 3, 'dropout': 0.0, 'gather_norm': False}}}}, 'loss_fn_config': {'target': 'sgm.modules.diffusionmodules.loss.VideoDiffusionLoss', 'params': {'offset_noise_level': 0, 'sigma_sampler_config': {'target': 'sgm.modules.diffusionmodules.sigma_sampling.DiscreteSampling', 'params': {'uniform_sampling': True, 'num_idx': 1000, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 3.0}}}}}}, 'sampler_config': {'target': 'sgm.modules.diffusionmodules.sampling.VPSDEDPMPP2MSampler', 'params': {'num_steps': 50, 'verbose': True, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 3.0}}, 'guider_config': {'target': 'sgm.modules.diffusionmodules.guiders.DynamicCFG', 'params': {'scale': 6, 'exp': 5, 'num_steps': 50}}}}}, data_config={'target': 'data_video.SFTDataset', 'params': {'video_size': [480, 720], 'fps': 8, 'max_num_frames': 49, 'skip_frms_num': 3.0}}, cuda=True, rank=0, world_size=1, deepspeed_activation_checkpointing=True, master_ip='nm04-a800-node083', master_port='10185', log_config=[{'model': {'scale_factor': 1.15258426, 'disable_first_stage_autocast': True, 'not_trainable_prefixes': ['all'], 'log_keys': ['txt'], 'denoiser_config': {'target': 'sgm.modules.diffusionmodules.denoiser.DiscreteDenoiser', 'params': {'num_idx': 1000, 'quantize_c_noise': False, 'weighting_config': {'target': 'sgm.modules.diffusionmodules.denoiser_weighting.EpsWeighting'}, 'scaling_config': {'target': 'sgm.modules.diffusionmodules.denoiser_scaling.VideoScaling'}, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 3.0}}}}, 'network_config': {'target': 'dit_video_concat.DiffusionTransformer', 'params': {'time_embed_dim': 512, 'elementwise_affine': True, 'num_frames': 49, 'time_compressed_rate': 4, 'latent_width': 90, 'latent_height': 60, 'num_layers': 30, 'patch_size': 2, 'in_channels': 16, 'out_channels': 16, 'hidden_size': 1920, 'adm_in_channels': 256, 'num_attention_heads': 30, 'transformer_args': {'checkpoint_activations': True, 'vocab_size': 1, 'max_sequence_length': 64, 'layernorm_order': 'pre', 'skip_init': False, 'model_parallel_size': 1, 'is_decoder': False}, 'modules': {'pos_embed_config': {'target': 'dit_video_concat.Basic3DPositionEmbeddingMixin', 'params': {'text_length': 226, 'height_interpolation': 1.875, 'width_interpolation': 1.875}}, 'lora_config': {'target': 'sat.model.finetune.lora2.LoraMixin', 'params': {'r': 128}}, 'patch_embed_config': {'target': 'dit_video_concat.ImagePatchEmbeddingMixin', 'params': {'text_hidden_size': 4096}}, 'adaln_layer_config': {'target': 'dit_video_concat.AdaLNMixin', 'params': {'qk_ln': True}}, 'final_layer_config': {'target': 'dit_video_concat.FinalLayerMixin'}}}}, 'conditioner_config': {'target': 'sgm.modules.GeneralConditioner', 'params': {'emb_models': [{'is_trainable': False, 'input_key': 'txt', 'ucg_rate': 0.1, 'target': 'sgm.modules.encoders.modules.FrozenT5Embedder', 'params': {'model_dir': '/mnt/ceph/develop/jiawei/model_checkpoint/t5-v1_1-xxl', 'max_length': 226}}]}}, 'first_stage_config': {'target': 'vae_modules.autoencoder.VideoAutoencoderInferenceWrapper', 'params': {'cp_size': 1, 'ckpt_path': '/mnt/ceph/develop/jiawei/model_checkpoint/CogVideoX-2b-sat/vae/3d-vae.pt', 'ignore_keys': ['loss'], 'loss_config': {'target': 'torch.nn.Identity'}, 'regularizer_config': {'target': 'vae_modules.regularizers.DiagonalGaussianRegularizer'}, 'encoder_config': {'target': 'vae_modules.cp_enc_dec.ContextParallelEncoder3D', 'params': {'double_z': True, 'z_channels': 16, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 2, 4], 'attn_resolutions': [], 'num_res_blocks': 3, 'dropout': 0.0, 'gather_norm': True}}, 'decoder_config': {'target': 'vae_modules.cp_enc_dec.ContextParallelDecoder3D', 'params': {'double_z': True, 'z_channels': 16, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 2, 4], 'attn_resolutions': [], 'num_res_blocks': 3, 'dropout': 0.0, 'gather_norm': False}}}}, 'loss_fn_config': {'target': 'sgm.modules.diffusionmodules.loss.VideoDiffusionLoss', 'params': {'offset_noise_level': 0, 'sigma_sampler_config': {'target': 'sgm.modules.diffusionmodules.sigma_sampling.DiscreteSampling', 'params': {'uniform_sampling': True, 'num_idx': 1000, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 3.0}}}}}}, 'sampler_config': {'target': 'sgm.modules.diffusionmodules.sampling.VPSDEDPMPP2MSampler', 'params': {'num_steps': 50, 'verbose': True, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 3.0}}, 'guider_config': {'target': 'sgm.modules.diffusionmodules.guiders.DynamicCFG', 'params': {'scale': 6, 'exp': 5, 'num_steps': 50}}}}}}, {'args': {'checkpoint_activations': True, 'model_parallel_size': 1, 'experiment_name': 'lora-disney', 'mode': 'finetune', 'load': '/mnt/ceph/develop/jiawei/model_checkpoint/CogVideoX-2b-sat/transformer', 'no_load_rng': True, 'train_iters': 1000, 'eval_iters': 1, 'eval_interval': 100, 'eval_batch_size': 1, 'save': '/mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora', 'save_interval': 500, 'log_interval': 20, 'train_data': ['/mnt/ceph/develop/jiawei/lora_dataset/cogvideo_lora_dataset'], 'valid_data': ['/mnt/ceph/develop/jiawei/lora_dataset/cogvideo_lora_dataset'], 'split': '1,0,0', 'num_workers': 8, 'force_train': True, 'only_log_video_latents': False, 'wandb': True, 'wandb_project_name': 'cogvideox_ckpts_2b_lora', 'summary_dir': '/mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora'}, 'data': {'target': 'data_video.SFTDataset', 'params': {'video_size': [480, 720], 'fps': 8, 'max_num_frames': 49, 'skip_frms_num': 3.0}}, 'deepspeed': {'train_micro_batch_size_per_gpu': 2, 'gradient_accumulation_steps': 1, 'steps_per_print': 50, 'gradient_clipping': 0.1, 'zero_optimization': {'stage': 2, 'cpu_offload': False, 'contiguous_gradients': False, 'overlap_comm': True, 'reduce_scatter': True, 'reduce_bucket_size': 1000000000, 'allgather_bucket_size': 1000000000, 'load_from_fp32_weights': False}, 'zero_allow_untested_optimizer': True, 'bf16': {'enabled': False}, 'fp16': {'enabled': True}, 'loss_scale': 0, 'loss_scale_window': 400, 'hysteresis': 2, 'min_loss_scale': 1, 'optimizer': {'type': 'sat.ops.FusedEmaAdam', 'params': {'lr': '1E-3', 'betas': [0.9, 0.95], 'eps': '1e-8', 'weight_decay': '1e-4'}}, 'activation_checkpointing': {'partition_activations': False, 'contiguous_memory_optimization': False}, 'wall_clock_breakdown': False}}])
[2024-09-09 14:16:39,235] [INFO] [RANK 0] building SATVideoDiffusionEngine model ...
[2024-09-09 14:16:49,594] [WARNING] [RANK 0] Failed to load bitsandbytes:No module named 'bitsandbytes'
[2024-09-09 14:16:49,594] [INFO] [RANK 0] replacing layer 0 attention with lora
[2024-09-09 14:16:49,621] [INFO] [RANK 0] replacing layer 1 attention with lora
[2024-09-09 14:16:49,644] [INFO] [RANK 0] replacing layer 2 attention with lora
[2024-09-09 14:16:49,667] [INFO] [RANK 0] replacing layer 3 attention with lora
[2024-09-09 14:16:49,693] [INFO] [RANK 0] replacing layer 4 attention with lora
[2024-09-09 14:16:49,716] [INFO] [RANK 0] replacing layer 5 attention with lora
[2024-09-09 14:16:49,739] [INFO] [RANK 0] replacing layer 6 attention with lora
[2024-09-09 14:16:49,758] [INFO] [RANK 0] replacing layer 7 attention with lora
[2024-09-09 14:16:49,776] [INFO] [RANK 0] replacing layer 8 attention with lora
[2024-09-09 14:16:49,795] [INFO] [RANK 0] replacing layer 9 attention with lora
[2024-09-09 14:16:49,812] [INFO] [RANK 0] replacing layer 10 attention with lora
[2024-09-09 14:16:49,830] [INFO] [RANK 0] replacing layer 11 attention with lora
[2024-09-09 14:16:49,849] [INFO] [RANK 0] replacing layer 12 attention with lora
[2024-09-09 14:16:49,867] [INFO] [RANK 0] replacing layer 13 attention with lora
[2024-09-09 14:16:49,884] [INFO] [RANK 0] replacing layer 14 attention with lora
[2024-09-09 14:16:49,902] [INFO] [RANK 0] replacing layer 15 attention with lora
[2024-09-09 14:16:49,920] [INFO] [RANK 0] replacing layer 16 attention with lora
[2024-09-09 14:16:49,937] [INFO] [RANK 0] replacing layer 17 attention with lora
[2024-09-09 14:16:49,952] [INFO] [RANK 0] replacing layer 18 attention with lora
[2024-09-09 14:16:49,968] [INFO] [RANK 0] replacing layer 19 attention with lora
[2024-09-09 14:16:49,984] [INFO] [RANK 0] replacing layer 20 attention with lora
[2024-09-09 14:16:49,999] [INFO] [RANK 0] replacing layer 21 attention with lora
[2024-09-09 14:16:50,013] [INFO] [RANK 0] replacing layer 22 attention with lora
[2024-09-09 14:16:50,028] [INFO] [RANK 0] replacing layer 23 attention with lora
[2024-09-09 14:16:50,043] [INFO] [RANK 0] replacing layer 24 attention with lora
[2024-09-09 14:16:50,061] [INFO] [RANK 0] replacing layer 25 attention with lora
[2024-09-09 14:16:50,078] [INFO] [RANK 0] replacing layer 26 attention with lora
[2024-09-09 14:16:50,095] [INFO] [RANK 0] replacing layer 27 attention with lora
[2024-09-09 14:16:50,114] [INFO] [RANK 0] replacing layer 28 attention with lora
[2024-09-09 14:16:50,131] [INFO] [RANK 0] replacing layer 29 attention with lora
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  1.56it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.62it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.61it/s]
Initialized embedder #0: FrozenT5Embedder with 4762310656 params. Trainable: False
Working with z of shape (1, 16, 32, 32) = 16384 dimensions.
/mnt/ceph/develop/jiawei/CogVideo/sat/vae_modules/autoencoder.py:565: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  sd = torch.load(path, map_location="cpu")["state_dict"]
Deleting key loss.logvar from state_dict.
Deleting key loss.perceptual_loss.scaling_layer.shift from state_dict.
Deleting key loss.perceptual_loss.scaling_layer.scale from state_dict.
Deleting key loss.perceptual_loss.net.slice1.0.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice1.0.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice1.2.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice1.2.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice2.5.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice2.5.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice2.7.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice2.7.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice3.10.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice3.10.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice3.12.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice3.12.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice3.14.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice3.14.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice4.17.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice4.17.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice4.19.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice4.19.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice4.21.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice4.21.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice5.24.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice5.24.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice5.26.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice5.26.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice5.28.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice5.28.bias from state_dict.
Deleting key loss.perceptual_loss.lin0.model.1.weight from state_dict.
Deleting key loss.perceptual_loss.lin1.model.1.weight from state_dict.
Deleting key loss.perceptual_loss.lin2.model.1.weight from state_dict.
Deleting key loss.perceptual_loss.lin3.model.1.weight from state_dict.
Deleting key loss.perceptual_loss.lin4.model.1.weight from state_dict.
Deleting key loss.discriminator.blocks.0.downsample_res.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.0.downsample_res.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.0.net.0.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.0.net.0.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.0.net.2.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.0.net.2.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.0.downsample.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.0.downsample.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.1.downsample_res.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.1.downsample_res.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.1.net.0.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.1.net.0.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.1.net.2.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.1.net.2.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.1.downsample.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.1.downsample.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.2.downsample_res.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.2.downsample_res.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.2.net.0.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.2.net.0.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.2.net.2.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.2.net.2.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.2.downsample.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.2.downsample.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.3.downsample_res.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.3.downsample_res.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.3.net.0.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.3.net.0.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.3.net.2.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.3.net.2.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.3.downsample.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.3.downsample.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.4.0.conv_res.weight from state_dict.
Deleting key loss.discriminator.blocks.4.0.conv_res.bias from state_dict.
Deleting key loss.discriminator.blocks.4.0.net.0.weight from state_dict.
Deleting key loss.discriminator.blocks.4.0.net.0.bias from state_dict.
Deleting key loss.discriminator.blocks.4.0.net.2.weight from state_dict.
Deleting key loss.discriminator.blocks.4.0.net.2.bias from state_dict.
Deleting key loss.discriminator.blocks.4.0.downsample.1.weight from state_dict.
Deleting key loss.discriminator.blocks.4.0.downsample.1.bias from state_dict.
Deleting key loss.discriminator.blocks.4.1.0.fn.norm.gamma from state_dict.
Deleting key loss.discriminator.blocks.4.1.0.fn.attn.to_q.0.weight from state_dict.
Deleting key loss.discriminator.blocks.4.1.0.fn.attn.to_kv.0.weight from state_dict.
Deleting key loss.discriminator.blocks.4.1.0.fn.attn.to_out.0.weight from state_dict.
Deleting key loss.discriminator.blocks.4.1.1.fn.norm.gamma from state_dict.
Deleting key loss.discriminator.blocks.4.1.1.fn.net.0.weight from state_dict.
Deleting key loss.discriminator.blocks.4.1.1.fn.net.0.bias from state_dict.
Deleting key loss.discriminator.blocks.4.1.1.fn.net.2.weight from state_dict.
Deleting key loss.discriminator.blocks.4.1.1.fn.net.2.bias from state_dict.
Deleting key loss.discriminator.blocks.5.0.conv_res.weight from state_dict.
Deleting key loss.discriminator.blocks.5.0.conv_res.bias from state_dict.
Deleting key loss.discriminator.blocks.5.0.net.0.weight from state_dict.
Deleting key loss.discriminator.blocks.5.0.net.0.bias from state_dict.
Deleting key loss.discriminator.blocks.5.0.net.2.weight from state_dict.
Deleting key loss.discriminator.blocks.5.0.net.2.bias from state_dict.
Deleting key loss.discriminator.blocks.5.0.downsample.1.weight from state_dict.
Deleting key loss.discriminator.blocks.5.0.downsample.1.bias from state_dict.
Deleting key loss.discriminator.blocks.5.1.0.fn.norm.gamma from state_dict.
Deleting key loss.discriminator.blocks.5.1.0.fn.attn.to_q.0.weight from state_dict.
Deleting key loss.discriminator.blocks.5.1.0.fn.attn.to_kv.0.weight from state_dict.
Deleting key loss.discriminator.blocks.5.1.0.fn.attn.to_out.0.weight from state_dict.
Deleting key loss.discriminator.blocks.5.1.1.fn.norm.gamma from state_dict.
Deleting key loss.discriminator.blocks.5.1.1.fn.net.0.weight from state_dict.
Deleting key loss.discriminator.blocks.5.1.1.fn.net.0.bias from state_dict.
Deleting key loss.discriminator.blocks.5.1.1.fn.net.2.weight from state_dict.
Deleting key loss.discriminator.blocks.5.1.1.fn.net.2.bias from state_dict.
Deleting key loss.discriminator.blocks.6.0.conv_res.weight from state_dict.
Deleting key loss.discriminator.blocks.6.0.conv_res.bias from state_dict.
Deleting key loss.discriminator.blocks.6.0.net.0.weight from state_dict.
Deleting key loss.discriminator.blocks.6.0.net.0.bias from state_dict.
Deleting key loss.discriminator.blocks.6.0.net.2.weight from state_dict.
Deleting key loss.discriminator.blocks.6.0.net.2.bias from state_dict.
Deleting key loss.discriminator.blocks.6.1.0.fn.norm.gamma from state_dict.
Deleting key loss.discriminator.blocks.6.1.0.fn.attn.to_q.0.weight from state_dict.
Deleting key loss.discriminator.blocks.6.1.0.fn.attn.to_kv.0.weight from state_dict.
Deleting key loss.discriminator.blocks.6.1.0.fn.attn.to_out.0.weight from state_dict.
Deleting key loss.discriminator.blocks.6.1.1.fn.norm.gamma from state_dict.
Deleting key loss.discriminator.blocks.6.1.1.fn.net.0.weight from state_dict.
Deleting key loss.discriminator.blocks.6.1.1.fn.net.0.bias from state_dict.
Deleting key loss.discriminator.blocks.6.1.1.fn.net.2.weight from state_dict.
Deleting key loss.discriminator.blocks.6.1.1.fn.net.2.bias from state_dict.
Deleting key loss.discriminator.to_logits.0.weight from state_dict.
Deleting key loss.discriminator.to_logits.0.bias from state_dict.
Deleting key loss.discriminator.to_logits.3.weight from state_dict.
Deleting key loss.discriminator.to_logits.3.bias from state_dict.
Missing keys:  []
Unexpected keys:  []
Restored from /mnt/ceph/develop/jiawei/model_checkpoint/CogVideoX-2b-sat/vae/3d-vae.pt
[2024-09-09 14:16:54,226] [INFO] [RANK 0]  > number of parameters on model parallel rank 0: 6764790755
RUN on nm04-a800-node083, CUDA_VISIBLE_DEVICES=0
torchrun --standalone --nproc_per_node=1 train_video.py --base configs/cogvideox_2b_lora.yaml configs/sft.yaml --seed 42
[2024-09-09 14:17:00,099] [INFO] [RANK 0] global rank 0 is loading checkpoint /mnt/ceph/develop/jiawei/model_checkpoint/CogVideoX-2b-sat/transformer/1000/mp_rank_00_model_states.pt
/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/sat/training/model_io.py:286: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  sd = torch.load(checkpoint_name, map_location='cpu')
[2024-09-09 14:17:02,189] [INFO] [RANK 0] > successfully loaded /mnt/ceph/develop/jiawei/model_checkpoint/CogVideoX-2b-sat/transformer/1000/mp_rank_00_model_states.pt
[2024-09-09 14:17:02,602] [INFO] [RANK 0] ***** Total trainable parameters: 58982400 *****
[2024-09-09 14:17:02,603] [INFO] [RANK 0] [<class 'sat.ops.layernorm.LayerNorm'>, <class 'torch.nn.modules.normalization.LayerNorm'>, <class 'sat.ops.layernorm.RMSNorm'>] is set to no_weight_decay
[2024-09-09 14:17:02,609] [INFO] [RANK 0] Syncing initialized parameters...
[2024-09-09 14:17:02,671] [INFO] [RANK 0] Finished syncing initialized parameters.
[2024-09-09 14:17:02,671] [INFO] [RANK 0] Using optimizer sat.ops.FusedEmaAdam from sat.
[2024-09-09 14:17:02,675] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.4, git-hash=unknown, git-branch=unknown
[2024-09-09 14:17:02,676] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
[2024-09-09 14:17:02,757] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /root/.cache/torch_extensions/py310_cu121/fused_ema_adam/build.ninja...
/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1965: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module fused_ema_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
[2024-09-09 14:17:06,123] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
ninja: no work to do.
Loading extension module fused_ema_adam...
Time to load fused_ema_adam op: 3.593937873840332 seconds
[2024-09-09 14:17:06,390] [INFO] [logging.py:96:log_dist] [Rank 0] Using client callable to create basic optimizer
[2024-09-09 14:17:06,392] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-09-09 14:17:06,421] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedEmaAdam
[2024-09-09 14:17:06,421] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedEmaAdam type=<class 'sat.ops.fused_ema_adam.FusedEmaAdam'>
[2024-09-09 14:17:06,427] [WARNING] [engine.py:1179:_do_optimizer_sanity_check] **** You are using ZeRO with an untested optimizer, proceed with caution *****
[2024-09-09 14:17:06,427] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2024-09-09 14:17:06,427] [INFO] [stage_1_and_2.py:148:__init__] Reduce bucket size 1000000000
[2024-09-09 14:17:06,427] [INFO] [stage_1_and_2.py:149:__init__] Allgather bucket size 1000000000
[2024-09-09 14:17:06,427] [INFO] [stage_1_and_2.py:150:__init__] CPU Offload: False
[2024-09-09 14:17:06,427] [INFO] [stage_1_and_2.py:151:__init__] Round robin gradient partitioning: False
[2024-09-09 14:17:06,756] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2024-09-09 14:17:06,757] [INFO] [utils.py:782:see_memory_usage] MA 12.86 GB         Max_MA 12.97 GB         CA 13.23 GB         Max_CA 13 GB 
[2024-09-09 14:17:06,759] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 116.33 GB, percent = 5.8%
[2024-09-09 14:17:06,903] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2024-09-09 14:17:06,904] [INFO] [utils.py:782:see_memory_usage] MA 12.86 GB         Max_MA 13.08 GB         CA 13.45 GB         Max_CA 13 GB 
[2024-09-09 14:17:06,906] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 116.33 GB, percent = 5.8%
[2024-09-09 14:17:06,906] [INFO] [stage_1_and_2.py:543:__init__] optimizer state initialized
[2024-09-09 14:17:07,052] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2024-09-09 14:17:07,052] [INFO] [utils.py:782:see_memory_usage] MA 12.86 GB         Max_MA 12.86 GB         CA 13.45 GB         Max_CA 13 GB 
[2024-09-09 14:17:07,055] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 116.33 GB, percent = 5.8%
[2024-09-09 14:17:07,058] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer
[2024-09-09 14:17:07,058] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2024-09-09 14:17:07,061] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2024-09-09 14:17:07,061] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[1.0], mom=[[0.9, 0.95]]
[2024-09-09 14:17:07,064] [INFO] [config.py:997:print] DeepSpeedEngine configuration:
[2024-09-09 14:17:07,064] [INFO] [config.py:1001:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-09-09 14:17:07,064] [INFO] [config.py:1001:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-09-09 14:17:07,064] [INFO] [config.py:1001:print]   amp_enabled .................. False
[2024-09-09 14:17:07,066] [INFO] [config.py:1001:print]   amp_params ................... False
[2024-09-09 14:17:07,066] [INFO] [config.py:1001:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-09-09 14:17:07,066] [INFO] [config.py:1001:print]   bfloat16_enabled ............. False
[2024-09-09 14:17:07,066] [INFO] [config.py:1001:print]   bfloat16_immediate_grad_update  False
[2024-09-09 14:17:07,066] [INFO] [config.py:1001:print]   checkpoint_parallel_write_pipeline  False
[2024-09-09 14:17:07,066] [INFO] [config.py:1001:print]   checkpoint_tag_validation_enabled  True
[2024-09-09 14:17:07,070] [INFO] [config.py:1001:print]   checkpoint_tag_validation_fail  False
[2024-09-09 14:17:07,070] [INFO] [config.py:1001:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f1f28277a90>
[2024-09-09 14:17:07,070] [INFO] [config.py:1001:print]   communication_data_type ...... None
[2024-09-09 14:17:07,070] [INFO] [config.py:1001:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-09-09 14:17:07,070] [INFO] [config.py:1001:print]   curriculum_enabled_legacy .... False
[2024-09-09 14:17:07,070] [INFO] [config.py:1001:print]   curriculum_params_legacy ..... False
[2024-09-09 14:17:07,070] [INFO] [config.py:1001:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-09-09 14:17:07,070] [INFO] [config.py:1001:print]   data_efficiency_enabled ...... False
[2024-09-09 14:17:07,070] [INFO] [config.py:1001:print]   dataloader_drop_last ......... False
[2024-09-09 14:17:07,070] [INFO] [config.py:1001:print]   disable_allgather ............ False
[2024-09-09 14:17:07,070] [INFO] [config.py:1001:print]   dump_state ................... False
[2024-09-09 14:17:07,070] [INFO] [config.py:1001:print]   dynamic_loss_scale_args ...... None
[2024-09-09 14:17:07,070] [INFO] [config.py:1001:print]   eigenvalue_enabled ........... False
[2024-09-09 14:17:07,070] [INFO] [config.py:1001:print]   eigenvalue_gas_boundary_resolution  1
[2024-09-09 14:17:07,070] [INFO] [config.py:1001:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-09-09 14:17:07,070] [INFO] [config.py:1001:print]   eigenvalue_layer_num ......... 0
[2024-09-09 14:17:07,070] [INFO] [config.py:1001:print]   eigenvalue_max_iter .......... 100
[2024-09-09 14:17:07,070] [INFO] [config.py:1001:print]   eigenvalue_stability ......... 1e-06
[2024-09-09 14:17:07,070] [INFO] [config.py:1001:print]   eigenvalue_tol ............... 0.01
[2024-09-09 14:17:07,070] [INFO] [config.py:1001:print]   eigenvalue_verbose ........... False
[2024-09-09 14:17:07,070] [INFO] [config.py:1001:print]   elasticity_enabled ........... False
[2024-09-09 14:17:07,070] [INFO] [config.py:1001:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-09-09 14:17:07,072] [INFO] [config.py:1001:print]   fp16_auto_cast ............... False
[2024-09-09 14:17:07,072] [INFO] [config.py:1001:print]   fp16_enabled ................. True
[2024-09-09 14:17:07,072] [INFO] [config.py:1001:print]   fp16_master_weights_and_gradients  False
[2024-09-09 14:17:07,072] [INFO] [config.py:1001:print]   global_rank .................. 0
[2024-09-09 14:17:07,072] [INFO] [config.py:1001:print]   grad_accum_dtype ............. None
[2024-09-09 14:17:07,072] [INFO] [config.py:1001:print]   gradient_accumulation_steps .. 1
[2024-09-09 14:17:07,072] [INFO] [config.py:1001:print]   gradient_clipping ............ 0.1
[2024-09-09 14:17:07,072] [INFO] [config.py:1001:print]   gradient_predivide_factor .... 1.0
[2024-09-09 14:17:07,072] [INFO] [config.py:1001:print]   graph_harvesting ............. False
[2024-09-09 14:17:07,072] [INFO] [config.py:1001:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-09-09 14:17:07,072] [INFO] [config.py:1001:print]   initial_dynamic_scale ........ 65536
[2024-09-09 14:17:07,072] [INFO] [config.py:1001:print]   load_universal_checkpoint .... False
[2024-09-09 14:17:07,072] [INFO] [config.py:1001:print]   loss_scale ................... 0
[2024-09-09 14:17:07,072] [INFO] [config.py:1001:print]   memory_breakdown ............. False
[2024-09-09 14:17:07,072] [INFO] [config.py:1001:print]   mics_hierarchial_params_gather  False
[2024-09-09 14:17:07,072] [INFO] [config.py:1001:print]   mics_shard_size .............. -1
[2024-09-09 14:17:07,072] [INFO] [config.py:1001:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-09-09 14:17:07,072] [INFO] [config.py:1001:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-09-09 14:17:07,075] [INFO] [config.py:1001:print]   optimizer_legacy_fusion ...... False
[2024-09-09 14:17:07,075] [INFO] [config.py:1001:print]   optimizer_name ............... None
[2024-09-09 14:17:07,075] [INFO] [config.py:1001:print]   optimizer_params ............. None
[2024-09-09 14:17:07,075] [INFO] [config.py:1001:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-09-09 14:17:07,075] [INFO] [config.py:1001:print]   pld_enabled .................. False
[2024-09-09 14:17:07,075] [INFO] [config.py:1001:print]   pld_params ................... False
[2024-09-09 14:17:07,075] [INFO] [config.py:1001:print]   prescale_gradients ........... False
[2024-09-09 14:17:07,075] [INFO] [config.py:1001:print]   scheduler_name ............... None
[2024-09-09 14:17:07,075] [INFO] [config.py:1001:print]   scheduler_params ............. None
[2024-09-09 14:17:07,075] [INFO] [config.py:1001:print]   seq_parallel_communication_data_type  torch.float32
[2024-09-09 14:17:07,075] [INFO] [config.py:1001:print]   sparse_attention ............. None
[2024-09-09 14:17:07,075] [INFO] [config.py:1001:print]   sparse_gradients_enabled ..... False
[2024-09-09 14:17:07,075] [INFO] [config.py:1001:print]   steps_per_print .............. 50
[2024-09-09 14:17:07,075] [INFO] [config.py:1001:print]   timers_config ................ enabled=True synchronized=True
[2024-09-09 14:17:07,075] [INFO] [config.py:1001:print]   train_batch_size ............. 2
[2024-09-09 14:17:07,078] [INFO] [config.py:1001:print]   train_micro_batch_size_per_gpu  2
[2024-09-09 14:17:07,078] [INFO] [config.py:1001:print]   use_data_before_expert_parallel_  False
[2024-09-09 14:17:07,078] [INFO] [config.py:1001:print]   use_node_local_storage ....... False
[2024-09-09 14:17:07,078] [INFO] [config.py:1001:print]   wall_clock_breakdown ......... False
[2024-09-09 14:17:07,078] [INFO] [config.py:1001:print]   weight_quantization_config ... None
[2024-09-09 14:17:07,078] [INFO] [config.py:1001:print]   world_size ................... 1
[2024-09-09 14:17:07,078] [INFO] [config.py:1001:print]   zero_allow_untested_optimizer  True
[2024-09-09 14:17:07,078] [INFO] [config.py:1001:print]   zero_config .................. stage=2 contiguous_gradients=False reduce_scatter=True reduce_bucket_size=1000000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=1000000000 overlap_comm=True load_from_fp32_weights=False elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-09-09 14:17:07,078] [INFO] [config.py:1001:print]   zero_enabled ................. True
[2024-09-09 14:17:07,078] [INFO] [config.py:1001:print]   zero_force_ds_cpu_optimizer .. True
[2024-09-09 14:17:07,079] [INFO] [config.py:1001:print]   zero_optimization_stage ...... 2
[2024-09-09 14:17:07,079] [INFO] [config.py:987:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 2, 
    "gradient_accumulation_steps": 1, 
    "steps_per_print": 50, 
    "gradient_clipping": 0.1, 
    "zero_optimization": {
        "stage": 2, 
        "cpu_offload": false, 
        "contiguous_gradients": false, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 1.000000e+09, 
        "allgather_bucket_size": 1.000000e+09, 
        "load_from_fp32_weights": false
    }, 
    "zero_allow_untested_optimizer": true, 
    "bf16": {
        "enabled": false
    }, 
    "fp16": {
        "enabled": true
    }, 
    "loss_scale": 0, 
    "loss_scale_window": 400, 
    "hysteresis": 2, 
    "min_loss_scale": 1, 
    "activation_checkpointing": {
        "partition_activations": false, 
        "contiguous_memory_optimization": false
    }, 
    "wall_clock_breakdown": false
}
[2024-09-09 14:17:07,081] [INFO] [RANK 0] learning rate decaying style linear, ratio 10.0
[2024-09-09 14:17:07,081] [INFO] [RANK 0] Finetuning Model...
[2024-09-09 14:17:07,081] [INFO] [RANK 0] arguments:
[2024-09-09 14:17:07,081] [INFO] [RANK 0]   base ......................... ['configs/cogvideox_2b_lora.yaml', 'configs/sft.yaml']
[2024-09-09 14:17:07,081] [INFO] [RANK 0]   model_parallel_size .......... 1
[2024-09-09 14:17:07,081] [INFO] [RANK 0]   force_pretrain ............... False
[2024-09-09 14:17:07,081] [INFO] [RANK 0]   device ....................... 0
[2024-09-09 14:17:07,084] [INFO] [RANK 0]   debug ........................ False
[2024-09-09 14:17:07,084] [INFO] [RANK 0]   log_image .................... True
[2024-09-09 14:17:07,084] [INFO] [RANK 0]   output_dir ................... samples
[2024-09-09 14:17:07,084] [INFO] [RANK 0]   input_dir .................... None
[2024-09-09 14:17:07,084] [INFO] [RANK 0]   input_type ................... cli
[2024-09-09 14:17:07,084] [INFO] [RANK 0]   input_file ................... input.txt
[2024-09-09 14:17:07,084] [INFO] [RANK 0]   final_size ................... 2048
[2024-09-09 14:17:07,084] [INFO] [RANK 0]   sdedit ....................... False
[2024-09-09 14:17:07,084] [INFO] [RANK 0]   grid_num_rows ................ 1
[2024-09-09 14:17:07,084] [INFO] [RANK 0]   force_inference .............. False
[2024-09-09 14:17:07,084] [INFO] [RANK 0]   lcm_steps .................... None
[2024-09-09 14:17:07,084] [INFO] [RANK 0]   sampling_num_frames .......... 32
[2024-09-09 14:17:07,086] [INFO] [RANK 0]   sampling_fps ................. 8
[2024-09-09 14:17:07,086] [INFO] [RANK 0]   only_save_latents ............ False
[2024-09-09 14:17:07,086] [INFO] [RANK 0]   only_log_video_latents ....... False
[2024-09-09 14:17:07,086] [INFO] [RANK 0]   latent_channels .............. 32
[2024-09-09 14:17:07,086] [INFO] [RANK 0]   image2video .................. False
[2024-09-09 14:17:07,086] [INFO] [RANK 0]   experiment_name .............. lora-disney-09-09-14-16
[2024-09-09 14:17:07,086] [INFO] [RANK 0]   train_iters .................. 1000
[2024-09-09 14:17:07,086] [INFO] [RANK 0]   batch_size ................... 2
[2024-09-09 14:17:07,086] [INFO] [RANK 0]   lr ........................... 0.001
[2024-09-09 14:17:07,086] [INFO] [RANK 0]   mode ......................... finetune
[2024-09-09 14:17:07,086] [INFO] [RANK 0]   seed ......................... 42
[2024-09-09 14:17:07,090] [INFO] [RANK 0]   zero_stage ................... 0
[2024-09-09 14:17:07,090] [INFO] [RANK 0]   checkpoint_activations ....... True
[2024-09-09 14:17:07,090] [INFO] [RANK 0]   checkpoint_num_layers ........ 1
[2024-09-09 14:17:07,090] [INFO] [RANK 0]   checkpoint_skip_layers ....... 0
[2024-09-09 14:17:07,090] [INFO] [RANK 0]   fp16 ......................... True
[2024-09-09 14:17:07,090] [INFO] [RANK 0]   bf16 ......................... False
[2024-09-09 14:17:07,090] [INFO] [RANK 0]   gradient_accumulation_steps .. 1
[2024-09-09 14:17:07,090] [INFO] [RANK 0]   profiling .................... -1
[2024-09-09 14:17:07,090] [INFO] [RANK 0]   epochs ....................... None
[2024-09-09 14:17:07,090] [INFO] [RANK 0]   log_interval ................. 20
[2024-09-09 14:17:07,090] [INFO] [RANK 0]   summary_dir .................. /mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora
[2024-09-09 14:17:07,090] [INFO] [RANK 0]   save_args .................... False
[2024-09-09 14:17:07,090] [INFO] [RANK 0]   lr_decay_iters ............... None
[2024-09-09 14:17:07,090] [INFO] [RANK 0]   lr_decay_style ............... linear
[2024-09-09 14:17:07,093] [INFO] [RANK 0]   lr_decay_ratio ............... 0.1
[2024-09-09 14:17:07,093] [INFO] [RANK 0]   warmup ....................... 0.01
[2024-09-09 14:17:07,093] [INFO] [RANK 0]   weight_decay ................. 0.0001
[2024-09-09 14:17:07,093] [INFO] [RANK 0]   save ......................... /mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora/lora-disney-09-09-14-16
[2024-09-09 14:17:07,093] [INFO] [RANK 0]   load ......................... /mnt/ceph/develop/jiawei/model_checkpoint/CogVideoX-2b-sat/transformer
[2024-09-09 14:17:07,093] [INFO] [RANK 0]   force_train .................. True
[2024-09-09 14:17:07,093] [INFO] [RANK 0]   save_interval ................ 500
[2024-09-09 14:17:07,093] [INFO] [RANK 0]   no_save_rng .................. False
[2024-09-09 14:17:07,093] [INFO] [RANK 0]   no_load_rng .................. True
[2024-09-09 14:17:07,093] [INFO] [RANK 0]   resume_dataloader ............ False
[2024-09-09 14:17:07,093] [INFO] [RANK 0]   distributed_backend .......... nccl
[2024-09-09 14:17:07,093] [INFO] [RANK 0]   local_rank ................... 0
[2024-09-09 14:17:07,093] [INFO] [RANK 0]   exit_interval ................ None
[2024-09-09 14:17:07,093] [INFO] [RANK 0]   wandb ........................ True
[2024-09-09 14:17:07,093] [INFO] [RANK 0]   wandb_project_name ........... cogvideox_ckpts_2b_lora
[2024-09-09 14:17:07,093] [INFO] [RANK 0]   eval_batch_size .............. 1
[2024-09-09 14:17:07,093] [INFO] [RANK 0]   eval_iters ................... 1
[2024-09-09 14:17:07,093] [INFO] [RANK 0]   eval_interval ................ 100
[2024-09-09 14:17:07,093] [INFO] [RANK 0]   strict_eval .................. False
[2024-09-09 14:17:07,097] [INFO] [RANK 0]   train_data ................... ['/mnt/ceph/develop/jiawei/lora_dataset/cogvideo_lora_dataset']
[2024-09-09 14:17:07,097] [INFO] [RANK 0]   train_data_weights ........... None
[2024-09-09 14:17:07,097] [INFO] [RANK 0]   iterable_dataset ............. False
[2024-09-09 14:17:07,097] [INFO] [RANK 0]   iterable_dataset_eval ........ 
[2024-09-09 14:17:07,097] [INFO] [RANK 0]   batch_from_same_dataset ...... False
[2024-09-09 14:17:07,097] [INFO] [RANK 0]   valid_data ................... ['/mnt/ceph/develop/jiawei/lora_dataset/cogvideo_lora_dataset']
[2024-09-09 14:17:07,097] [INFO] [RANK 0]   test_data .................... None
[2024-09-09 14:17:07,097] [INFO] [RANK 0]   split ........................ 1,0,0
[2024-09-09 14:17:07,097] [INFO] [RANK 0]   num_workers .................. 8
[2024-09-09 14:17:07,097] [INFO] [RANK 0]   block_size ................... 10000
[2024-09-09 14:17:07,097] [INFO] [RANK 0]   prefetch_factor .............. 4
[2024-09-09 14:17:07,097] [INFO] [RANK 0]   deepspeed .................... True
[2024-09-09 14:17:07,097] [INFO] [RANK 0]   deepspeed_config ............. {'train_micro_batch_size_per_gpu': 2, 'gradient_accumulation_steps': 1, 'steps_per_print': 50, 'gradient_clipping': 0.1, 'zero_optimization': {'stage': 2, 'cpu_offload': False, 'contiguous_gradients': False, 'overlap_comm': True, 'reduce_scatter': True, 'reduce_bucket_size': 1000000000, 'allgather_bucket_size': 1000000000, 'load_from_fp32_weights': False}, 'zero_allow_untested_optimizer': True, 'bf16': {'enabled': False}, 'fp16': {'enabled': True}, 'loss_scale': 0, 'loss_scale_window': 400, 'hysteresis': 2, 'min_loss_scale': 1, 'activation_checkpointing': {'partition_activations': False, 'contiguous_memory_optimization': False}, 'wall_clock_breakdown': False}
[2024-09-09 14:17:07,099] [INFO] [RANK 0]   deepscale .................... False
[2024-09-09 14:17:07,099] [INFO] [RANK 0]   deepscale_config ............. None
[2024-09-09 14:17:07,100] [INFO] [RANK 0]   model_config ................. {'scale_factor': 1.15258426, 'disable_first_stage_autocast': True, 'not_trainable_prefixes': ['all'], 'log_keys': ['txt'], 'denoiser_config': {'target': 'sgm.modules.diffusionmodules.denoiser.DiscreteDenoiser', 'params': {'num_idx': 1000, 'quantize_c_noise': False, 'weighting_config': {'target': 'sgm.modules.diffusionmodules.denoiser_weighting.EpsWeighting'}, 'scaling_config': {'target': 'sgm.modules.diffusionmodules.denoiser_scaling.VideoScaling'}, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 3.0}}}}, 'network_config': {'target': 'dit_video_concat.DiffusionTransformer', 'params': {'time_embed_dim': 512, 'elementwise_affine': True, 'num_frames': 49, 'time_compressed_rate': 4, 'latent_width': 90, 'latent_height': 60, 'num_layers': 30, 'patch_size': 2, 'in_channels': 16, 'out_channels': 16, 'hidden_size': 1920, 'adm_in_channels': 256, 'num_attention_heads': 30, 'transformer_args': {'checkpoint_activations': True, 'vocab_size': 1, 'max_sequence_length': 64, 'layernorm_order': 'pre', 'skip_init': False, 'model_parallel_size': 1, 'is_decoder': False, 'num_layers': 30, 'hidden_size': 1920, 'num_attention_heads': 30, 'parallel_output': True}, 'modules': {'pos_embed_config': {'target': 'dit_video_concat.Basic3DPositionEmbeddingMixin', 'params': {'text_length': 226, 'height_interpolation': 1.875, 'width_interpolation': 1.875}}, 'lora_config': {'target': 'sat.model.finetune.lora2.LoraMixin', 'params': {'r': 128}}, 'patch_embed_config': {'target': 'dit_video_concat.ImagePatchEmbeddingMixin', 'params': {'text_hidden_size': 4096}}, 'adaln_layer_config': {'target': 'dit_video_concat.AdaLNMixin', 'params': {'qk_ln': True}}, 'final_layer_config': {'target': 'dit_video_concat.FinalLayerMixin'}}, 'dtype': 'fp16'}}, 'conditioner_config': {'target': 'sgm.modules.GeneralConditioner', 'params': {'emb_models': [{'is_trainable': False, 'input_key': 'txt', 'ucg_rate': 0.1, 'target': 'sgm.modules.encoders.modules.FrozenT5Embedder', 'params': {'model_dir': '/mnt/ceph/develop/jiawei/model_checkpoint/t5-v1_1-xxl', 'max_length': 226}}]}}, 'first_stage_config': {'target': 'vae_modules.autoencoder.VideoAutoencoderInferenceWrapper', 'params': {'cp_size': 1, 'ckpt_path': '/mnt/ceph/develop/jiawei/model_checkpoint/CogVideoX-2b-sat/vae/3d-vae.pt', 'ignore_keys': ['loss'], 'loss_config': {'target': 'torch.nn.Identity'}, 'regularizer_config': {'target': 'vae_modules.regularizers.DiagonalGaussianRegularizer'}, 'encoder_config': {'target': 'vae_modules.cp_enc_dec.ContextParallelEncoder3D', 'params': {'double_z': True, 'z_channels': 16, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 2, 4], 'attn_resolutions': [], 'num_res_blocks': 3, 'dropout': 0.0, 'gather_norm': True}}, 'decoder_config': {'target': 'vae_modules.cp_enc_dec.ContextParallelDecoder3D', 'params': {'double_z': True, 'z_channels': 16, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 2, 4], 'attn_resolutions': [], 'num_res_blocks': 3, 'dropout': 0.0, 'gather_norm': False}}}}, 'loss_fn_config': {'target': 'sgm.modules.diffusionmodules.loss.VideoDiffusionLoss', 'params': {'offset_noise_level': 0, 'sigma_sampler_config': {'target': 'sgm.modules.diffusionmodules.sigma_sampling.DiscreteSampling', 'params': {'uniform_sampling': True, 'num_idx': 1000, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 3.0}}}}}}, 'sampler_config': {'target': 'sgm.modules.diffusionmodules.sampling.VPSDEDPMPP2MSampler', 'params': {'num_steps': 50, 'verbose': True, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 3.0}}, 'guider_config': {'target': 'sgm.modules.diffusionmodules.guiders.DynamicCFG', 'params': {'scale': 6, 'exp': 5, 'num_steps': 50}}}}}
[2024-09-09 14:17:07,102] [INFO] [RANK 0]   data_config .................. {'target': 'data_video.SFTDataset', 'params': {'video_size': [480, 720], 'fps': 8, 'max_num_frames': 49, 'skip_frms_num': 3.0}}
[2024-09-09 14:17:07,102] [INFO] [RANK 0]   cuda ......................... True
[2024-09-09 14:17:07,102] [INFO] [RANK 0]   rank ......................... 0
[2024-09-09 14:17:07,102] [INFO] [RANK 0]   world_size ................... 1
[2024-09-09 14:17:07,102] [INFO] [RANK 0]   deepspeed_activation_checkpointing  True
[2024-09-09 14:17:07,102] [INFO] [RANK 0]   master_ip .................... nm04-a800-node083
[2024-09-09 14:17:07,102] [INFO] [RANK 0]   master_port .................. 10185
[2024-09-09 14:17:07,102] [INFO] [RANK 0]   log_config ................... [{'model': {'scale_factor': 1.15258426, 'disable_first_stage_autocast': True, 'not_trainable_prefixes': ['all'], 'log_keys': ['txt'], 'denoiser_config': {'target': 'sgm.modules.diffusionmodules.denoiser.DiscreteDenoiser', 'params': {'num_idx': 1000, 'quantize_c_noise': False, 'weighting_config': {'target': 'sgm.modules.diffusionmodules.denoiser_weighting.EpsWeighting'}, 'scaling_config': {'target': 'sgm.modules.diffusionmodules.denoiser_scaling.VideoScaling'}, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 3.0}}}}, 'network_config': {'target': 'dit_video_concat.DiffusionTransformer', 'params': {'time_embed_dim': 512, 'elementwise_affine': True, 'num_frames': 49, 'time_compressed_rate': 4, 'latent_width': 90, 'latent_height': 60, 'num_layers': 30, 'patch_size': 2, 'in_channels': 16, 'out_channels': 16, 'hidden_size': 1920, 'adm_in_channels': 256, 'num_attention_heads': 30, 'transformer_args': {'checkpoint_activations': True, 'vocab_size': 1, 'max_sequence_length': 64, 'layernorm_order': 'pre', 'skip_init': False, 'model_parallel_size': 1, 'is_decoder': False}, 'modules': {'pos_embed_config': {'target': 'dit_video_concat.Basic3DPositionEmbeddingMixin', 'params': {'text_length': 226, 'height_interpolation': 1.875, 'width_interpolation': 1.875}}, 'lora_config': {'target': 'sat.model.finetune.lora2.LoraMixin', 'params': {'r': 128}}, 'patch_embed_config': {'target': 'dit_video_concat.ImagePatchEmbeddingMixin', 'params': {'text_hidden_size': 4096}}, 'adaln_layer_config': {'target': 'dit_video_concat.AdaLNMixin', 'params': {'qk_ln': True}}, 'final_layer_config': {'target': 'dit_video_concat.FinalLayerMixin'}}}}, 'conditioner_config': {'target': 'sgm.modules.GeneralConditioner', 'params': {'emb_models': [{'is_trainable': False, 'input_key': 'txt', 'ucg_rate': 0.1, 'target': 'sgm.modules.encoders.modules.FrozenT5Embedder', 'params': {'model_dir': '/mnt/ceph/develop/jiawei/model_checkpoint/t5-v1_1-xxl', 'max_length': 226}}]}}, 'first_stage_config': {'target': 'vae_modules.autoencoder.VideoAutoencoderInferenceWrapper', 'params': {'cp_size': 1, 'ckpt_path': '/mnt/ceph/develop/jiawei/model_checkpoint/CogVideoX-2b-sat/vae/3d-vae.pt', 'ignore_keys': ['loss'], 'loss_config': {'target': 'torch.nn.Identity'}, 'regularizer_config': {'target': 'vae_modules.regularizers.DiagonalGaussianRegularizer'}, 'encoder_config': {'target': 'vae_modules.cp_enc_dec.ContextParallelEncoder3D', 'params': {'double_z': True, 'z_channels': 16, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 2, 4], 'attn_resolutions': [], 'num_res_blocks': 3, 'dropout': 0.0, 'gather_norm': True}}, 'decoder_config': {'target': 'vae_modules.cp_enc_dec.ContextParallelDecoder3D', 'params': {'double_z': True, 'z_channels': 16, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 2, 4], 'attn_resolutions': [], 'num_res_blocks': 3, 'dropout': 0.0, 'gather_norm': False}}}}, 'loss_fn_config': {'target': 'sgm.modules.diffusionmodules.loss.VideoDiffusionLoss', 'params': {'offset_noise_level': 0, 'sigma_sampler_config': {'target': 'sgm.modules.diffusionmodules.sigma_sampling.DiscreteSampling', 'params': {'uniform_sampling': True, 'num_idx': 1000, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 3.0}}}}}}, 'sampler_config': {'target': 'sgm.modules.diffusionmodules.sampling.VPSDEDPMPP2MSampler', 'params': {'num_steps': 50, 'verbose': True, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 3.0}}, 'guider_config': {'target': 'sgm.modules.diffusionmodules.guiders.DynamicCFG', 'params': {'scale': 6, 'exp': 5, 'num_steps': 50}}}}}}, {'args': {'checkpoint_activations': True, 'model_parallel_size': 1, 'experiment_name': 'lora-disney', 'mode': 'finetune', 'load': '/mnt/ceph/develop/jiawei/model_checkpoint/CogVideoX-2b-sat/transformer', 'no_load_rng': True, 'train_iters': 1000, 'eval_iters': 1, 'eval_interval': 100, 'eval_batch_size': 1, 'save': '/mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora', 'save_interval': 500, 'log_interval': 20, 'train_data': ['/mnt/ceph/develop/jiawei/lora_dataset/cogvideo_lora_dataset'], 'valid_data': ['/mnt/ceph/develop/jiawei/lora_dataset/cogvideo_lora_dataset'], 'split': '1,0,0', 'num_workers': 8, 'force_train': True, 'only_log_video_latents': False, 'wandb': True, 'wandb_project_name': 'cogvideox_ckpts_2b_lora', 'summary_dir': '/mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora'}, 'data': {'target': 'data_video.SFTDataset', 'params': {'video_size': [480, 720], 'fps': 8, 'max_num_frames': 49, 'skip_frms_num': 3.0}}, 'deepspeed': {'train_micro_batch_size_per_gpu': 2, 'gradient_accumulation_steps': 1, 'steps_per_print': 50, 'gradient_clipping': 0.1, 'zero_optimization': {'stage': 2, 'cpu_offload': False, 'contiguous_gradients': False, 'overlap_comm': True, 'reduce_scatter': True, 'reduce_bucket_size': 1000000000, 'allgather_bucket_size': 1000000000, 'load_from_fp32_weights': False}, 'zero_allow_untested_optimizer': True, 'bf16': {'enabled': False}, 'fp16': {'enabled': True}, 'loss_scale': 0, 'loss_scale_window': 400, 'hysteresis': 2, 'min_loss_scale': 1, 'optimizer': {'type': 'sat.ops.FusedEmaAdam', 'params': {'lr': '1E-3', 'betas': [0.9, 0.95], 'eps': '1e-8', 'weight_decay': '1e-4'}}, 'activation_checkpointing': {'partition_activations': False, 'contiguous_memory_optimization': False}, 'wall_clock_breakdown': False}}]
[2024-09-09 14:17:07,104] [INFO] [RANK 0]   do_train ..................... True
[2024-09-09 14:17:07,104] [INFO] [RANK 0]   val_last_shape ............... []
[2024-09-09 14:17:07,104] [INFO] [RANK 0]   val_drop_number .............. 0
[2024-09-09 14:17:07,104] [INFO] [RANK 0]   do_valid ..................... True
[2024-09-09 14:17:07,104] [INFO] [RANK 0]   do_test ...................... False
[2024-09-09 14:17:07,104] [INFO] [RANK 0]   iteration .................... 0
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
[93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, input, weight, bias=None):
/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
wandb: Currently logged in as: dmeck. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora/wandb/run-20240909_141711-gdt6b4zh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lora-disney-09-09-14-16
wandb: â­ï¸ View project at https://wandb.ai/dmeck/cogvideox_ckpts_2b_lora
wandb: ðŸš€ View run at https://wandb.ai/dmeck/cogvideox_ckpts_2b_lora/runs/gdt6b4zh
/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/kornia/feature/lightglue.py:44: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)
no module 'xformers'. Processing without...
no module 'xformers'. Processing without...
[2024-09-09 14:17:20,023] [INFO] using world size: 1
[2024-09-09 14:17:20,024] [INFO] Will override arguments with manually specified deepspeed_config!
[2024-09-09 14:17:20,031] [INFO] [RANK 0] > initializing model parallel with size 1
[2024-09-09 14:17:20,032] [INFO] [comm.py:637:init_distributed] cdb=None
Namespace(base=['configs/cogvideox_2b_lora.yaml', 'configs/sft.yaml'], model_parallel_size=1, force_pretrain=False, device=0, debug=False, log_image=True, output_dir='samples', input_dir=None, input_type='cli', input_file='input.txt', final_size=2048, sdedit=False, grid_num_rows=1, force_inference=False, lcm_steps=None, sampling_num_frames=32, sampling_fps=8, only_save_latents=False, only_log_video_latents=False, latent_channels=32, image2video=False, experiment_name='lora-disney', train_iters=1000, batch_size=2, lr=0.001, mode='finetune', seed=42, zero_stage=0, checkpoint_activations=True, checkpoint_num_layers=1, checkpoint_skip_layers=0, fp16=True, bf16=False, gradient_accumulation_steps=1, profiling=-1, epochs=None, log_interval=20, summary_dir='/mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora', save_args=False, lr_decay_iters=None, lr_decay_style='linear', lr_decay_ratio=0.1, warmup=0.01, weight_decay=0.0001, save='/mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora', load='/mnt/ceph/develop/jiawei/model_checkpoint/CogVideoX-2b-sat/transformer', force_train=True, save_interval=500, no_save_rng=False, no_load_rng=True, resume_dataloader=False, distributed_backend='nccl', local_rank=0, exit_interval=None, wandb=True, wandb_project_name='cogvideox_ckpts_2b_lora_base_001', eval_batch_size=1, eval_iters=1, eval_interval=100, strict_eval=False, train_data=['/mnt/ceph/develop/jiawei/lora_dataset/cogvideo_lora_dataset'], train_data_weights=None, iterable_dataset=False, iterable_dataset_eval='', batch_from_same_dataset=False, valid_data=['/mnt/ceph/develop/jiawei/lora_dataset/cogvideo_lora_dataset'], test_data=None, split='1,0,0', num_workers=8, block_size=10000, prefetch_factor=4, deepspeed=True, deepspeed_config={'train_micro_batch_size_per_gpu': 2, 'gradient_accumulation_steps': 1, 'steps_per_print': 50, 'gradient_clipping': 0.1, 'zero_optimization': {'stage': 2, 'cpu_offload': False, 'contiguous_gradients': False, 'overlap_comm': True, 'reduce_scatter': True, 'reduce_bucket_size': 1000000000, 'allgather_bucket_size': 1000000000, 'load_from_fp32_weights': False}, 'zero_allow_untested_optimizer': True, 'bf16': {'enabled': False}, 'fp16': {'enabled': True}, 'loss_scale': 0, 'loss_scale_window': 400, 'hysteresis': 2, 'min_loss_scale': 1, 'optimizer': {'type': 'sat.ops.FusedEmaAdam', 'params': {'lr': 0.001, 'betas': [0.9, 0.95], 'eps': 1e-08, 'weight_decay': 0.0001}}, 'activation_checkpointing': {'partition_activations': False, 'contiguous_memory_optimization': False}, 'wall_clock_breakdown': False}, deepscale=False, deepscale_config=None, model_config={'scale_factor': 1.15258426, 'disable_first_stage_autocast': True, 'not_trainable_prefixes': ['all'], 'log_keys': ['txt'], 'denoiser_config': {'target': 'sgm.modules.diffusionmodules.denoiser.DiscreteDenoiser', 'params': {'num_idx': 1000, 'quantize_c_noise': False, 'weighting_config': {'target': 'sgm.modules.diffusionmodules.denoiser_weighting.EpsWeighting'}, 'scaling_config': {'target': 'sgm.modules.diffusionmodules.denoiser_scaling.VideoScaling'}, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 3.0}}}}, 'network_config': {'target': 'dit_video_concat.DiffusionTransformer', 'params': {'time_embed_dim': 512, 'elementwise_affine': True, 'num_frames': 49, 'time_compressed_rate': 4, 'latent_width': 90, 'latent_height': 60, 'num_layers': 30, 'patch_size': 2, 'in_channels': 16, 'out_channels': 16, 'hidden_size': 1920, 'adm_in_channels': 256, 'num_attention_heads': 30, 'transformer_args': {'checkpoint_activations': True, 'vocab_size': 1, 'max_sequence_length': 64, 'layernorm_order': 'pre', 'skip_init': False, 'model_parallel_size': 1, 'is_decoder': False}, 'modules': {'pos_embed_config': {'target': 'dit_video_concat.Basic3DPositionEmbeddingMixin', 'params': {'text_length': 226, 'height_interpolation': 1.875, 'width_interpolation': 1.875}}, 'lora_config': {'target': 'sat.model.finetune.lora2.LoraMixin', 'params': {'r': 128}}, 'patch_embed_config': {'target': 'dit_video_concat.ImagePatchEmbeddingMixin', 'params': {'text_hidden_size': 4096}}, 'adaln_layer_config': {'target': 'dit_video_concat.AdaLNMixin', 'params': {'qk_ln': True}}, 'final_layer_config': {'target': 'dit_video_concat.FinalLayerMixin'}}}}, 'conditioner_config': {'target': 'sgm.modules.GeneralConditioner', 'params': {'emb_models': [{'is_trainable': False, 'input_key': 'txt', 'ucg_rate': 0.1, 'target': 'sgm.modules.encoders.modules.FrozenT5Embedder', 'params': {'model_dir': '/mnt/ceph/develop/jiawei/model_checkpoint/t5-v1_1-xxl', 'max_length': 226}}]}}, 'first_stage_config': {'target': 'vae_modules.autoencoder.VideoAutoencoderInferenceWrapper', 'params': {'cp_size': 1, 'ckpt_path': '/mnt/ceph/develop/jiawei/model_checkpoint/CogVideoX-2b-sat/vae/3d-vae.pt', 'ignore_keys': ['loss'], 'loss_config': {'target': 'torch.nn.Identity'}, 'regularizer_config': {'target': 'vae_modules.regularizers.DiagonalGaussianRegularizer'}, 'encoder_config': {'target': 'vae_modules.cp_enc_dec.ContextParallelEncoder3D', 'params': {'double_z': True, 'z_channels': 16, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 2, 4], 'attn_resolutions': [], 'num_res_blocks': 3, 'dropout': 0.0, 'gather_norm': True}}, 'decoder_config': {'target': 'vae_modules.cp_enc_dec.ContextParallelDecoder3D', 'params': {'double_z': True, 'z_channels': 16, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 2, 4], 'attn_resolutions': [], 'num_res_blocks': 3, 'dropout': 0.0, 'gather_norm': False}}}}, 'loss_fn_config': {'target': 'sgm.modules.diffusionmodules.loss.VideoDiffusionLoss', 'params': {'offset_noise_level': 0, 'sigma_sampler_config': {'target': 'sgm.modules.diffusionmodules.sigma_sampling.DiscreteSampling', 'params': {'uniform_sampling': True, 'num_idx': 1000, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 3.0}}}}}}, 'sampler_config': {'target': 'sgm.modules.diffusionmodules.sampling.VPSDEDPMPP2MSampler', 'params': {'num_steps': 50, 'verbose': True, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 3.0}}, 'guider_config': {'target': 'sgm.modules.diffusionmodules.guiders.DynamicCFG', 'params': {'scale': 6, 'exp': 5, 'num_steps': 50}}}}}, data_config={'target': 'data_video.SFTDataset', 'params': {'video_size': [480, 720], 'fps': 8, 'max_num_frames': 49, 'skip_frms_num': 3.0}}, cuda=True, rank=0, world_size=1, deepspeed_activation_checkpointing=True, master_ip='nm04-a800-node083', master_port='9323', log_config=[{'model': {'scale_factor': 1.15258426, 'disable_first_stage_autocast': True, 'not_trainable_prefixes': ['all'], 'log_keys': ['txt'], 'denoiser_config': {'target': 'sgm.modules.diffusionmodules.denoiser.DiscreteDenoiser', 'params': {'num_idx': 1000, 'quantize_c_noise': False, 'weighting_config': {'target': 'sgm.modules.diffusionmodules.denoiser_weighting.EpsWeighting'}, 'scaling_config': {'target': 'sgm.modules.diffusionmodules.denoiser_scaling.VideoScaling'}, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 3.0}}}}, 'network_config': {'target': 'dit_video_concat.DiffusionTransformer', 'params': {'time_embed_dim': 512, 'elementwise_affine': True, 'num_frames': 49, 'time_compressed_rate': 4, 'latent_width': 90, 'latent_height': 60, 'num_layers': 30, 'patch_size': 2, 'in_channels': 16, 'out_channels': 16, 'hidden_size': 1920, 'adm_in_channels': 256, 'num_attention_heads': 30, 'transformer_args': {'checkpoint_activations': True, 'vocab_size': 1, 'max_sequence_length': 64, 'layernorm_order': 'pre', 'skip_init': False, 'model_parallel_size': 1, 'is_decoder': False}, 'modules': {'pos_embed_config': {'target': 'dit_video_concat.Basic3DPositionEmbeddingMixin', 'params': {'text_length': 226, 'height_interpolation': 1.875, 'width_interpolation': 1.875}}, 'lora_config': {'target': 'sat.model.finetune.lora2.LoraMixin', 'params': {'r': 128}}, 'patch_embed_config': {'target': 'dit_video_concat.ImagePatchEmbeddingMixin', 'params': {'text_hidden_size': 4096}}, 'adaln_layer_config': {'target': 'dit_video_concat.AdaLNMixin', 'params': {'qk_ln': True}}, 'final_layer_config': {'target': 'dit_video_concat.FinalLayerMixin'}}}}, 'conditioner_config': {'target': 'sgm.modules.GeneralConditioner', 'params': {'emb_models': [{'is_trainable': False, 'input_key': 'txt', 'ucg_rate': 0.1, 'target': 'sgm.modules.encoders.modules.FrozenT5Embedder', 'params': {'model_dir': '/mnt/ceph/develop/jiawei/model_checkpoint/t5-v1_1-xxl', 'max_length': 226}}]}}, 'first_stage_config': {'target': 'vae_modules.autoencoder.VideoAutoencoderInferenceWrapper', 'params': {'cp_size': 1, 'ckpt_path': '/mnt/ceph/develop/jiawei/model_checkpoint/CogVideoX-2b-sat/vae/3d-vae.pt', 'ignore_keys': ['loss'], 'loss_config': {'target': 'torch.nn.Identity'}, 'regularizer_config': {'target': 'vae_modules.regularizers.DiagonalGaussianRegularizer'}, 'encoder_config': {'target': 'vae_modules.cp_enc_dec.ContextParallelEncoder3D', 'params': {'double_z': True, 'z_channels': 16, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 2, 4], 'attn_resolutions': [], 'num_res_blocks': 3, 'dropout': 0.0, 'gather_norm': True}}, 'decoder_config': {'target': 'vae_modules.cp_enc_dec.ContextParallelDecoder3D', 'params': {'double_z': True, 'z_channels': 16, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 2, 4], 'attn_resolutions': [], 'num_res_blocks': 3, 'dropout': 0.0, 'gather_norm': False}}}}, 'loss_fn_config': {'target': 'sgm.modules.diffusionmodules.loss.VideoDiffusionLoss', 'params': {'offset_noise_level': 0, 'sigma_sampler_config': {'target': 'sgm.modules.diffusionmodules.sigma_sampling.DiscreteSampling', 'params': {'uniform_sampling': True, 'num_idx': 1000, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 3.0}}}}}}, 'sampler_config': {'target': 'sgm.modules.diffusionmodules.sampling.VPSDEDPMPP2MSampler', 'params': {'num_steps': 50, 'verbose': True, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 3.0}}, 'guider_config': {'target': 'sgm.modules.diffusionmodules.guiders.DynamicCFG', 'params': {'scale': 6, 'exp': 5, 'num_steps': 50}}}}}}, {'args': {'checkpoint_activations': True, 'model_parallel_size': 1, 'experiment_name': 'lora-disney', 'mode': 'finetune', 'load': '/mnt/ceph/develop/jiawei/model_checkpoint/CogVideoX-2b-sat/transformer', 'no_load_rng': True, 'train_iters': 1000, 'eval_iters': 1, 'eval_interval': 100, 'eval_batch_size': 1, 'save': '/mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora', 'save_interval': 500, 'log_interval': 20, 'train_data': ['/mnt/ceph/develop/jiawei/lora_dataset/cogvideo_lora_dataset'], 'valid_data': ['/mnt/ceph/develop/jiawei/lora_dataset/cogvideo_lora_dataset'], 'split': '1,0,0', 'num_workers': 8, 'force_train': True, 'only_log_video_latents': False, 'wandb': True, 'wandb_project_name': 'cogvideox_ckpts_2b_lora_base_001', 'summary_dir': '/mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora'}, 'data': {'target': 'data_video.SFTDataset', 'params': {'video_size': [480, 720], 'fps': 8, 'max_num_frames': 49, 'skip_frms_num': 3.0}}, 'deepspeed': {'train_micro_batch_size_per_gpu': 2, 'gradient_accumulation_steps': 1, 'steps_per_print': 50, 'gradient_clipping': 0.1, 'zero_optimization': {'stage': 2, 'cpu_offload': False, 'contiguous_gradients': False, 'overlap_comm': True, 'reduce_scatter': True, 'reduce_bucket_size': 1000000000, 'allgather_bucket_size': 1000000000, 'load_from_fp32_weights': False}, 'zero_allow_untested_optimizer': True, 'bf16': {'enabled': False}, 'fp16': {'enabled': True}, 'loss_scale': 0, 'loss_scale_window': 400, 'hysteresis': 2, 'min_loss_scale': 1, 'optimizer': {'type': 'sat.ops.FusedEmaAdam', 'params': {'lr': '1E-3', 'betas': [0.9, 0.95], 'eps': '1e-8', 'weight_decay': '1e-4'}}, 'activation_checkpointing': {'partition_activations': False, 'contiguous_memory_optimization': False}, 'wall_clock_breakdown': False}}])
[2024-09-09 14:17:21,172] [INFO] [RANK 0] building SATVideoDiffusionEngine model ...
[2024-09-09 14:17:31,953] [WARNING] [RANK 0] Failed to load bitsandbytes:No module named 'bitsandbytes'
[2024-09-09 14:17:31,953] [INFO] [RANK 0] replacing layer 0 attention with lora
[2024-09-09 14:17:31,980] [INFO] [RANK 0] replacing layer 1 attention with lora
[2024-09-09 14:17:32,002] [INFO] [RANK 0] replacing layer 2 attention with lora
[2024-09-09 14:17:32,024] [INFO] [RANK 0] replacing layer 3 attention with lora
[2024-09-09 14:17:32,046] [INFO] [RANK 0] replacing layer 4 attention with lora
[2024-09-09 14:17:32,069] [INFO] [RANK 0] replacing layer 5 attention with lora
[2024-09-09 14:17:32,091] [INFO] [RANK 0] replacing layer 6 attention with lora
[2024-09-09 14:17:32,115] [INFO] [RANK 0] replacing layer 7 attention with lora
[2024-09-09 14:17:32,140] [INFO] [RANK 0] replacing layer 8 attention with lora
[2024-09-09 14:17:32,165] [INFO] [RANK 0] replacing layer 9 attention with lora
[2024-09-09 14:17:32,188] [INFO] [RANK 0] replacing layer 10 attention with lora
[2024-09-09 14:17:32,214] [INFO] [RANK 0] replacing layer 11 attention with lora
[2024-09-09 14:17:32,236] [INFO] [RANK 0] replacing layer 12 attention with lora
[2024-09-09 14:17:32,258] [INFO] [RANK 0] replacing layer 13 attention with lora
[2024-09-09 14:17:32,282] [INFO] [RANK 0] replacing layer 14 attention with lora
[2024-09-09 14:17:32,305] [INFO] [RANK 0] replacing layer 15 attention with lora
[2024-09-09 14:17:32,328] [INFO] [RANK 0] replacing layer 16 attention with lora
[2024-09-09 14:17:32,351] [INFO] [RANK 0] replacing layer 17 attention with lora
[2024-09-09 14:17:32,374] [INFO] [RANK 0] replacing layer 18 attention with lora
[2024-09-09 14:17:32,397] [INFO] [RANK 0] replacing layer 19 attention with lora
[2024-09-09 14:17:32,422] [INFO] [RANK 0] replacing layer 20 attention with lora
[2024-09-09 14:17:32,444] [INFO] [RANK 0] replacing layer 21 attention with lora
[2024-09-09 14:17:32,464] [INFO] [RANK 0] replacing layer 22 attention with lora
[2024-09-09 14:17:32,485] [INFO] [RANK 0] replacing layer 23 attention with lora
[2024-09-09 14:17:32,505] [INFO] [RANK 0] replacing layer 24 attention with lora
[2024-09-09 14:17:32,526] [INFO] [RANK 0] replacing layer 25 attention with lora
[2024-09-09 14:17:32,547] [INFO] [RANK 0] replacing layer 26 attention with lora
[2024-09-09 14:17:32,568] [INFO] [RANK 0] replacing layer 27 attention with lora
[2024-09-09 14:17:32,588] [INFO] [RANK 0] replacing layer 28 attention with lora
[2024-09-09 14:17:32,609] [INFO] [RANK 0] replacing layer 29 attention with lora
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  1.58it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.67it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.66it/s]
Initialized embedder #0: FrozenT5Embedder with 4762310656 params. Trainable: False
Working with z of shape (1, 16, 32, 32) = 16384 dimensions.
/mnt/ceph/develop/jiawei/CogVideo/sat/vae_modules/autoencoder.py:565: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  sd = torch.load(path, map_location="cpu")["state_dict"]
Deleting key loss.logvar from state_dict.
Deleting key loss.perceptual_loss.scaling_layer.shift from state_dict.
Deleting key loss.perceptual_loss.scaling_layer.scale from state_dict.
Deleting key loss.perceptual_loss.net.slice1.0.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice1.0.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice1.2.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice1.2.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice2.5.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice2.5.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice2.7.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice2.7.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice3.10.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice3.10.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice3.12.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice3.12.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice3.14.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice3.14.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice4.17.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice4.17.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice4.19.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice4.19.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice4.21.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice4.21.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice5.24.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice5.24.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice5.26.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice5.26.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice5.28.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice5.28.bias from state_dict.
Deleting key loss.perceptual_loss.lin0.model.1.weight from state_dict.
Deleting key loss.perceptual_loss.lin1.model.1.weight from state_dict.
Deleting key loss.perceptual_loss.lin2.model.1.weight from state_dict.
Deleting key loss.perceptual_loss.lin3.model.1.weight from state_dict.
Deleting key loss.perceptual_loss.lin4.model.1.weight from state_dict.
Deleting key loss.discriminator.blocks.0.downsample_res.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.0.downsample_res.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.0.net.0.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.0.net.0.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.0.net.2.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.0.net.2.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.0.downsample.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.0.downsample.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.1.downsample_res.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.1.downsample_res.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.1.net.0.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.1.net.0.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.1.net.2.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.1.net.2.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.1.downsample.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.1.downsample.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.2.downsample_res.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.2.downsample_res.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.2.net.0.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.2.net.0.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.2.net.2.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.2.net.2.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.2.downsample.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.2.downsample.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.3.downsample_res.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.3.downsample_res.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.3.net.0.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.3.net.0.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.3.net.2.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.3.net.2.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.3.downsample.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.3.downsample.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.4.0.conv_res.weight from state_dict.
Deleting key loss.discriminator.blocks.4.0.conv_res.bias from state_dict.
Deleting key loss.discriminator.blocks.4.0.net.0.weight from state_dict.
Deleting key loss.discriminator.blocks.4.0.net.0.bias from state_dict.
Deleting key loss.discriminator.blocks.4.0.net.2.weight from state_dict.
Deleting key loss.discriminator.blocks.4.0.net.2.bias from state_dict.
Deleting key loss.discriminator.blocks.4.0.downsample.1.weight from state_dict.
Deleting key loss.discriminator.blocks.4.0.downsample.1.bias from state_dict.
Deleting key loss.discriminator.blocks.4.1.0.fn.norm.gamma from state_dict.
Deleting key loss.discriminator.blocks.4.1.0.fn.attn.to_q.0.weight from state_dict.
Deleting key loss.discriminator.blocks.4.1.0.fn.attn.to_kv.0.weight from state_dict.
Deleting key loss.discriminator.blocks.4.1.0.fn.attn.to_out.0.weight from state_dict.
Deleting key loss.discriminator.blocks.4.1.1.fn.norm.gamma from state_dict.
Deleting key loss.discriminator.blocks.4.1.1.fn.net.0.weight from state_dict.
Deleting key loss.discriminator.blocks.4.1.1.fn.net.0.bias from state_dict.
Deleting key loss.discriminator.blocks.4.1.1.fn.net.2.weight from state_dict.
Deleting key loss.discriminator.blocks.4.1.1.fn.net.2.bias from state_dict.
Deleting key loss.discriminator.blocks.5.0.conv_res.weight from state_dict.
Deleting key loss.discriminator.blocks.5.0.conv_res.bias from state_dict.
Deleting key loss.discriminator.blocks.5.0.net.0.weight from state_dict.
Deleting key loss.discriminator.blocks.5.0.net.0.bias from state_dict.
Deleting key loss.discriminator.blocks.5.0.net.2.weight from state_dict.
Deleting key loss.discriminator.blocks.5.0.net.2.bias from state_dict.
Deleting key loss.discriminator.blocks.5.0.downsample.1.weight from state_dict.
Deleting key loss.discriminator.blocks.5.0.downsample.1.bias from state_dict.
Deleting key loss.discriminator.blocks.5.1.0.fn.norm.gamma from state_dict.
Deleting key loss.discriminator.blocks.5.1.0.fn.attn.to_q.0.weight from state_dict.
Deleting key loss.discriminator.blocks.5.1.0.fn.attn.to_kv.0.weight from state_dict.
Deleting key loss.discriminator.blocks.5.1.0.fn.attn.to_out.0.weight from state_dict.
Deleting key loss.discriminator.blocks.5.1.1.fn.norm.gamma from state_dict.
Deleting key loss.discriminator.blocks.5.1.1.fn.net.0.weight from state_dict.
Deleting key loss.discriminator.blocks.5.1.1.fn.net.0.bias from state_dict.
Deleting key loss.discriminator.blocks.5.1.1.fn.net.2.weight from state_dict.
Deleting key loss.discriminator.blocks.5.1.1.fn.net.2.bias from state_dict.
Deleting key loss.discriminator.blocks.6.0.conv_res.weight from state_dict.
Deleting key loss.discriminator.blocks.6.0.conv_res.bias from state_dict.
Deleting key loss.discriminator.blocks.6.0.net.0.weight from state_dict.
Deleting key loss.discriminator.blocks.6.0.net.0.bias from state_dict.
Deleting key loss.discriminator.blocks.6.0.net.2.weight from state_dict.
Deleting key loss.discriminator.blocks.6.0.net.2.bias from state_dict.
Deleting key loss.discriminator.blocks.6.1.0.fn.norm.gamma from state_dict.
Deleting key loss.discriminator.blocks.6.1.0.fn.attn.to_q.0.weight from state_dict.
Deleting key loss.discriminator.blocks.6.1.0.fn.attn.to_kv.0.weight from state_dict.
Deleting key loss.discriminator.blocks.6.1.0.fn.attn.to_out.0.weight from state_dict.
Deleting key loss.discriminator.blocks.6.1.1.fn.norm.gamma from state_dict.
Deleting key loss.discriminator.blocks.6.1.1.fn.net.0.weight from state_dict.
Deleting key loss.discriminator.blocks.6.1.1.fn.net.0.bias from state_dict.
Deleting key loss.discriminator.blocks.6.1.1.fn.net.2.weight from state_dict.
Deleting key loss.discriminator.blocks.6.1.1.fn.net.2.bias from state_dict.
Deleting key loss.discriminator.to_logits.0.weight from state_dict.
Deleting key loss.discriminator.to_logits.0.bias from state_dict.
Deleting key loss.discriminator.to_logits.3.weight from state_dict.
Deleting key loss.discriminator.to_logits.3.bias from state_dict.
Missing keys:  []
Unexpected keys:  []
Restored from /mnt/ceph/develop/jiawei/model_checkpoint/CogVideoX-2b-sat/vae/3d-vae.pt
[2024-09-09 14:17:36,666] [INFO] [RANK 0]  > number of parameters on model parallel rank 0: 6764790755
[2024-09-09 14:17:48,601] [INFO] [checkpointing.py:541:forward] Activation Checkpointing Information
[2024-09-09 14:17:48,601] [INFO] [checkpointing.py:542:forward] ----Partition Activations False, CPU CHECKPOINTING False
[2024-09-09 14:17:48,605] [INFO] [checkpointing.py:543:forward] ----contiguous Memory Checkpointing False with None total layers
[2024-09-09 14:17:48,605] [INFO] [checkpointing.py:545:forward] ----Synchronization False
[2024-09-09 14:17:48,605] [INFO] [checkpointing.py:546:forward] ----Profiling time in checkpointing False
[2024-09-09 14:17:50,649] [INFO] [RANK 0] global rank 0 is loading checkpoint /mnt/ceph/develop/jiawei/model_checkpoint/CogVideoX-2b-sat/transformer/1000/mp_rank_00_model_states.pt
/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/sat/training/model_io.py:286: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  sd = torch.load(checkpoint_name, map_location='cpu')
[2024-09-09 14:17:53,477] [INFO] [RANK 0] > successfully loaded /mnt/ceph/develop/jiawei/model_checkpoint/CogVideoX-2b-sat/transformer/1000/mp_rank_00_model_states.pt
[2024-09-09 14:17:53,977] [INFO] [RANK 0] ***** Total trainable parameters: 58982400 *****
[2024-09-09 14:17:53,977] [INFO] [RANK 0] [<class 'sat.ops.layernorm.LayerNorm'>, <class 'torch.nn.modules.normalization.LayerNorm'>, <class 'sat.ops.layernorm.RMSNorm'>] is set to no_weight_decay
[2024-09-09 14:17:53,984] [INFO] [RANK 0] Syncing initialized parameters...
[2024-09-09 14:17:54,100] [INFO] [RANK 0] Finished syncing initialized parameters.
[2024-09-09 14:17:54,101] [INFO] [RANK 0] Using optimizer sat.ops.FusedEmaAdam from sat.
[2024-09-09 14:17:54,104] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.4, git-hash=unknown, git-branch=unknown
[2024-09-09 14:17:54,105] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
[2024-09-09 14:17:54,221] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /root/.cache/torch_extensions/py310_cu121/fused_ema_adam/build.ninja...
/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1965: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module fused_ema_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_ema_adam...
Time to load fused_ema_adam op: 0.23062920570373535 seconds
[2024-09-09 14:17:54,478] [INFO] [logging.py:96:log_dist] [Rank 0] Using client callable to create basic optimizer
[2024-09-09 14:17:54,478] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-09-09 14:17:54,506] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedEmaAdam
[2024-09-09 14:17:54,506] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedEmaAdam type=<class 'sat.ops.fused_ema_adam.FusedEmaAdam'>
[2024-09-09 14:17:54,509] [WARNING] [engine.py:1179:_do_optimizer_sanity_check] **** You are using ZeRO with an untested optimizer, proceed with caution *****
[2024-09-09 14:17:54,509] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2024-09-09 14:17:54,509] [INFO] [stage_1_and_2.py:148:__init__] Reduce bucket size 1000000000
[2024-09-09 14:17:54,509] [INFO] [stage_1_and_2.py:149:__init__] Allgather bucket size 1000000000
[2024-09-09 14:17:54,510] [INFO] [stage_1_and_2.py:150:__init__] CPU Offload: False
[2024-09-09 14:17:54,510] [INFO] [stage_1_and_2.py:151:__init__] Round robin gradient partitioning: False
[2024-09-09 14:17:55,329] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2024-09-09 14:17:55,330] [INFO] [utils.py:782:see_memory_usage] MA 12.86 GB         Max_MA 12.97 GB         CA 13.23 GB         Max_CA 13 GB 
[2024-09-09 14:17:55,337] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 151.45 GB, percent = 7.5%
[2024-09-09 14:17:55,474] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2024-09-09 14:17:55,474] [INFO] [utils.py:782:see_memory_usage] MA 12.86 GB         Max_MA 13.08 GB         CA 13.45 GB         Max_CA 13 GB 
[2024-09-09 14:17:55,477] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 151.45 GB, percent = 7.5%
[2024-09-09 14:17:55,477] [INFO] [stage_1_and_2.py:543:__init__] optimizer state initialized
[2024-09-09 14:17:55,533] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4294967296, reducing to 2147483648
[2024-09-09 14:17:55,628] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2024-09-09 14:17:55,629] [INFO] [utils.py:782:see_memory_usage] MA 12.86 GB         Max_MA 12.86 GB         CA 13.45 GB         Max_CA 13 GB 
[2024-09-09 14:17:55,631] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 151.59 GB, percent = 7.5%
[2024-09-09 14:17:55,635] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer
[2024-09-09 14:17:55,635] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2024-09-09 14:17:55,637] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2024-09-09 14:17:55,637] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[1.0], mom=[[0.9, 0.95]]
[2024-09-09 14:17:55,640] [INFO] [config.py:997:print] DeepSpeedEngine configuration:
[2024-09-09 14:17:55,642] [INFO] [config.py:1001:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-09-09 14:17:55,644] [INFO] [config.py:1001:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-09-09 14:17:55,644] [INFO] [config.py:1001:print]   amp_enabled .................. False
[2024-09-09 14:17:55,644] [INFO] [config.py:1001:print]   amp_params ................... False
[2024-09-09 14:17:55,644] [INFO] [config.py:1001:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-09-09 14:17:55,647] [INFO] [config.py:1001:print]   bfloat16_enabled ............. False
[2024-09-09 14:17:55,647] [INFO] [config.py:1001:print]   bfloat16_immediate_grad_update  False
[2024-09-09 14:17:55,647] [INFO] [config.py:1001:print]   checkpoint_parallel_write_pipeline  False
[2024-09-09 14:17:55,647] [INFO] [config.py:1001:print]   checkpoint_tag_validation_enabled  True
[2024-09-09 14:17:55,647] [INFO] [config.py:1001:print]   checkpoint_tag_validation_fail  False
[2024-09-09 14:17:55,647] [INFO] [config.py:1001:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fd3f414bb80>
[2024-09-09 14:17:55,647] [INFO] [config.py:1001:print]   communication_data_type ...... None
[2024-09-09 14:17:55,647] [INFO] [config.py:1001:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-09-09 14:17:55,647] [INFO] [config.py:1001:print]   curriculum_enabled_legacy .... False
[2024-09-09 14:17:55,647] [INFO] [config.py:1001:print]   curriculum_params_legacy ..... False
[2024-09-09 14:17:55,647] [INFO] [config.py:1001:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-09-09 14:17:55,647] [INFO] [config.py:1001:print]   data_efficiency_enabled ...... False
[2024-09-09 14:17:55,647] [INFO] [config.py:1001:print]   dataloader_drop_last ......... False
[2024-09-09 14:17:55,647] [INFO] [config.py:1001:print]   disable_allgather ............ False
[2024-09-09 14:17:55,647] [INFO] [config.py:1001:print]   dump_state ................... False
[2024-09-09 14:17:55,647] [INFO] [config.py:1001:print]   dynamic_loss_scale_args ...... None
[2024-09-09 14:17:55,647] [INFO] [config.py:1001:print]   eigenvalue_enabled ........... False
[2024-09-09 14:17:55,647] [INFO] [config.py:1001:print]   eigenvalue_gas_boundary_resolution  1
[2024-09-09 14:17:55,647] [INFO] [config.py:1001:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-09-09 14:17:55,647] [INFO] [config.py:1001:print]   eigenvalue_layer_num ......... 0
[2024-09-09 14:17:55,647] [INFO] [config.py:1001:print]   eigenvalue_max_iter .......... 100
[2024-09-09 14:17:55,647] [INFO] [config.py:1001:print]   eigenvalue_stability ......... 1e-06
[2024-09-09 14:17:55,647] [INFO] [config.py:1001:print]   eigenvalue_tol ............... 0.01
[2024-09-09 14:17:55,647] [INFO] [config.py:1001:print]   eigenvalue_verbose ........... False
[2024-09-09 14:17:55,647] [INFO] [config.py:1001:print]   elasticity_enabled ........... False
[2024-09-09 14:17:55,647] [INFO] [config.py:1001:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-09-09 14:17:55,647] [INFO] [config.py:1001:print]   fp16_auto_cast ............... False
[2024-09-09 14:17:55,647] [INFO] [config.py:1001:print]   fp16_enabled ................. True
[2024-09-09 14:17:55,647] [INFO] [config.py:1001:print]   fp16_master_weights_and_gradients  False
[2024-09-09 14:17:55,647] [INFO] [config.py:1001:print]   global_rank .................. 0
[2024-09-09 14:17:55,647] [INFO] [config.py:1001:print]   grad_accum_dtype ............. None
[2024-09-09 14:17:55,647] [INFO] [config.py:1001:print]   gradient_accumulation_steps .. 1
[2024-09-09 14:17:55,647] [INFO] [config.py:1001:print]   gradient_clipping ............ 0.1
[2024-09-09 14:17:55,647] [INFO] [config.py:1001:print]   gradient_predivide_factor .... 1.0
[2024-09-09 14:17:55,649] [INFO] [config.py:1001:print]   graph_harvesting ............. False
[2024-09-09 14:17:55,649] [INFO] [config.py:1001:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-09-09 14:17:55,650] [INFO] [config.py:1001:print]   initial_dynamic_scale ........ 65536
[2024-09-09 14:17:55,650] [INFO] [config.py:1001:print]   load_universal_checkpoint .... False
[2024-09-09 14:17:55,650] [INFO] [config.py:1001:print]   loss_scale ................... 0
[2024-09-09 14:17:55,650] [INFO] [config.py:1001:print]   memory_breakdown ............. False
[2024-09-09 14:17:55,650] [INFO] [config.py:1001:print]   mics_hierarchial_params_gather  False
[2024-09-09 14:17:55,650] [INFO] [config.py:1001:print]   mics_shard_size .............. -1
[2024-09-09 14:17:55,650] [INFO] [config.py:1001:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-09-09 14:17:55,650] [INFO] [config.py:1001:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-09-09 14:17:55,650] [INFO] [config.py:1001:print]   optimizer_legacy_fusion ...... False
[2024-09-09 14:17:55,650] [INFO] [config.py:1001:print]   optimizer_name ............... None
[2024-09-09 14:17:55,650] [INFO] [config.py:1001:print]   optimizer_params ............. None
[2024-09-09 14:17:55,650] [INFO] [config.py:1001:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-09-09 14:17:55,650] [INFO] [config.py:1001:print]   pld_enabled .................. False
[2024-09-09 14:17:55,650] [INFO] [config.py:1001:print]   pld_params ................... False
[2024-09-09 14:17:55,650] [INFO] [config.py:1001:print]   prescale_gradients ........... False
[2024-09-09 14:17:55,650] [INFO] [config.py:1001:print]   scheduler_name ............... None
[2024-09-09 14:17:55,650] [INFO] [config.py:1001:print]   scheduler_params ............. None
[2024-09-09 14:17:55,650] [INFO] [config.py:1001:print]   seq_parallel_communication_data_type  torch.float32
[2024-09-09 14:17:55,650] [INFO] [config.py:1001:print]   sparse_attention ............. None
[2024-09-09 14:17:55,650] [INFO] [config.py:1001:print]   sparse_gradients_enabled ..... False
[2024-09-09 14:17:55,650] [INFO] [config.py:1001:print]   steps_per_print .............. 50
[2024-09-09 14:17:55,650] [INFO] [config.py:1001:print]   timers_config ................ enabled=True synchronized=True
[2024-09-09 14:17:55,650] [INFO] [config.py:1001:print]   train_batch_size ............. 2
[2024-09-09 14:17:55,650] [INFO] [config.py:1001:print]   train_micro_batch_size_per_gpu  2
[2024-09-09 14:17:55,650] [INFO] [config.py:1001:print]   use_data_before_expert_parallel_  False
[2024-09-09 14:17:55,650] [INFO] [config.py:1001:print]   use_node_local_storage ....... False
[2024-09-09 14:17:55,650] [INFO] [config.py:1001:print]   wall_clock_breakdown ......... False
[2024-09-09 14:17:55,650] [INFO] [config.py:1001:print]   weight_quantization_config ... None
[2024-09-09 14:17:55,650] [INFO] [config.py:1001:print]   world_size ................... 1
[2024-09-09 14:17:55,650] [INFO] [config.py:1001:print]   zero_allow_untested_optimizer  True
[2024-09-09 14:17:55,650] [INFO] [config.py:1001:print]   zero_config .................. stage=2 contiguous_gradients=False reduce_scatter=True reduce_bucket_size=1000000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=1000000000 overlap_comm=True load_from_fp32_weights=False elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-09-09 14:17:55,650] [INFO] [config.py:1001:print]   zero_enabled ................. True
[2024-09-09 14:17:55,650] [INFO] [config.py:1001:print]   zero_force_ds_cpu_optimizer .. True
[2024-09-09 14:17:55,650] [INFO] [config.py:1001:print]   zero_optimization_stage ...... 2
[2024-09-09 14:17:55,650] [INFO] [config.py:987:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 2, 
    "gradient_accumulation_steps": 1, 
    "steps_per_print": 50, 
    "gradient_clipping": 0.1, 
    "zero_optimization": {
        "stage": 2, 
        "cpu_offload": false, 
        "contiguous_gradients": false, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 1.000000e+09, 
        "allgather_bucket_size": 1.000000e+09, 
        "load_from_fp32_weights": false
    }, 
    "zero_allow_untested_optimizer": true, 
    "bf16": {
        "enabled": false
    }, 
    "fp16": {
        "enabled": true
    }, 
    "loss_scale": 0, 
    "loss_scale_window": 400, 
    "hysteresis": 2, 
    "min_loss_scale": 1, 
    "activation_checkpointing": {
        "partition_activations": false, 
        "contiguous_memory_optimization": false
    }, 
    "wall_clock_breakdown": false
}
[2024-09-09 14:17:55,650] [INFO] [RANK 0] learning rate decaying style linear, ratio 10.0
[2024-09-09 14:17:55,650] [INFO] [RANK 0] Finetuning Model...
[2024-09-09 14:17:55,650] [INFO] [RANK 0] arguments:
[2024-09-09 14:17:55,650] [INFO] [RANK 0]   base ......................... ['configs/cogvideox_2b_lora.yaml', 'configs/sft.yaml']
[2024-09-09 14:17:55,651] [INFO] [RANK 0]   model_parallel_size .......... 1
[2024-09-09 14:17:55,651] [INFO] [RANK 0]   force_pretrain ............... False
[2024-09-09 14:17:55,651] [INFO] [RANK 0]   device ....................... 0
[2024-09-09 14:17:55,651] [INFO] [RANK 0]   debug ........................ False
[2024-09-09 14:17:55,651] [INFO] [RANK 0]   log_image .................... True
[2024-09-09 14:17:55,651] [INFO] [RANK 0]   output_dir ................... samples
[2024-09-09 14:17:55,651] [INFO] [RANK 0]   input_dir .................... None
[2024-09-09 14:17:55,651] [INFO] [RANK 0]   input_type ................... cli
[2024-09-09 14:17:55,651] [INFO] [RANK 0]   input_file ................... input.txt
[2024-09-09 14:17:55,651] [INFO] [RANK 0]   final_size ................... 2048
[2024-09-09 14:17:55,651] [INFO] [RANK 0]   sdedit ....................... False
[2024-09-09 14:17:55,651] [INFO] [RANK 0]   grid_num_rows ................ 1
[2024-09-09 14:17:55,651] [INFO] [RANK 0]   force_inference .............. False
[2024-09-09 14:17:55,651] [INFO] [RANK 0]   lcm_steps .................... None
[2024-09-09 14:17:55,651] [INFO] [RANK 0]   sampling_num_frames .......... 32
[2024-09-09 14:17:55,651] [INFO] [RANK 0]   sampling_fps ................. 8
[2024-09-09 14:17:55,651] [INFO] [RANK 0]   only_save_latents ............ False
[2024-09-09 14:17:55,651] [INFO] [RANK 0]   only_log_video_latents ....... False
[2024-09-09 14:17:55,651] [INFO] [RANK 0]   latent_channels .............. 32
[2024-09-09 14:17:55,651] [INFO] [RANK 0]   image2video .................. False
[2024-09-09 14:17:55,651] [INFO] [RANK 0]   experiment_name .............. lora-disney-09-09-14-17
[2024-09-09 14:17:55,651] [INFO] [RANK 0]   train_iters .................. 1000
[2024-09-09 14:17:55,654] [INFO] [RANK 0]   batch_size ................... 2
[2024-09-09 14:17:55,654] [INFO] [RANK 0]   lr ........................... 0.001
[2024-09-09 14:17:55,654] [INFO] [RANK 0]   mode ......................... finetune
[2024-09-09 14:17:55,654] [INFO] [RANK 0]   seed ......................... 42
[2024-09-09 14:17:55,654] [INFO] [RANK 0]   zero_stage ................... 0
[2024-09-09 14:17:55,654] [INFO] [RANK 0]   checkpoint_activations ....... True
[2024-09-09 14:17:55,654] [INFO] [RANK 0]   checkpoint_num_layers ........ 1
[2024-09-09 14:17:55,654] [INFO] [RANK 0]   checkpoint_skip_layers ....... 0
[2024-09-09 14:17:55,654] [INFO] [RANK 0]   fp16 ......................... True
[2024-09-09 14:17:55,654] [INFO] [RANK 0]   bf16 ......................... False
[2024-09-09 14:17:55,654] [INFO] [RANK 0]   gradient_accumulation_steps .. 1
[2024-09-09 14:17:55,654] [INFO] [RANK 0]   profiling .................... -1
[2024-09-09 14:17:55,654] [INFO] [RANK 0]   epochs ....................... None
[2024-09-09 14:17:55,654] [INFO] [RANK 0]   log_interval ................. 20
[2024-09-09 14:17:55,654] [INFO] [RANK 0]   summary_dir .................. /mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora
[2024-09-09 14:17:55,654] [INFO] [RANK 0]   save_args .................... False
[2024-09-09 14:17:55,654] [INFO] [RANK 0]   lr_decay_iters ............... None
[2024-09-09 14:17:55,654] [INFO] [RANK 0]   lr_decay_style ............... linear
[2024-09-09 14:17:55,656] [INFO] [RANK 0]   lr_decay_ratio ............... 0.1
[2024-09-09 14:17:55,656] [INFO] [RANK 0]   warmup ....................... 0.01
[2024-09-09 14:17:55,656] [INFO] [RANK 0]   weight_decay ................. 0.0001
[2024-09-09 14:17:55,656] [INFO] [RANK 0]   save ......................... /mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora/lora-disney-09-09-14-17
[2024-09-09 14:17:55,656] [INFO] [RANK 0]   load ......................... /mnt/ceph/develop/jiawei/model_checkpoint/CogVideoX-2b-sat/transformer
[2024-09-09 14:17:55,657] [INFO] [RANK 0]   force_train .................. True
[2024-09-09 14:17:55,657] [INFO] [RANK 0]   save_interval ................ 500
[2024-09-09 14:17:55,657] [INFO] [RANK 0]   no_save_rng .................. False
[2024-09-09 14:17:55,657] [INFO] [RANK 0]   no_load_rng .................. True
[2024-09-09 14:17:55,657] [INFO] [RANK 0]   resume_dataloader ............ False
[2024-09-09 14:17:55,659] [INFO] [RANK 0]   distributed_backend .......... nccl
[2024-09-09 14:17:55,659] [INFO] [RANK 0]   local_rank ................... 0
[2024-09-09 14:17:55,659] [INFO] [RANK 0]   exit_interval ................ None
[2024-09-09 14:17:55,659] [INFO] [RANK 0]   wandb ........................ True
[2024-09-09 14:17:55,659] [INFO] [RANK 0]   wandb_project_name ........... cogvideox_ckpts_2b_lora_base_001
[2024-09-09 14:17:55,659] [INFO] [RANK 0]   eval_batch_size .............. 1
[2024-09-09 14:17:55,659] [INFO] [RANK 0]   eval_iters ................... 1
[2024-09-09 14:17:55,659] [INFO] [RANK 0]   eval_interval ................ 100
[2024-09-09 14:17:55,659] [INFO] [RANK 0]   strict_eval .................. False
[2024-09-09 14:17:55,659] [INFO] [RANK 0]   train_data ................... ['/mnt/ceph/develop/jiawei/lora_dataset/cogvideo_lora_dataset']
[2024-09-09 14:17:55,659] [INFO] [RANK 0]   train_data_weights ........... None
[2024-09-09 14:17:55,659] [INFO] [RANK 0]   iterable_dataset ............. False
[2024-09-09 14:17:55,659] [INFO] [RANK 0]   iterable_dataset_eval ........ 
[2024-09-09 14:17:55,659] [INFO] [RANK 0]   batch_from_same_dataset ...... False
[2024-09-09 14:17:55,659] [INFO] [RANK 0]   valid_data ................... ['/mnt/ceph/develop/jiawei/lora_dataset/cogvideo_lora_dataset']
[2024-09-09 14:17:55,659] [INFO] [RANK 0]   test_data .................... None
[2024-09-09 14:17:55,659] [INFO] [RANK 0]   split ........................ 1,0,0
[2024-09-09 14:17:55,659] [INFO] [RANK 0]   num_workers .................. 8
[2024-09-09 14:17:55,659] [INFO] [RANK 0]   block_size ................... 10000
[2024-09-09 14:17:55,659] [INFO] [RANK 0]   prefetch_factor .............. 4
[2024-09-09 14:17:55,659] [INFO] [RANK 0]   deepspeed .................... True
[2024-09-09 14:17:55,659] [INFO] [RANK 0]   deepspeed_config ............. {'train_micro_batch_size_per_gpu': 2, 'gradient_accumulation_steps': 1, 'steps_per_print': 50, 'gradient_clipping': 0.1, 'zero_optimization': {'stage': 2, 'cpu_offload': False, 'contiguous_gradients': False, 'overlap_comm': True, 'reduce_scatter': True, 'reduce_bucket_size': 1000000000, 'allgather_bucket_size': 1000000000, 'load_from_fp32_weights': False}, 'zero_allow_untested_optimizer': True, 'bf16': {'enabled': False}, 'fp16': {'enabled': True}, 'loss_scale': 0, 'loss_scale_window': 400, 'hysteresis': 2, 'min_loss_scale': 1, 'activation_checkpointing': {'partition_activations': False, 'contiguous_memory_optimization': False}, 'wall_clock_breakdown': False}
[2024-09-09 14:17:55,659] [INFO] [RANK 0]   deepscale .................... False
[2024-09-09 14:17:55,659] [INFO] [RANK 0]   deepscale_config ............. None
[2024-09-09 14:17:55,660] [INFO] [RANK 0]   model_config ................. {'scale_factor': 1.15258426, 'disable_first_stage_autocast': True, 'not_trainable_prefixes': ['all'], 'log_keys': ['txt'], 'denoiser_config': {'target': 'sgm.modules.diffusionmodules.denoiser.DiscreteDenoiser', 'params': {'num_idx': 1000, 'quantize_c_noise': False, 'weighting_config': {'target': 'sgm.modules.diffusionmodules.denoiser_weighting.EpsWeighting'}, 'scaling_config': {'target': 'sgm.modules.diffusionmodules.denoiser_scaling.VideoScaling'}, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 3.0}}}}, 'network_config': {'target': 'dit_video_concat.DiffusionTransformer', 'params': {'time_embed_dim': 512, 'elementwise_affine': True, 'num_frames': 49, 'time_compressed_rate': 4, 'latent_width': 90, 'latent_height': 60, 'num_layers': 30, 'patch_size': 2, 'in_channels': 16, 'out_channels': 16, 'hidden_size': 1920, 'adm_in_channels': 256, 'num_attention_heads': 30, 'transformer_args': {'checkpoint_activations': True, 'vocab_size': 1, 'max_sequence_length': 64, 'layernorm_order': 'pre', 'skip_init': False, 'model_parallel_size': 1, 'is_decoder': False, 'num_layers': 30, 'hidden_size': 1920, 'num_attention_heads': 30, 'parallel_output': True}, 'modules': {'pos_embed_config': {'target': 'dit_video_concat.Basic3DPositionEmbeddingMixin', 'params': {'text_length': 226, 'height_interpolation': 1.875, 'width_interpolation': 1.875}}, 'lora_config': {'target': 'sat.model.finetune.lora2.LoraMixin', 'params': {'r': 128}}, 'patch_embed_config': {'target': 'dit_video_concat.ImagePatchEmbeddingMixin', 'params': {'text_hidden_size': 4096}}, 'adaln_layer_config': {'target': 'dit_video_concat.AdaLNMixin', 'params': {'qk_ln': True}}, 'final_layer_config': {'target': 'dit_video_concat.FinalLayerMixin'}}, 'dtype': 'fp16'}}, 'conditioner_config': {'target': 'sgm.modules.GeneralConditioner', 'params': {'emb_models': [{'is_trainable': False, 'input_key': 'txt', 'ucg_rate': 0.1, 'target': 'sgm.modules.encoders.modules.FrozenT5Embedder', 'params': {'model_dir': '/mnt/ceph/develop/jiawei/model_checkpoint/t5-v1_1-xxl', 'max_length': 226}}]}}, 'first_stage_config': {'target': 'vae_modules.autoencoder.VideoAutoencoderInferenceWrapper', 'params': {'cp_size': 1, 'ckpt_path': '/mnt/ceph/develop/jiawei/model_checkpoint/CogVideoX-2b-sat/vae/3d-vae.pt', 'ignore_keys': ['loss'], 'loss_config': {'target': 'torch.nn.Identity'}, 'regularizer_config': {'target': 'vae_modules.regularizers.DiagonalGaussianRegularizer'}, 'encoder_config': {'target': 'vae_modules.cp_enc_dec.ContextParallelEncoder3D', 'params': {'double_z': True, 'z_channels': 16, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 2, 4], 'attn_resolutions': [], 'num_res_blocks': 3, 'dropout': 0.0, 'gather_norm': True}}, 'decoder_config': {'target': 'vae_modules.cp_enc_dec.ContextParallelDecoder3D', 'params': {'double_z': True, 'z_channels': 16, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 2, 4], 'attn_resolutions': [], 'num_res_blocks': 3, 'dropout': 0.0, 'gather_norm': False}}}}, 'loss_fn_config': {'target': 'sgm.modules.diffusionmodules.loss.VideoDiffusionLoss', 'params': {'offset_noise_level': 0, 'sigma_sampler_config': {'target': 'sgm.modules.diffusionmodules.sigma_sampling.DiscreteSampling', 'params': {'uniform_sampling': True, 'num_idx': 1000, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 3.0}}}}}}, 'sampler_config': {'target': 'sgm.modules.diffusionmodules.sampling.VPSDEDPMPP2MSampler', 'params': {'num_steps': 50, 'verbose': True, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 3.0}}, 'guider_config': {'target': 'sgm.modules.diffusionmodules.guiders.DynamicCFG', 'params': {'scale': 6, 'exp': 5, 'num_steps': 50}}}}}
[2024-09-09 14:17:55,661] [INFO] [RANK 0]   data_config .................. {'target': 'data_video.SFTDataset', 'params': {'video_size': [480, 720], 'fps': 8, 'max_num_frames': 49, 'skip_frms_num': 3.0}}
[2024-09-09 14:17:55,661] [INFO] [RANK 0]   cuda ......................... True
[2024-09-09 14:17:55,662] [INFO] [RANK 0]   rank ......................... 0
[2024-09-09 14:17:55,662] [INFO] [RANK 0]   world_size ................... 1
[2024-09-09 14:17:55,662] [INFO] [RANK 0]   deepspeed_activation_checkpointing  True
[2024-09-09 14:17:55,662] [INFO] [RANK 0]   master_ip .................... nm04-a800-node083
[2024-09-09 14:17:55,662] [INFO] [RANK 0]   master_port .................. 9323
[2024-09-09 14:17:55,662] [INFO] [RANK 0]   log_config ................... [{'model': {'scale_factor': 1.15258426, 'disable_first_stage_autocast': True, 'not_trainable_prefixes': ['all'], 'log_keys': ['txt'], 'denoiser_config': {'target': 'sgm.modules.diffusionmodules.denoiser.DiscreteDenoiser', 'params': {'num_idx': 1000, 'quantize_c_noise': False, 'weighting_config': {'target': 'sgm.modules.diffusionmodules.denoiser_weighting.EpsWeighting'}, 'scaling_config': {'target': 'sgm.modules.diffusionmodules.denoiser_scaling.VideoScaling'}, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 3.0}}}}, 'network_config': {'target': 'dit_video_concat.DiffusionTransformer', 'params': {'time_embed_dim': 512, 'elementwise_affine': True, 'num_frames': 49, 'time_compressed_rate': 4, 'latent_width': 90, 'latent_height': 60, 'num_layers': 30, 'patch_size': 2, 'in_channels': 16, 'out_channels': 16, 'hidden_size': 1920, 'adm_in_channels': 256, 'num_attention_heads': 30, 'transformer_args': {'checkpoint_activations': True, 'vocab_size': 1, 'max_sequence_length': 64, 'layernorm_order': 'pre', 'skip_init': False, 'model_parallel_size': 1, 'is_decoder': False}, 'modules': {'pos_embed_config': {'target': 'dit_video_concat.Basic3DPositionEmbeddingMixin', 'params': {'text_length': 226, 'height_interpolation': 1.875, 'width_interpolation': 1.875}}, 'lora_config': {'target': 'sat.model.finetune.lora2.LoraMixin', 'params': {'r': 128}}, 'patch_embed_config': {'target': 'dit_video_concat.ImagePatchEmbeddingMixin', 'params': {'text_hidden_size': 4096}}, 'adaln_layer_config': {'target': 'dit_video_concat.AdaLNMixin', 'params': {'qk_ln': True}}, 'final_layer_config': {'target': 'dit_video_concat.FinalLayerMixin'}}}}, 'conditioner_config': {'target': 'sgm.modules.GeneralConditioner', 'params': {'emb_models': [{'is_trainable': False, 'input_key': 'txt', 'ucg_rate': 0.1, 'target': 'sgm.modules.encoders.modules.FrozenT5Embedder', 'params': {'model_dir': '/mnt/ceph/develop/jiawei/model_checkpoint/t5-v1_1-xxl', 'max_length': 226}}]}}, 'first_stage_config': {'target': 'vae_modules.autoencoder.VideoAutoencoderInferenceWrapper', 'params': {'cp_size': 1, 'ckpt_path': '/mnt/ceph/develop/jiawei/model_checkpoint/CogVideoX-2b-sat/vae/3d-vae.pt', 'ignore_keys': ['loss'], 'loss_config': {'target': 'torch.nn.Identity'}, 'regularizer_config': {'target': 'vae_modules.regularizers.DiagonalGaussianRegularizer'}, 'encoder_config': {'target': 'vae_modules.cp_enc_dec.ContextParallelEncoder3D', 'params': {'double_z': True, 'z_channels': 16, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 2, 4], 'attn_resolutions': [], 'num_res_blocks': 3, 'dropout': 0.0, 'gather_norm': True}}, 'decoder_config': {'target': 'vae_modules.cp_enc_dec.ContextParallelDecoder3D', 'params': {'double_z': True, 'z_channels': 16, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 2, 4], 'attn_resolutions': [], 'num_res_blocks': 3, 'dropout': 0.0, 'gather_norm': False}}}}, 'loss_fn_config': {'target': 'sgm.modules.diffusionmodules.loss.VideoDiffusionLoss', 'params': {'offset_noise_level': 0, 'sigma_sampler_config': {'target': 'sgm.modules.diffusionmodules.sigma_sampling.DiscreteSampling', 'params': {'uniform_sampling': True, 'num_idx': 1000, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 3.0}}}}}}, 'sampler_config': {'target': 'sgm.modules.diffusionmodules.sampling.VPSDEDPMPP2MSampler', 'params': {'num_steps': 50, 'verbose': True, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 3.0}}, 'guider_config': {'target': 'sgm.modules.diffusionmodules.guiders.DynamicCFG', 'params': {'scale': 6, 'exp': 5, 'num_steps': 50}}}}}}, {'args': {'checkpoint_activations': True, 'model_parallel_size': 1, 'experiment_name': 'lora-disney', 'mode': 'finetune', 'load': '/mnt/ceph/develop/jiawei/model_checkpoint/CogVideoX-2b-sat/transformer', 'no_load_rng': True, 'train_iters': 1000, 'eval_iters': 1, 'eval_interval': 100, 'eval_batch_size': 1, 'save': '/mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora', 'save_interval': 500, 'log_interval': 20, 'train_data': ['/mnt/ceph/develop/jiawei/lora_dataset/cogvideo_lora_dataset'], 'valid_data': ['/mnt/ceph/develop/jiawei/lora_dataset/cogvideo_lora_dataset'], 'split': '1,0,0', 'num_workers': 8, 'force_train': True, 'only_log_video_latents': False, 'wandb': True, 'wandb_project_name': 'cogvideox_ckpts_2b_lora_base_001', 'summary_dir': '/mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora'}, 'data': {'target': 'data_video.SFTDataset', 'params': {'video_size': [480, 720], 'fps': 8, 'max_num_frames': 49, 'skip_frms_num': 3.0}}, 'deepspeed': {'train_micro_batch_size_per_gpu': 2, 'gradient_accumulation_steps': 1, 'steps_per_print': 50, 'gradient_clipping': 0.1, 'zero_optimization': {'stage': 2, 'cpu_offload': False, 'contiguous_gradients': False, 'overlap_comm': True, 'reduce_scatter': True, 'reduce_bucket_size': 1000000000, 'allgather_bucket_size': 1000000000, 'load_from_fp32_weights': False}, 'zero_allow_untested_optimizer': True, 'bf16': {'enabled': False}, 'fp16': {'enabled': True}, 'loss_scale': 0, 'loss_scale_window': 400, 'hysteresis': 2, 'min_loss_scale': 1, 'optimizer': {'type': 'sat.ops.FusedEmaAdam', 'params': {'lr': '1E-3', 'betas': [0.9, 0.95], 'eps': '1e-8', 'weight_decay': '1e-4'}}, 'activation_checkpointing': {'partition_activations': False, 'contiguous_memory_optimization': False}, 'wall_clock_breakdown': False}}]
[2024-09-09 14:17:55,664] [INFO] [RANK 0]   do_train ..................... True
[2024-09-09 14:17:55,664] [INFO] [RANK 0]   val_last_shape ............... []
[2024-09-09 14:17:55,664] [INFO] [RANK 0]   val_drop_number .............. 0
[2024-09-09 14:17:55,664] [INFO] [RANK 0]   do_valid ..................... True
[2024-09-09 14:17:55,664] [INFO] [RANK 0]   do_test ...................... False
[2024-09-09 14:17:55,665] [INFO] [RANK 0]   iteration .................... 0
Traceback (most recent call last):
  File "/mnt/ceph/develop/jiawei/CogVideo/sat/train_video.py", line 223, in <module>
    training_main(
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/sat/training/deepspeed_training.py", line 157, in training_main
    iteration, skipped = train(model, optimizer,
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/sat/training/deepspeed_training.py", line 359, in train
    lm_loss, skipped_iter, metrics = train_step(train_data_iterator,
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/sat/training/deepspeed_training.py", line 443, in train_step
    forward_ret = forward_step(data_iterator, model, args, timers, **kwargs)
  File "/mnt/ceph/develop/jiawei/CogVideo/sat/train_video.py", line 195, in forward_step
    loss, loss_dict = model.shared_step(batch)
  File "/mnt/ceph/develop/jiawei/CogVideo/sat/diffusion_video.py", line 142, in shared_step
    x = self.encode_first_stage(x, batch)
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/mnt/ceph/develop/jiawei/CogVideo/sat/diffusion_video.py", line 186, in encode_first_stage
    out = self.first_stage_model.encode(x[n * n_samples : (n + 1) * n_samples])
  File "/mnt/ceph/develop/jiawei/CogVideo/sat/vae_modules/autoencoder.py", line 608, in encode
    z = super().encode(x, return_reg_log, unregularized)
  File "/mnt/ceph/develop/jiawei/CogVideo/sat/vae_modules/autoencoder.py", line 224, in encode
    z = self.encoder(x)
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/ceph/develop/jiawei/CogVideo/sat/vae_modules/cp_enc_dec.py", line 818, in forward
    h = self.down[i_level].block[i_block](h, temb)
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/ceph/develop/jiawei/CogVideo/sat/vae_modules/cp_enc_dec.py", line 689, in forward
    h = self.conv1(h, clear_cache=clear_fake_cp_cache)
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/ceph/develop/jiawei/CogVideo/sat/vae_modules/cp_enc_dec.py", line 428, in forward
    output_parallel = self.conv(input_parallel)
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/ceph/develop/jiawei/CogVideo/sat/vae_modules/utils.py", line 88, in forward
    output = torch.cat(output_chunks, dim=2)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 8.08 GiB. GPU 0 has a total capacity of 79.32 GiB of which 6.44 GiB is free. Including non-PyTorch memory, this process has 57.82 GiB memory in use. Process 1850229 has 15.01 GiB memory in use. Of the allocated memory 54.21 GiB is allocated by PyTorch, and 1.19 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]: Traceback (most recent call last):
[rank0]:   File "/mnt/ceph/develop/jiawei/CogVideo/sat/train_video.py", line 223, in <module>
[rank0]:     training_main(
[rank0]:   File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/sat/training/deepspeed_training.py", line 157, in training_main
[rank0]:     iteration, skipped = train(model, optimizer,
[rank0]:   File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/sat/training/deepspeed_training.py", line 359, in train
[rank0]:     lm_loss, skipped_iter, metrics = train_step(train_data_iterator,
[rank0]:   File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/sat/training/deepspeed_training.py", line 443, in train_step
[rank0]:     forward_ret = forward_step(data_iterator, model, args, timers, **kwargs)
[rank0]:   File "/mnt/ceph/develop/jiawei/CogVideo/sat/train_video.py", line 195, in forward_step
[rank0]:     loss, loss_dict = model.shared_step(batch)
[rank0]:   File "/mnt/ceph/develop/jiawei/CogVideo/sat/diffusion_video.py", line 142, in shared_step
[rank0]:     x = self.encode_first_stage(x, batch)
[rank0]:   File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank0]:     return func(*args, **kwargs)
[rank0]:   File "/mnt/ceph/develop/jiawei/CogVideo/sat/diffusion_video.py", line 186, in encode_first_stage
[rank0]:     out = self.first_stage_model.encode(x[n * n_samples : (n + 1) * n_samples])
[rank0]:   File "/mnt/ceph/develop/jiawei/CogVideo/sat/vae_modules/autoencoder.py", line 608, in encode
[rank0]:     z = super().encode(x, return_reg_log, unregularized)
[rank0]:   File "/mnt/ceph/develop/jiawei/CogVideo/sat/vae_modules/autoencoder.py", line 224, in encode
[rank0]:     z = self.encoder(x)
[rank0]:   File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/mnt/ceph/develop/jiawei/CogVideo/sat/vae_modules/cp_enc_dec.py", line 818, in forward
[rank0]:     h = self.down[i_level].block[i_block](h, temb)
[rank0]:   File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/mnt/ceph/develop/jiawei/CogVideo/sat/vae_modules/cp_enc_dec.py", line 689, in forward
[rank0]:     h = self.conv1(h, clear_cache=clear_fake_cp_cache)
[rank0]:   File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/mnt/ceph/develop/jiawei/CogVideo/sat/vae_modules/cp_enc_dec.py", line 428, in forward
[rank0]:     output_parallel = self.conv(input_parallel)
[rank0]:   File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/mnt/ceph/develop/jiawei/CogVideo/sat/vae_modules/utils.py", line 88, in forward
[rank0]:     output = torch.cat(output_chunks, dim=2)
[rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 8.08 GiB. GPU 0 has a total capacity of 79.32 GiB of which 6.44 GiB is free. Including non-PyTorch memory, this process has 57.82 GiB memory in use. Process 1850229 has 15.01 GiB memory in use. Of the allocated memory 54.21 GiB is allocated by PyTorch, and 1.19 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: - 0.015 MB of 0.015 MB uploadedwandb: Currently logged in as: dmeck. Use `wandb login --relogin` to force relogin
wandb: \ 0.015 MB of 0.015 MB uploadedwandb: | 0.015 MB of 0.015 MB uploadedwandb: / 0.015 MB of 0.015 MB uploadedwandb: - 0.015 MB of 0.015 MB uploadedwandb: \ 0.018 MB of 0.021 MB uploadedwandb: wandb version 0.17.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora/wandb/run-20240909_141759-5ad4a1ph
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lora-disney-09-09-14-17
wandb: â­ï¸ View project at https://wandb.ai/dmeck/cogvideox_ckpts_2b_lora_base_001
wandb: ðŸš€ View run at https://wandb.ai/dmeck/cogvideox_ckpts_2b_lora_base_001/runs/5ad4a1ph
wandb: | 0.018 MB of 0.033 MB uploadedwandb: / 0.033 MB of 0.033 MB uploadedwandb: ðŸš€ View run lora-disney-09-09-14-16 at: https://wandb.ai/dmeck/cogvideox_ckpts_2b_lora/runs/gdt6b4zh
wandb: â­ï¸ View project at: https://wandb.ai/dmeck/cogvideox_ckpts_2b_lora
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: /mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora/wandb/run-20240909_141711-gdt6b4zh/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Traceback (most recent call last):
  File "/mnt/ceph/develop/jiawei/CogVideo/sat/train_video.py", line 223, in <module>
    training_main(
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/sat/training/deepspeed_training.py", line 157, in training_main
    iteration, skipped = train(model, optimizer,
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/sat/training/deepspeed_training.py", line 359, in train
    lm_loss, skipped_iter, metrics = train_step(train_data_iterator,
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/sat/training/deepspeed_training.py", line 443, in train_step
    forward_ret = forward_step(data_iterator, model, args, timers, **kwargs)
  File "/mnt/ceph/develop/jiawei/CogVideo/sat/train_video.py", line 195, in forward_step
    loss, loss_dict = model.shared_step(batch)
  File "/mnt/ceph/develop/jiawei/CogVideo/sat/diffusion_video.py", line 142, in shared_step
    x = self.encode_first_stage(x, batch)
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/mnt/ceph/develop/jiawei/CogVideo/sat/diffusion_video.py", line 186, in encode_first_stage
    out = self.first_stage_model.encode(x[n * n_samples : (n + 1) * n_samples])
  File "/mnt/ceph/develop/jiawei/CogVideo/sat/vae_modules/autoencoder.py", line 608, in encode
    z = super().encode(x, return_reg_log, unregularized)
  File "/mnt/ceph/develop/jiawei/CogVideo/sat/vae_modules/autoencoder.py", line 224, in encode
    z = self.encoder(x)
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/ceph/develop/jiawei/CogVideo/sat/vae_modules/cp_enc_dec.py", line 815, in forward
    h = self.conv_in(x)
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/ceph/develop/jiawei/CogVideo/sat/vae_modules/cp_enc_dec.py", line 428, in forward
    output_parallel = self.conv(input_parallel)
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/ceph/develop/jiawei/CogVideo/sat/vae_modules/utils.py", line 91, in forward
    return super(SafeConv3d, self).forward(input)
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 608, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 603, in _conv_forward
    return F.conv3d(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 8.08 GiB. GPU 0 has a total capacity of 79.32 GiB of which 5.73 GiB is free. Process 1849309 has 57.82 GiB memory in use. Including non-PyTorch memory, this process has 15.71 GiB memory in use. Of the allocated memory 13.63 GiB is allocated by PyTorch, and 279.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]: Traceback (most recent call last):
[rank0]:   File "/mnt/ceph/develop/jiawei/CogVideo/sat/train_video.py", line 223, in <module>
[rank0]:     training_main(
[rank0]:   File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/sat/training/deepspeed_training.py", line 157, in training_main
[rank0]:     iteration, skipped = train(model, optimizer,
[rank0]:   File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/sat/training/deepspeed_training.py", line 359, in train
[rank0]:     lm_loss, skipped_iter, metrics = train_step(train_data_iterator,
[rank0]:   File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/sat/training/deepspeed_training.py", line 443, in train_step
[rank0]:     forward_ret = forward_step(data_iterator, model, args, timers, **kwargs)
[rank0]:   File "/mnt/ceph/develop/jiawei/CogVideo/sat/train_video.py", line 195, in forward_step
[rank0]:     loss, loss_dict = model.shared_step(batch)
[rank0]:   File "/mnt/ceph/develop/jiawei/CogVideo/sat/diffusion_video.py", line 142, in shared_step
[rank0]:     x = self.encode_first_stage(x, batch)
[rank0]:   File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank0]:     return func(*args, **kwargs)
[rank0]:   File "/mnt/ceph/develop/jiawei/CogVideo/sat/diffusion_video.py", line 186, in encode_first_stage
[rank0]:     out = self.first_stage_model.encode(x[n * n_samples : (n + 1) * n_samples])
[rank0]:   File "/mnt/ceph/develop/jiawei/CogVideo/sat/vae_modules/autoencoder.py", line 608, in encode
[rank0]:     z = super().encode(x, return_reg_log, unregularized)
[rank0]:   File "/mnt/ceph/develop/jiawei/CogVideo/sat/vae_modules/autoencoder.py", line 224, in encode
[rank0]:     z = self.encoder(x)
[rank0]:   File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/mnt/ceph/develop/jiawei/CogVideo/sat/vae_modules/cp_enc_dec.py", line 815, in forward
[rank0]:     h = self.conv_in(x)
[rank0]:   File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/mnt/ceph/develop/jiawei/CogVideo/sat/vae_modules/cp_enc_dec.py", line 428, in forward
[rank0]:     output_parallel = self.conv(input_parallel)
[rank0]:   File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/mnt/ceph/develop/jiawei/CogVideo/sat/vae_modules/utils.py", line 91, in forward
[rank0]:     return super(SafeConv3d, self).forward(input)
[rank0]:   File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 608, in forward
[rank0]:     return self._conv_forward(input, self.weight, self.bias)
[rank0]:   File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 603, in _conv_forward
[rank0]:     return F.conv3d(
[rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 8.08 GiB. GPU 0 has a total capacity of 79.32 GiB of which 5.73 GiB is free. Process 1849309 has 57.82 GiB memory in use. Including non-PyTorch memory, this process has 15.71 GiB memory in use. Of the allocated memory 13.63 GiB is allocated by PyTorch, and 279.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: - 0.015 MB of 0.015 MB uploadedwandb: \ 0.015 MB of 0.015 MB uploadedwandb: | 0.015 MB of 0.015 MB uploadedwandb: / 0.015 MB of 0.015 MB uploadedwandb: - 0.015 MB of 0.015 MB uploadedwandb: \ 0.015 MB of 0.015 MB uploadedE0909 14:18:17.447000 140332157638464 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 0 (pid: 1849309) of binary: /mnt/ceph/develop/jiawei/conda_env/cogvideo/bin/python
Traceback (most recent call last):
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train_video.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-09-09_14:18:17
  host      : nm04-a800-node083
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 1849309)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
wandb: | 0.015 MB of 0.015 MB uploadedDONE on nm04-a800-node083
wandb: / 0.015 MB of 0.015 MB uploadedwandb: - 0.018 MB of 0.032 MB uploadedwandb: \ 0.021 MB of 0.032 MB uploadedwandb: | 0.032 MB of 0.032 MB uploadedwandb: ðŸš€ View run lora-disney-09-09-14-17 at: https://wandb.ai/dmeck/cogvideox_ckpts_2b_lora_base_001/runs/5ad4a1ph
wandb: â­ï¸ View project at: https://wandb.ai/dmeck/cogvideox_ckpts_2b_lora_base_001
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: /mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora/wandb/run-20240909_141759-5ad4a1ph/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
E0909 14:18:31.123000 139984075953984 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 0 (pid: 1850229) of binary: /mnt/ceph/develop/jiawei/conda_env/cogvideo/bin/python
Traceback (most recent call last):
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train_video.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-09-09_14:18:31
  host      : nm04-a800-node083
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 1850229)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
DONE on nm04-a800-node083
RUN on nm04-a800-node083, CUDA_VISIBLE_DEVICES=0
torchrun --standalone --nproc_per_node=1 train_video.py --base configs/cogvideox_2b_lora.yaml configs/sft.yaml --seed 42
[2024-09-09 14:22:11,731] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
[93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, input, weight, bias=None):
/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/kornia/feature/lightglue.py:44: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)
no module 'xformers'. Processing without...
no module 'xformers'. Processing without...
[2024-09-09 14:22:29,526] [INFO] using world size: 1
[2024-09-09 14:22:29,526] [INFO] Will override arguments with manually specified deepspeed_config!
[2024-09-09 14:22:29,533] [INFO] [RANK 0] > initializing model parallel with size 1
[2024-09-09 14:22:29,533] [INFO] [comm.py:637:init_distributed] cdb=None
Namespace(base=['configs/cogvideox_2b_lora.yaml', 'configs/sft.yaml'], model_parallel_size=1, force_pretrain=False, device=0, debug=False, log_image=True, output_dir='samples', input_dir=None, input_type='cli', input_file='input.txt', final_size=2048, sdedit=False, grid_num_rows=1, force_inference=False, lcm_steps=None, sampling_num_frames=32, sampling_fps=8, only_save_latents=False, only_log_video_latents=False, latent_channels=32, image2video=False, experiment_name='lora-disney', train_iters=1000, batch_size=2, lr=0.001, mode='finetune', seed=42, zero_stage=0, checkpoint_activations=True, checkpoint_num_layers=1, checkpoint_skip_layers=0, fp16=True, bf16=False, gradient_accumulation_steps=1, profiling=-1, epochs=None, log_interval=20, summary_dir='/mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora', save_args=False, lr_decay_iters=None, lr_decay_style='linear', lr_decay_ratio=0.1, warmup=0.01, weight_decay=0.0001, save='/mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora', load='/mnt/ceph/develop/jiawei/model_checkpoint/CogVideoX-2b-sat/transformer', force_train=True, save_interval=500, no_save_rng=False, no_load_rng=True, resume_dataloader=False, distributed_backend='nccl', local_rank=0, exit_interval=None, wandb=True, wandb_project_name='cogv_2b_lora_base_001', eval_batch_size=1, eval_iters=1, eval_interval=100, strict_eval=False, train_data=['/mnt/ceph/develop/jiawei/lora_dataset/cogvideo_lora_dataset'], train_data_weights=None, iterable_dataset=False, iterable_dataset_eval='', batch_from_same_dataset=False, valid_data=['/mnt/ceph/develop/jiawei/lora_dataset/cogvideo_lora_dataset'], test_data=None, split='1,0,0', num_workers=8, block_size=10000, prefetch_factor=4, deepspeed=True, deepspeed_config={'train_micro_batch_size_per_gpu': 2, 'gradient_accumulation_steps': 1, 'steps_per_print': 50, 'gradient_clipping': 0.1, 'zero_optimization': {'stage': 2, 'cpu_offload': False, 'contiguous_gradients': False, 'overlap_comm': True, 'reduce_scatter': True, 'reduce_bucket_size': 1000000000, 'allgather_bucket_size': 1000000000, 'load_from_fp32_weights': False}, 'zero_allow_untested_optimizer': True, 'bf16': {'enabled': False}, 'fp16': {'enabled': True}, 'loss_scale': 0, 'loss_scale_window': 400, 'hysteresis': 2, 'min_loss_scale': 1, 'optimizer': {'type': 'sat.ops.FusedEmaAdam', 'params': {'lr': 0.001, 'betas': [0.9, 0.95], 'eps': 1e-08, 'weight_decay': 0.0001}}, 'activation_checkpointing': {'partition_activations': False, 'contiguous_memory_optimization': False}, 'wall_clock_breakdown': False}, deepscale=False, deepscale_config=None, model_config={'scale_factor': 1.15258426, 'disable_first_stage_autocast': True, 'not_trainable_prefixes': ['all'], 'log_keys': ['txt'], 'denoiser_config': {'target': 'sgm.modules.diffusionmodules.denoiser.DiscreteDenoiser', 'params': {'num_idx': 1000, 'quantize_c_noise': False, 'weighting_config': {'target': 'sgm.modules.diffusionmodules.denoiser_weighting.EpsWeighting'}, 'scaling_config': {'target': 'sgm.modules.diffusionmodules.denoiser_scaling.VideoScaling'}, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 3.0}}}}, 'network_config': {'target': 'dit_video_concat.DiffusionTransformer', 'params': {'time_embed_dim': 512, 'elementwise_affine': True, 'num_frames': 49, 'time_compressed_rate': 4, 'latent_width': 90, 'latent_height': 60, 'num_layers': 30, 'patch_size': 2, 'in_channels': 16, 'out_channels': 16, 'hidden_size': 1920, 'adm_in_channels': 256, 'num_attention_heads': 30, 'transformer_args': {'checkpoint_activations': True, 'vocab_size': 1, 'max_sequence_length': 64, 'layernorm_order': 'pre', 'skip_init': False, 'model_parallel_size': 1, 'is_decoder': False}, 'modules': {'pos_embed_config': {'target': 'dit_video_concat.Basic3DPositionEmbeddingMixin', 'params': {'text_length': 226, 'height_interpolation': 1.875, 'width_interpolation': 1.875}}, 'lora_config': {'target': 'sat.model.finetune.lora2.LoraMixin', 'params': {'r': 128}}, 'patch_embed_config': {'target': 'dit_video_concat.ImagePatchEmbeddingMixin', 'params': {'text_hidden_size': 4096}}, 'adaln_layer_config': {'target': 'dit_video_concat.AdaLNMixin', 'params': {'qk_ln': True}}, 'final_layer_config': {'target': 'dit_video_concat.FinalLayerMixin'}}}}, 'conditioner_config': {'target': 'sgm.modules.GeneralConditioner', 'params': {'emb_models': [{'is_trainable': False, 'input_key': 'txt', 'ucg_rate': 0.1, 'target': 'sgm.modules.encoders.modules.FrozenT5Embedder', 'params': {'model_dir': '/mnt/ceph/develop/jiawei/model_checkpoint/t5-v1_1-xxl', 'max_length': 226}}]}}, 'first_stage_config': {'target': 'vae_modules.autoencoder.VideoAutoencoderInferenceWrapper', 'params': {'cp_size': 1, 'ckpt_path': '/mnt/ceph/develop/jiawei/model_checkpoint/CogVideoX-2b-sat/vae/3d-vae.pt', 'ignore_keys': ['loss'], 'loss_config': {'target': 'torch.nn.Identity'}, 'regularizer_config': {'target': 'vae_modules.regularizers.DiagonalGaussianRegularizer'}, 'encoder_config': {'target': 'vae_modules.cp_enc_dec.ContextParallelEncoder3D', 'params': {'double_z': True, 'z_channels': 16, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 2, 4], 'attn_resolutions': [], 'num_res_blocks': 3, 'dropout': 0.0, 'gather_norm': True}}, 'decoder_config': {'target': 'vae_modules.cp_enc_dec.ContextParallelDecoder3D', 'params': {'double_z': True, 'z_channels': 16, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 2, 4], 'attn_resolutions': [], 'num_res_blocks': 3, 'dropout': 0.0, 'gather_norm': False}}}}, 'loss_fn_config': {'target': 'sgm.modules.diffusionmodules.loss.VideoDiffusionLoss', 'params': {'offset_noise_level': 0, 'sigma_sampler_config': {'target': 'sgm.modules.diffusionmodules.sigma_sampling.DiscreteSampling', 'params': {'uniform_sampling': True, 'num_idx': 1000, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 3.0}}}}}}, 'sampler_config': {'target': 'sgm.modules.diffusionmodules.sampling.VPSDEDPMPP2MSampler', 'params': {'num_steps': 50, 'verbose': True, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 3.0}}, 'guider_config': {'target': 'sgm.modules.diffusionmodules.guiders.DynamicCFG', 'params': {'scale': 6, 'exp': 5, 'num_steps': 50}}}}}, data_config={'target': 'data_video.SFTDataset', 'params': {'video_size': [480, 720], 'fps': 8, 'max_num_frames': 49, 'skip_frms_num': 3.0}}, cuda=True, rank=0, world_size=1, deepspeed_activation_checkpointing=True, master_ip='nm04-a800-node083', master_port='45865', log_config=[{'model': {'scale_factor': 1.15258426, 'disable_first_stage_autocast': True, 'not_trainable_prefixes': ['all'], 'log_keys': ['txt'], 'denoiser_config': {'target': 'sgm.modules.diffusionmodules.denoiser.DiscreteDenoiser', 'params': {'num_idx': 1000, 'quantize_c_noise': False, 'weighting_config': {'target': 'sgm.modules.diffusionmodules.denoiser_weighting.EpsWeighting'}, 'scaling_config': {'target': 'sgm.modules.diffusionmodules.denoiser_scaling.VideoScaling'}, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 3.0}}}}, 'network_config': {'target': 'dit_video_concat.DiffusionTransformer', 'params': {'time_embed_dim': 512, 'elementwise_affine': True, 'num_frames': 49, 'time_compressed_rate': 4, 'latent_width': 90, 'latent_height': 60, 'num_layers': 30, 'patch_size': 2, 'in_channels': 16, 'out_channels': 16, 'hidden_size': 1920, 'adm_in_channels': 256, 'num_attention_heads': 30, 'transformer_args': {'checkpoint_activations': True, 'vocab_size': 1, 'max_sequence_length': 64, 'layernorm_order': 'pre', 'skip_init': False, 'model_parallel_size': 1, 'is_decoder': False}, 'modules': {'pos_embed_config': {'target': 'dit_video_concat.Basic3DPositionEmbeddingMixin', 'params': {'text_length': 226, 'height_interpolation': 1.875, 'width_interpolation': 1.875}}, 'lora_config': {'target': 'sat.model.finetune.lora2.LoraMixin', 'params': {'r': 128}}, 'patch_embed_config': {'target': 'dit_video_concat.ImagePatchEmbeddingMixin', 'params': {'text_hidden_size': 4096}}, 'adaln_layer_config': {'target': 'dit_video_concat.AdaLNMixin', 'params': {'qk_ln': True}}, 'final_layer_config': {'target': 'dit_video_concat.FinalLayerMixin'}}}}, 'conditioner_config': {'target': 'sgm.modules.GeneralConditioner', 'params': {'emb_models': [{'is_trainable': False, 'input_key': 'txt', 'ucg_rate': 0.1, 'target': 'sgm.modules.encoders.modules.FrozenT5Embedder', 'params': {'model_dir': '/mnt/ceph/develop/jiawei/model_checkpoint/t5-v1_1-xxl', 'max_length': 226}}]}}, 'first_stage_config': {'target': 'vae_modules.autoencoder.VideoAutoencoderInferenceWrapper', 'params': {'cp_size': 1, 'ckpt_path': '/mnt/ceph/develop/jiawei/model_checkpoint/CogVideoX-2b-sat/vae/3d-vae.pt', 'ignore_keys': ['loss'], 'loss_config': {'target': 'torch.nn.Identity'}, 'regularizer_config': {'target': 'vae_modules.regularizers.DiagonalGaussianRegularizer'}, 'encoder_config': {'target': 'vae_modules.cp_enc_dec.ContextParallelEncoder3D', 'params': {'double_z': True, 'z_channels': 16, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 2, 4], 'attn_resolutions': [], 'num_res_blocks': 3, 'dropout': 0.0, 'gather_norm': True}}, 'decoder_config': {'target': 'vae_modules.cp_enc_dec.ContextParallelDecoder3D', 'params': {'double_z': True, 'z_channels': 16, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 2, 4], 'attn_resolutions': [], 'num_res_blocks': 3, 'dropout': 0.0, 'gather_norm': False}}}}, 'loss_fn_config': {'target': 'sgm.modules.diffusionmodules.loss.VideoDiffusionLoss', 'params': {'offset_noise_level': 0, 'sigma_sampler_config': {'target': 'sgm.modules.diffusionmodules.sigma_sampling.DiscreteSampling', 'params': {'uniform_sampling': True, 'num_idx': 1000, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 3.0}}}}}}, 'sampler_config': {'target': 'sgm.modules.diffusionmodules.sampling.VPSDEDPMPP2MSampler', 'params': {'num_steps': 50, 'verbose': True, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 3.0}}, 'guider_config': {'target': 'sgm.modules.diffusionmodules.guiders.DynamicCFG', 'params': {'scale': 6, 'exp': 5, 'num_steps': 50}}}}}}, {'args': {'checkpoint_activations': True, 'model_parallel_size': 1, 'experiment_name': 'lora-disney', 'mode': 'finetune', 'load': '/mnt/ceph/develop/jiawei/model_checkpoint/CogVideoX-2b-sat/transformer', 'no_load_rng': True, 'train_iters': 1000, 'eval_iters': 1, 'eval_interval': 100, 'eval_batch_size': 1, 'save': '/mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora', 'save_interval': 500, 'log_interval': 20, 'train_data': ['/mnt/ceph/develop/jiawei/lora_dataset/cogvideo_lora_dataset'], 'valid_data': ['/mnt/ceph/develop/jiawei/lora_dataset/cogvideo_lora_dataset'], 'split': '1,0,0', 'num_workers': 8, 'force_train': True, 'only_log_video_latents': False, 'wandb': True, 'wandb_project_name': 'cogv_2b_lora_base_001', 'summary_dir': '/mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora'}, 'data': {'target': 'data_video.SFTDataset', 'params': {'video_size': [480, 720], 'fps': 8, 'max_num_frames': 49, 'skip_frms_num': 3.0}}, 'deepspeed': {'train_micro_batch_size_per_gpu': 2, 'gradient_accumulation_steps': 1, 'steps_per_print': 50, 'gradient_clipping': 0.1, 'zero_optimization': {'stage': 2, 'cpu_offload': False, 'contiguous_gradients': False, 'overlap_comm': True, 'reduce_scatter': True, 'reduce_bucket_size': 1000000000, 'allgather_bucket_size': 1000000000, 'load_from_fp32_weights': False}, 'zero_allow_untested_optimizer': True, 'bf16': {'enabled': False}, 'fp16': {'enabled': True}, 'loss_scale': 0, 'loss_scale_window': 400, 'hysteresis': 2, 'min_loss_scale': 1, 'optimizer': {'type': 'sat.ops.FusedEmaAdam', 'params': {'lr': '1E-3', 'betas': [0.9, 0.95], 'eps': '1e-8', 'weight_decay': '1e-4'}}, 'activation_checkpointing': {'partition_activations': False, 'contiguous_memory_optimization': False}, 'wall_clock_breakdown': False}}])
[2024-09-09 14:22:30,563] [INFO] [RANK 0] building SATVideoDiffusionEngine model ...
[2024-09-09 14:22:40,871] [WARNING] [RANK 0] Failed to load bitsandbytes:No module named 'bitsandbytes'
[2024-09-09 14:22:40,871] [INFO] [RANK 0] replacing layer 0 attention with lora
[2024-09-09 14:22:40,898] [INFO] [RANK 0] replacing layer 1 attention with lora
[2024-09-09 14:22:40,926] [INFO] [RANK 0] replacing layer 2 attention with lora
[2024-09-09 14:22:40,950] [INFO] [RANK 0] replacing layer 3 attention with lora
[2024-09-09 14:22:40,974] [INFO] [RANK 0] replacing layer 4 attention with lora
[2024-09-09 14:22:40,999] [INFO] [RANK 0] replacing layer 5 attention with lora
[2024-09-09 14:22:41,023] [INFO] [RANK 0] replacing layer 6 attention with lora
[2024-09-09 14:22:41,047] [INFO] [RANK 0] replacing layer 7 attention with lora
[2024-09-09 14:22:41,070] [INFO] [RANK 0] replacing layer 8 attention with lora
[2024-09-09 14:22:41,093] [INFO] [RANK 0] replacing layer 9 attention with lora
[2024-09-09 14:22:41,116] [INFO] [RANK 0] replacing layer 10 attention with lora
[2024-09-09 14:22:41,140] [INFO] [RANK 0] replacing layer 11 attention with lora
[2024-09-09 14:22:41,164] [INFO] [RANK 0] replacing layer 12 attention with lora
[2024-09-09 14:22:41,186] [INFO] [RANK 0] replacing layer 13 attention with lora
[2024-09-09 14:22:41,209] [INFO] [RANK 0] replacing layer 14 attention with lora
[2024-09-09 14:22:41,232] [INFO] [RANK 0] replacing layer 15 attention with lora
[2024-09-09 14:22:41,255] [INFO] [RANK 0] replacing layer 16 attention with lora
[2024-09-09 14:22:41,278] [INFO] [RANK 0] replacing layer 17 attention with lora
[2024-09-09 14:22:41,301] [INFO] [RANK 0] replacing layer 18 attention with lora
[2024-09-09 14:22:41,324] [INFO] [RANK 0] replacing layer 19 attention with lora
[2024-09-09 14:22:41,347] [INFO] [RANK 0] replacing layer 20 attention with lora
[2024-09-09 14:22:41,370] [INFO] [RANK 0] replacing layer 21 attention with lora
[2024-09-09 14:22:41,394] [INFO] [RANK 0] replacing layer 22 attention with lora
[2024-09-09 14:22:41,416] [INFO] [RANK 0] replacing layer 23 attention with lora
[2024-09-09 14:22:41,436] [INFO] [RANK 0] replacing layer 24 attention with lora
[2024-09-09 14:22:41,456] [INFO] [RANK 0] replacing layer 25 attention with lora
[2024-09-09 14:22:41,476] [INFO] [RANK 0] replacing layer 26 attention with lora
[2024-09-09 14:22:41,496] [INFO] [RANK 0] replacing layer 27 attention with lora
[2024-09-09 14:22:41,517] [INFO] [RANK 0] replacing layer 28 attention with lora
[2024-09-09 14:22:41,537] [INFO] [RANK 0] replacing layer 29 attention with lora
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  1.46it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.57it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.55it/s]
Initialized embedder #0: FrozenT5Embedder with 4762310656 params. Trainable: False
Working with z of shape (1, 16, 32, 32) = 16384 dimensions.
/mnt/ceph/develop/jiawei/CogVideo/sat/vae_modules/autoencoder.py:565: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  sd = torch.load(path, map_location="cpu")["state_dict"]
Deleting key loss.logvar from state_dict.
Deleting key loss.perceptual_loss.scaling_layer.shift from state_dict.
Deleting key loss.perceptual_loss.scaling_layer.scale from state_dict.
Deleting key loss.perceptual_loss.net.slice1.0.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice1.0.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice1.2.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice1.2.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice2.5.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice2.5.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice2.7.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice2.7.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice3.10.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice3.10.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice3.12.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice3.12.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice3.14.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice3.14.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice4.17.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice4.17.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice4.19.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice4.19.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice4.21.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice4.21.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice5.24.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice5.24.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice5.26.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice5.26.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice5.28.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice5.28.bias from state_dict.
Deleting key loss.perceptual_loss.lin0.model.1.weight from state_dict.
Deleting key loss.perceptual_loss.lin1.model.1.weight from state_dict.
Deleting key loss.perceptual_loss.lin2.model.1.weight from state_dict.
Deleting key loss.perceptual_loss.lin3.model.1.weight from state_dict.
Deleting key loss.perceptual_loss.lin4.model.1.weight from state_dict.
Deleting key loss.discriminator.blocks.0.downsample_res.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.0.downsample_res.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.0.net.0.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.0.net.0.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.0.net.2.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.0.net.2.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.0.downsample.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.0.downsample.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.1.downsample_res.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.1.downsample_res.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.1.net.0.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.1.net.0.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.1.net.2.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.1.net.2.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.1.downsample.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.1.downsample.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.2.downsample_res.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.2.downsample_res.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.2.net.0.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.2.net.0.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.2.net.2.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.2.net.2.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.2.downsample.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.2.downsample.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.3.downsample_res.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.3.downsample_res.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.3.net.0.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.3.net.0.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.3.net.2.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.3.net.2.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.3.downsample.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.3.downsample.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.4.0.conv_res.weight from state_dict.
Deleting key loss.discriminator.blocks.4.0.conv_res.bias from state_dict.
Deleting key loss.discriminator.blocks.4.0.net.0.weight from state_dict.
Deleting key loss.discriminator.blocks.4.0.net.0.bias from state_dict.
Deleting key loss.discriminator.blocks.4.0.net.2.weight from state_dict.
Deleting key loss.discriminator.blocks.4.0.net.2.bias from state_dict.
Deleting key loss.discriminator.blocks.4.0.downsample.1.weight from state_dict.
Deleting key loss.discriminator.blocks.4.0.downsample.1.bias from state_dict.
Deleting key loss.discriminator.blocks.4.1.0.fn.norm.gamma from state_dict.
Deleting key loss.discriminator.blocks.4.1.0.fn.attn.to_q.0.weight from state_dict.
Deleting key loss.discriminator.blocks.4.1.0.fn.attn.to_kv.0.weight from state_dict.
Deleting key loss.discriminator.blocks.4.1.0.fn.attn.to_out.0.weight from state_dict.
Deleting key loss.discriminator.blocks.4.1.1.fn.norm.gamma from state_dict.
Deleting key loss.discriminator.blocks.4.1.1.fn.net.0.weight from state_dict.
Deleting key loss.discriminator.blocks.4.1.1.fn.net.0.bias from state_dict.
Deleting key loss.discriminator.blocks.4.1.1.fn.net.2.weight from state_dict.
Deleting key loss.discriminator.blocks.4.1.1.fn.net.2.bias from state_dict.
Deleting key loss.discriminator.blocks.5.0.conv_res.weight from state_dict.
Deleting key loss.discriminator.blocks.5.0.conv_res.bias from state_dict.
Deleting key loss.discriminator.blocks.5.0.net.0.weight from state_dict.
Deleting key loss.discriminator.blocks.5.0.net.0.bias from state_dict.
Deleting key loss.discriminator.blocks.5.0.net.2.weight from state_dict.
Deleting key loss.discriminator.blocks.5.0.net.2.bias from state_dict.
Deleting key loss.discriminator.blocks.5.0.downsample.1.weight from state_dict.
Deleting key loss.discriminator.blocks.5.0.downsample.1.bias from state_dict.
Deleting key loss.discriminator.blocks.5.1.0.fn.norm.gamma from state_dict.
Deleting key loss.discriminator.blocks.5.1.0.fn.attn.to_q.0.weight from state_dict.
Deleting key loss.discriminator.blocks.5.1.0.fn.attn.to_kv.0.weight from state_dict.
Deleting key loss.discriminator.blocks.5.1.0.fn.attn.to_out.0.weight from state_dict.
Deleting key loss.discriminator.blocks.5.1.1.fn.norm.gamma from state_dict.
Deleting key loss.discriminator.blocks.5.1.1.fn.net.0.weight from state_dict.
Deleting key loss.discriminator.blocks.5.1.1.fn.net.0.bias from state_dict.
Deleting key loss.discriminator.blocks.5.1.1.fn.net.2.weight from state_dict.
Deleting key loss.discriminator.blocks.5.1.1.fn.net.2.bias from state_dict.
Deleting key loss.discriminator.blocks.6.0.conv_res.weight from state_dict.
Deleting key loss.discriminator.blocks.6.0.conv_res.bias from state_dict.
Deleting key loss.discriminator.blocks.6.0.net.0.weight from state_dict.
Deleting key loss.discriminator.blocks.6.0.net.0.bias from state_dict.
Deleting key loss.discriminator.blocks.6.0.net.2.weight from state_dict.
Deleting key loss.discriminator.blocks.6.0.net.2.bias from state_dict.
Deleting key loss.discriminator.blocks.6.1.0.fn.norm.gamma from state_dict.
Deleting key loss.discriminator.blocks.6.1.0.fn.attn.to_q.0.weight from state_dict.
Deleting key loss.discriminator.blocks.6.1.0.fn.attn.to_kv.0.weight from state_dict.
Deleting key loss.discriminator.blocks.6.1.0.fn.attn.to_out.0.weight from state_dict.
Deleting key loss.discriminator.blocks.6.1.1.fn.norm.gamma from state_dict.
Deleting key loss.discriminator.blocks.6.1.1.fn.net.0.weight from state_dict.
Deleting key loss.discriminator.blocks.6.1.1.fn.net.0.bias from state_dict.
Deleting key loss.discriminator.blocks.6.1.1.fn.net.2.weight from state_dict.
Deleting key loss.discriminator.blocks.6.1.1.fn.net.2.bias from state_dict.
Deleting key loss.discriminator.to_logits.0.weight from state_dict.
Deleting key loss.discriminator.to_logits.0.bias from state_dict.
Deleting key loss.discriminator.to_logits.3.weight from state_dict.
Deleting key loss.discriminator.to_logits.3.bias from state_dict.
Missing keys:  []
Unexpected keys:  []
Restored from /mnt/ceph/develop/jiawei/model_checkpoint/CogVideoX-2b-sat/vae/3d-vae.pt
[2024-09-09 14:22:45,703] [INFO] [RANK 0]  > number of parameters on model parallel rank 0: 6764790755
[2024-09-09 14:22:52,263] [INFO] [RANK 0] global rank 0 is loading checkpoint /mnt/ceph/develop/jiawei/model_checkpoint/CogVideoX-2b-sat/transformer/1000/mp_rank_00_model_states.pt
/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/sat/training/model_io.py:286: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  sd = torch.load(checkpoint_name, map_location='cpu')
[2024-09-09 14:22:54,301] [INFO] [RANK 0] > successfully loaded /mnt/ceph/develop/jiawei/model_checkpoint/CogVideoX-2b-sat/transformer/1000/mp_rank_00_model_states.pt
[2024-09-09 14:22:54,702] [INFO] [RANK 0] ***** Total trainable parameters: 58982400 *****
[2024-09-09 14:22:54,703] [INFO] [RANK 0] [<class 'sat.ops.layernorm.LayerNorm'>, <class 'torch.nn.modules.normalization.LayerNorm'>, <class 'sat.ops.layernorm.RMSNorm'>] is set to no_weight_decay
[2024-09-09 14:22:54,709] [INFO] [RANK 0] Syncing initialized parameters...
[2024-09-09 14:22:54,770] [INFO] [RANK 0] Finished syncing initialized parameters.
[2024-09-09 14:22:54,771] [INFO] [RANK 0] Using optimizer sat.ops.FusedEmaAdam from sat.
[2024-09-09 14:22:54,773] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.4, git-hash=unknown, git-branch=unknown
[2024-09-09 14:22:54,774] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
[2024-09-09 14:22:54,862] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /root/.cache/torch_extensions/py310_cu121/fused_ema_adam/build.ninja...
/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1965: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module fused_ema_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_ema_adam...
Time to load fused_ema_adam op: 3.563777446746826 seconds
[2024-09-09 14:22:58,453] [INFO] [logging.py:96:log_dist] [Rank 0] Using client callable to create basic optimizer
[2024-09-09 14:22:58,453] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-09-09 14:22:58,481] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedEmaAdam
[2024-09-09 14:22:58,481] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedEmaAdam type=<class 'sat.ops.fused_ema_adam.FusedEmaAdam'>
[2024-09-09 14:22:58,481] [WARNING] [engine.py:1179:_do_optimizer_sanity_check] **** You are using ZeRO with an untested optimizer, proceed with caution *****
[2024-09-09 14:22:58,484] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2024-09-09 14:22:58,484] [INFO] [stage_1_and_2.py:148:__init__] Reduce bucket size 1000000000
[2024-09-09 14:22:58,484] [INFO] [stage_1_and_2.py:149:__init__] Allgather bucket size 1000000000
[2024-09-09 14:22:58,484] [INFO] [stage_1_and_2.py:150:__init__] CPU Offload: False
[2024-09-09 14:22:58,484] [INFO] [stage_1_and_2.py:151:__init__] Round robin gradient partitioning: False
[2024-09-09 14:22:58,739] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2024-09-09 14:22:58,740] [INFO] [utils.py:782:see_memory_usage] MA 12.86 GB         Max_MA 12.97 GB         CA 13.23 GB         Max_CA 13 GB 
[2024-09-09 14:22:58,742] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 115.59 GB, percent = 5.7%
[2024-09-09 14:22:58,880] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2024-09-09 14:22:58,881] [INFO] [utils.py:782:see_memory_usage] MA 12.86 GB         Max_MA 13.08 GB         CA 13.45 GB         Max_CA 13 GB 
[2024-09-09 14:22:58,883] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 115.59 GB, percent = 5.7%
[2024-09-09 14:22:58,883] [INFO] [stage_1_and_2.py:543:__init__] optimizer state initialized
[2024-09-09 14:22:59,015] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2024-09-09 14:22:59,016] [INFO] [utils.py:782:see_memory_usage] MA 12.86 GB         Max_MA 12.86 GB         CA 13.45 GB         Max_CA 13 GB 
[2024-09-09 14:22:59,019] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 115.59 GB, percent = 5.7%
[2024-09-09 14:22:59,023] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer
[2024-09-09 14:22:59,023] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2024-09-09 14:22:59,025] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2024-09-09 14:22:59,025] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[1.0], mom=[[0.9, 0.95]]
[2024-09-09 14:22:59,027] [INFO] [config.py:997:print] DeepSpeedEngine configuration:
[2024-09-09 14:22:59,029] [INFO] [config.py:1001:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-09-09 14:22:59,029] [INFO] [config.py:1001:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-09-09 14:22:59,031] [INFO] [config.py:1001:print]   amp_enabled .................. False
[2024-09-09 14:22:59,031] [INFO] [config.py:1001:print]   amp_params ................... False
[2024-09-09 14:22:59,031] [INFO] [config.py:1001:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-09-09 14:22:59,031] [INFO] [config.py:1001:print]   bfloat16_enabled ............. False
[2024-09-09 14:22:59,031] [INFO] [config.py:1001:print]   bfloat16_immediate_grad_update  False
[2024-09-09 14:22:59,031] [INFO] [config.py:1001:print]   checkpoint_parallel_write_pipeline  False
[2024-09-09 14:22:59,031] [INFO] [config.py:1001:print]   checkpoint_tag_validation_enabled  True
[2024-09-09 14:22:59,031] [INFO] [config.py:1001:print]   checkpoint_tag_validation_fail  False
[2024-09-09 14:22:59,031] [INFO] [config.py:1001:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f6f48d77ac0>
[2024-09-09 14:22:59,031] [INFO] [config.py:1001:print]   communication_data_type ...... None
[2024-09-09 14:22:59,031] [INFO] [config.py:1001:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-09-09 14:22:59,031] [INFO] [config.py:1001:print]   curriculum_enabled_legacy .... False
[2024-09-09 14:22:59,031] [INFO] [config.py:1001:print]   curriculum_params_legacy ..... False
[2024-09-09 14:22:59,031] [INFO] [config.py:1001:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-09-09 14:22:59,031] [INFO] [config.py:1001:print]   data_efficiency_enabled ...... False
[2024-09-09 14:22:59,031] [INFO] [config.py:1001:print]   dataloader_drop_last ......... False
[2024-09-09 14:22:59,031] [INFO] [config.py:1001:print]   disable_allgather ............ False
[2024-09-09 14:22:59,031] [INFO] [config.py:1001:print]   dump_state ................... False
[2024-09-09 14:22:59,031] [INFO] [config.py:1001:print]   dynamic_loss_scale_args ...... None
[2024-09-09 14:22:59,034] [INFO] [config.py:1001:print]   eigenvalue_enabled ........... False
[2024-09-09 14:22:59,034] [INFO] [config.py:1001:print]   eigenvalue_gas_boundary_resolution  1
[2024-09-09 14:22:59,034] [INFO] [config.py:1001:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-09-09 14:22:59,034] [INFO] [config.py:1001:print]   eigenvalue_layer_num ......... 0
[2024-09-09 14:22:59,034] [INFO] [config.py:1001:print]   eigenvalue_max_iter .......... 100
[2024-09-09 14:22:59,034] [INFO] [config.py:1001:print]   eigenvalue_stability ......... 1e-06
[2024-09-09 14:22:59,034] [INFO] [config.py:1001:print]   eigenvalue_tol ............... 0.01
[2024-09-09 14:22:59,034] [INFO] [config.py:1001:print]   eigenvalue_verbose ........... False
[2024-09-09 14:22:59,034] [INFO] [config.py:1001:print]   elasticity_enabled ........... False
[2024-09-09 14:22:59,034] [INFO] [config.py:1001:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-09-09 14:22:59,034] [INFO] [config.py:1001:print]   fp16_auto_cast ............... False
[2024-09-09 14:22:59,034] [INFO] [config.py:1001:print]   fp16_enabled ................. True
[2024-09-09 14:22:59,034] [INFO] [config.py:1001:print]   fp16_master_weights_and_gradients  False
[2024-09-09 14:22:59,034] [INFO] [config.py:1001:print]   global_rank .................. 0
[2024-09-09 14:22:59,034] [INFO] [config.py:1001:print]   grad_accum_dtype ............. None
[2024-09-09 14:22:59,034] [INFO] [config.py:1001:print]   gradient_accumulation_steps .. 1
[2024-09-09 14:22:59,034] [INFO] [config.py:1001:print]   gradient_clipping ............ 0.1
[2024-09-09 14:22:59,034] [INFO] [config.py:1001:print]   gradient_predivide_factor .... 1.0
[2024-09-09 14:22:59,034] [INFO] [config.py:1001:print]   graph_harvesting ............. False
[2024-09-09 14:22:59,034] [INFO] [config.py:1001:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-09-09 14:22:59,034] [INFO] [config.py:1001:print]   initial_dynamic_scale ........ 65536
[2024-09-09 14:22:59,034] [INFO] [config.py:1001:print]   load_universal_checkpoint .... False
[2024-09-09 14:22:59,034] [INFO] [config.py:1001:print]   loss_scale ................... 0
[2024-09-09 14:22:59,034] [INFO] [config.py:1001:print]   memory_breakdown ............. False
[2024-09-09 14:22:59,034] [INFO] [config.py:1001:print]   mics_hierarchial_params_gather  False
[2024-09-09 14:22:59,034] [INFO] [config.py:1001:print]   mics_shard_size .............. -1
[2024-09-09 14:22:59,034] [INFO] [config.py:1001:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-09-09 14:22:59,034] [INFO] [config.py:1001:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-09-09 14:22:59,037] [INFO] [config.py:1001:print]   optimizer_legacy_fusion ...... False
[2024-09-09 14:22:59,037] [INFO] [config.py:1001:print]   optimizer_name ............... None
[2024-09-09 14:22:59,037] [INFO] [config.py:1001:print]   optimizer_params ............. None
[2024-09-09 14:22:59,037] [INFO] [config.py:1001:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-09-09 14:22:59,037] [INFO] [config.py:1001:print]   pld_enabled .................. False
[2024-09-09 14:22:59,037] [INFO] [config.py:1001:print]   pld_params ................... False
[2024-09-09 14:22:59,037] [INFO] [config.py:1001:print]   prescale_gradients ........... False
[2024-09-09 14:22:59,037] [INFO] [config.py:1001:print]   scheduler_name ............... None
[2024-09-09 14:22:59,037] [INFO] [config.py:1001:print]   scheduler_params ............. None
[2024-09-09 14:22:59,037] [INFO] [config.py:1001:print]   seq_parallel_communication_data_type  torch.float32
[2024-09-09 14:22:59,037] [INFO] [config.py:1001:print]   sparse_attention ............. None
[2024-09-09 14:22:59,037] [INFO] [config.py:1001:print]   sparse_gradients_enabled ..... False
[2024-09-09 14:22:59,037] [INFO] [config.py:1001:print]   steps_per_print .............. 50
[2024-09-09 14:22:59,037] [INFO] [config.py:1001:print]   timers_config ................ enabled=True synchronized=True
[2024-09-09 14:22:59,037] [INFO] [config.py:1001:print]   train_batch_size ............. 2
[2024-09-09 14:22:59,037] [INFO] [config.py:1001:print]   train_micro_batch_size_per_gpu  2
[2024-09-09 14:22:59,037] [INFO] [config.py:1001:print]   use_data_before_expert_parallel_  False
[2024-09-09 14:22:59,040] [INFO] [config.py:1001:print]   use_node_local_storage ....... False
[2024-09-09 14:22:59,040] [INFO] [config.py:1001:print]   wall_clock_breakdown ......... False
[2024-09-09 14:22:59,040] [INFO] [config.py:1001:print]   weight_quantization_config ... None
[2024-09-09 14:22:59,040] [INFO] [config.py:1001:print]   world_size ................... 1
[2024-09-09 14:22:59,040] [INFO] [config.py:1001:print]   zero_allow_untested_optimizer  True
[2024-09-09 14:22:59,040] [INFO] [config.py:1001:print]   zero_config .................. stage=2 contiguous_gradients=False reduce_scatter=True reduce_bucket_size=1000000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=1000000000 overlap_comm=True load_from_fp32_weights=False elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-09-09 14:22:59,040] [INFO] [config.py:1001:print]   zero_enabled ................. True
[2024-09-09 14:22:59,040] [INFO] [config.py:1001:print]   zero_force_ds_cpu_optimizer .. True
[2024-09-09 14:22:59,040] [INFO] [config.py:1001:print]   zero_optimization_stage ...... 2
[2024-09-09 14:22:59,040] [INFO] [config.py:987:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 2, 
    "gradient_accumulation_steps": 1, 
    "steps_per_print": 50, 
    "gradient_clipping": 0.1, 
    "zero_optimization": {
        "stage": 2, 
        "cpu_offload": false, 
        "contiguous_gradients": false, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 1.000000e+09, 
        "allgather_bucket_size": 1.000000e+09, 
        "load_from_fp32_weights": false
    }, 
    "zero_allow_untested_optimizer": true, 
    "bf16": {
        "enabled": false
    }, 
    "fp16": {
        "enabled": true
    }, 
    "loss_scale": 0, 
    "loss_scale_window": 400, 
    "hysteresis": 2, 
    "min_loss_scale": 1, 
    "activation_checkpointing": {
        "partition_activations": false, 
        "contiguous_memory_optimization": false
    }, 
    "wall_clock_breakdown": false
}
[2024-09-09 14:22:59,042] [INFO] [RANK 0] learning rate decaying style linear, ratio 10.0
[2024-09-09 14:22:59,042] [INFO] [RANK 0] Finetuning Model...
[2024-09-09 14:22:59,042] [INFO] [RANK 0] arguments:
[2024-09-09 14:22:59,042] [INFO] [RANK 0]   base ......................... ['configs/cogvideox_2b_lora.yaml', 'configs/sft.yaml']
[2024-09-09 14:22:59,042] [INFO] [RANK 0]   model_parallel_size .......... 1
[2024-09-09 14:22:59,042] [INFO] [RANK 0]   force_pretrain ............... False
[2024-09-09 14:22:59,042] [INFO] [RANK 0]   device ....................... 0
[2024-09-09 14:22:59,042] [INFO] [RANK 0]   debug ........................ False
[2024-09-09 14:22:59,042] [INFO] [RANK 0]   log_image .................... True
[2024-09-09 14:22:59,044] [INFO] [RANK 0]   output_dir ................... samples
[2024-09-09 14:22:59,044] [INFO] [RANK 0]   input_dir .................... None
[2024-09-09 14:22:59,044] [INFO] [RANK 0]   input_type ................... cli
[2024-09-09 14:22:59,044] [INFO] [RANK 0]   input_file ................... input.txt
[2024-09-09 14:22:59,044] [INFO] [RANK 0]   final_size ................... 2048
[2024-09-09 14:22:59,044] [INFO] [RANK 0]   sdedit ....................... False
[2024-09-09 14:22:59,044] [INFO] [RANK 0]   grid_num_rows ................ 1
[2024-09-09 14:22:59,044] [INFO] [RANK 0]   force_inference .............. False
[2024-09-09 14:22:59,044] [INFO] [RANK 0]   lcm_steps .................... None
[2024-09-09 14:22:59,044] [INFO] [RANK 0]   sampling_num_frames .......... 32
[2024-09-09 14:22:59,044] [INFO] [RANK 0]   sampling_fps ................. 8
[2024-09-09 14:22:59,044] [INFO] [RANK 0]   only_save_latents ............ False
[2024-09-09 14:22:59,046] [INFO] [RANK 0]   only_log_video_latents ....... False
[2024-09-09 14:22:59,046] [INFO] [RANK 0]   latent_channels .............. 32
[2024-09-09 14:22:59,047] [INFO] [RANK 0]   image2video .................. False
[2024-09-09 14:22:59,047] [INFO] [RANK 0]   experiment_name .............. lora-disney-09-09-14-22
[2024-09-09 14:22:59,047] [INFO] [RANK 0]   train_iters .................. 1000
[2024-09-09 14:22:59,047] [INFO] [RANK 0]   batch_size ................... 2
[2024-09-09 14:22:59,047] [INFO] [RANK 0]   lr ........................... 0.001
[2024-09-09 14:22:59,047] [INFO] [RANK 0]   mode ......................... finetune
[2024-09-09 14:22:59,047] [INFO] [RANK 0]   seed ......................... 42
[2024-09-09 14:22:59,047] [INFO] [RANK 0]   zero_stage ................... 0
[2024-09-09 14:22:59,047] [INFO] [RANK 0]   checkpoint_activations ....... True
[2024-09-09 14:22:59,047] [INFO] [RANK 0]   checkpoint_num_layers ........ 1
[2024-09-09 14:22:59,047] [INFO] [RANK 0]   checkpoint_skip_layers ....... 0
[2024-09-09 14:22:59,047] [INFO] [RANK 0]   fp16 ......................... True
[2024-09-09 14:22:59,047] [INFO] [RANK 0]   bf16 ......................... False
[2024-09-09 14:22:59,047] [INFO] [RANK 0]   gradient_accumulation_steps .. 1
[2024-09-09 14:22:59,049] [INFO] [RANK 0]   profiling .................... -1
[2024-09-09 14:22:59,049] [INFO] [RANK 0]   epochs ....................... None
[2024-09-09 14:22:59,049] [INFO] [RANK 0]   log_interval ................. 20
[2024-09-09 14:22:59,049] [INFO] [RANK 0]   summary_dir .................. /mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora
[2024-09-09 14:22:59,049] [INFO] [RANK 0]   save_args .................... False
[2024-09-09 14:22:59,049] [INFO] [RANK 0]   lr_decay_iters ............... None
[2024-09-09 14:22:59,050] [INFO] [RANK 0]   lr_decay_style ............... linear
[2024-09-09 14:22:59,050] [INFO] [RANK 0]   lr_decay_ratio ............... 0.1
[2024-09-09 14:22:59,050] [INFO] [RANK 0]   warmup ....................... 0.01
[2024-09-09 14:22:59,050] [INFO] [RANK 0]   weight_decay ................. 0.0001
[2024-09-09 14:22:59,050] [INFO] [RANK 0]   save ......................... /mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora/lora-disney-09-09-14-22
[2024-09-09 14:22:59,050] [INFO] [RANK 0]   load ......................... /mnt/ceph/develop/jiawei/model_checkpoint/CogVideoX-2b-sat/transformer
[2024-09-09 14:22:59,050] [INFO] [RANK 0]   force_train .................. True
[2024-09-09 14:22:59,050] [INFO] [RANK 0]   save_interval ................ 500
[2024-09-09 14:22:59,050] [INFO] [RANK 0]   no_save_rng .................. False
[2024-09-09 14:22:59,050] [INFO] [RANK 0]   no_load_rng .................. True
[2024-09-09 14:22:59,050] [INFO] [RANK 0]   resume_dataloader ............ False
[2024-09-09 14:22:59,050] [INFO] [RANK 0]   distributed_backend .......... nccl
[2024-09-09 14:22:59,050] [INFO] [RANK 0]   local_rank ................... 0
[2024-09-09 14:22:59,050] [INFO] [RANK 0]   exit_interval ................ None
[2024-09-09 14:22:59,050] [INFO] [RANK 0]   wandb ........................ True
[2024-09-09 14:22:59,050] [INFO] [RANK 0]   wandb_project_name ........... cogv_2b_lora_base_001
[2024-09-09 14:22:59,050] [INFO] [RANK 0]   eval_batch_size .............. 1
[2024-09-09 14:22:59,050] [INFO] [RANK 0]   eval_iters ................... 1
[2024-09-09 14:22:59,050] [INFO] [RANK 0]   eval_interval ................ 100
[2024-09-09 14:22:59,052] [INFO] [RANK 0]   strict_eval .................. False
[2024-09-09 14:22:59,052] [INFO] [RANK 0]   train_data ................... ['/mnt/ceph/develop/jiawei/lora_dataset/cogvideo_lora_dataset']
[2024-09-09 14:22:59,052] [INFO] [RANK 0]   train_data_weights ........... None
[2024-09-09 14:22:59,052] [INFO] [RANK 0]   iterable_dataset ............. False
[2024-09-09 14:22:59,052] [INFO] [RANK 0]   iterable_dataset_eval ........ 
[2024-09-09 14:22:59,052] [INFO] [RANK 0]   batch_from_same_dataset ...... False
[2024-09-09 14:22:59,052] [INFO] [RANK 0]   valid_data ................... ['/mnt/ceph/develop/jiawei/lora_dataset/cogvideo_lora_dataset']
[2024-09-09 14:22:59,052] [INFO] [RANK 0]   test_data .................... None
[2024-09-09 14:22:59,052] [INFO] [RANK 0]   split ........................ 1,0,0
[2024-09-09 14:22:59,052] [INFO] [RANK 0]   num_workers .................. 8
[2024-09-09 14:22:59,052] [INFO] [RANK 0]   block_size ................... 10000
[2024-09-09 14:22:59,054] [INFO] [RANK 0]   prefetch_factor .............. 4
[2024-09-09 14:22:59,054] [INFO] [RANK 0]   deepspeed .................... True
[2024-09-09 14:22:59,054] [INFO] [RANK 0]   deepspeed_config ............. {'train_micro_batch_size_per_gpu': 2, 'gradient_accumulation_steps': 1, 'steps_per_print': 50, 'gradient_clipping': 0.1, 'zero_optimization': {'stage': 2, 'cpu_offload': False, 'contiguous_gradients': False, 'overlap_comm': True, 'reduce_scatter': True, 'reduce_bucket_size': 1000000000, 'allgather_bucket_size': 1000000000, 'load_from_fp32_weights': False}, 'zero_allow_untested_optimizer': True, 'bf16': {'enabled': False}, 'fp16': {'enabled': True}, 'loss_scale': 0, 'loss_scale_window': 400, 'hysteresis': 2, 'min_loss_scale': 1, 'activation_checkpointing': {'partition_activations': False, 'contiguous_memory_optimization': False}, 'wall_clock_breakdown': False}
[2024-09-09 14:22:59,054] [INFO] [RANK 0]   deepscale .................... False
[2024-09-09 14:22:59,054] [INFO] [RANK 0]   deepscale_config ............. None
[2024-09-09 14:22:59,055] [INFO] [RANK 0]   model_config ................. {'scale_factor': 1.15258426, 'disable_first_stage_autocast': True, 'not_trainable_prefixes': ['all'], 'log_keys': ['txt'], 'denoiser_config': {'target': 'sgm.modules.diffusionmodules.denoiser.DiscreteDenoiser', 'params': {'num_idx': 1000, 'quantize_c_noise': False, 'weighting_config': {'target': 'sgm.modules.diffusionmodules.denoiser_weighting.EpsWeighting'}, 'scaling_config': {'target': 'sgm.modules.diffusionmodules.denoiser_scaling.VideoScaling'}, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 3.0}}}}, 'network_config': {'target': 'dit_video_concat.DiffusionTransformer', 'params': {'time_embed_dim': 512, 'elementwise_affine': True, 'num_frames': 49, 'time_compressed_rate': 4, 'latent_width': 90, 'latent_height': 60, 'num_layers': 30, 'patch_size': 2, 'in_channels': 16, 'out_channels': 16, 'hidden_size': 1920, 'adm_in_channels': 256, 'num_attention_heads': 30, 'transformer_args': {'checkpoint_activations': True, 'vocab_size': 1, 'max_sequence_length': 64, 'layernorm_order': 'pre', 'skip_init': False, 'model_parallel_size': 1, 'is_decoder': False, 'num_layers': 30, 'hidden_size': 1920, 'num_attention_heads': 30, 'parallel_output': True}, 'modules': {'pos_embed_config': {'target': 'dit_video_concat.Basic3DPositionEmbeddingMixin', 'params': {'text_length': 226, 'height_interpolation': 1.875, 'width_interpolation': 1.875}}, 'lora_config': {'target': 'sat.model.finetune.lora2.LoraMixin', 'params': {'r': 128}}, 'patch_embed_config': {'target': 'dit_video_concat.ImagePatchEmbeddingMixin', 'params': {'text_hidden_size': 4096}}, 'adaln_layer_config': {'target': 'dit_video_concat.AdaLNMixin', 'params': {'qk_ln': True}}, 'final_layer_config': {'target': 'dit_video_concat.FinalLayerMixin'}}, 'dtype': 'fp16'}}, 'conditioner_config': {'target': 'sgm.modules.GeneralConditioner', 'params': {'emb_models': [{'is_trainable': False, 'input_key': 'txt', 'ucg_rate': 0.1, 'target': 'sgm.modules.encoders.modules.FrozenT5Embedder', 'params': {'model_dir': '/mnt/ceph/develop/jiawei/model_checkpoint/t5-v1_1-xxl', 'max_length': 226}}]}}, 'first_stage_config': {'target': 'vae_modules.autoencoder.VideoAutoencoderInferenceWrapper', 'params': {'cp_size': 1, 'ckpt_path': '/mnt/ceph/develop/jiawei/model_checkpoint/CogVideoX-2b-sat/vae/3d-vae.pt', 'ignore_keys': ['loss'], 'loss_config': {'target': 'torch.nn.Identity'}, 'regularizer_config': {'target': 'vae_modules.regularizers.DiagonalGaussianRegularizer'}, 'encoder_config': {'target': 'vae_modules.cp_enc_dec.ContextParallelEncoder3D', 'params': {'double_z': True, 'z_channels': 16, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 2, 4], 'attn_resolutions': [], 'num_res_blocks': 3, 'dropout': 0.0, 'gather_norm': True}}, 'decoder_config': {'target': 'vae_modules.cp_enc_dec.ContextParallelDecoder3D', 'params': {'double_z': True, 'z_channels': 16, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 2, 4], 'attn_resolutions': [], 'num_res_blocks': 3, 'dropout': 0.0, 'gather_norm': False}}}}, 'loss_fn_config': {'target': 'sgm.modules.diffusionmodules.loss.VideoDiffusionLoss', 'params': {'offset_noise_level': 0, 'sigma_sampler_config': {'target': 'sgm.modules.diffusionmodules.sigma_sampling.DiscreteSampling', 'params': {'uniform_sampling': True, 'num_idx': 1000, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 3.0}}}}}}, 'sampler_config': {'target': 'sgm.modules.diffusionmodules.sampling.VPSDEDPMPP2MSampler', 'params': {'num_steps': 50, 'verbose': True, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 3.0}}, 'guider_config': {'target': 'sgm.modules.diffusionmodules.guiders.DynamicCFG', 'params': {'scale': 6, 'exp': 5, 'num_steps': 50}}}}}
[2024-09-09 14:22:59,057] [INFO] [RANK 0]   data_config .................. {'target': 'data_video.SFTDataset', 'params': {'video_size': [480, 720], 'fps': 8, 'max_num_frames': 49, 'skip_frms_num': 3.0}}
[2024-09-09 14:22:59,057] [INFO] [RANK 0]   cuda ......................... True
[2024-09-09 14:22:59,057] [INFO] [RANK 0]   rank ......................... 0
[2024-09-09 14:22:59,057] [INFO] [RANK 0]   world_size ................... 1
[2024-09-09 14:22:59,057] [INFO] [RANK 0]   deepspeed_activation_checkpointing  True
[2024-09-09 14:22:59,057] [INFO] [RANK 0]   master_ip .................... nm04-a800-node083
[2024-09-09 14:22:59,057] [INFO] [RANK 0]   master_port .................. 45865
[2024-09-09 14:22:59,057] [INFO] [RANK 0]   log_config ................... [{'model': {'scale_factor': 1.15258426, 'disable_first_stage_autocast': True, 'not_trainable_prefixes': ['all'], 'log_keys': ['txt'], 'denoiser_config': {'target': 'sgm.modules.diffusionmodules.denoiser.DiscreteDenoiser', 'params': {'num_idx': 1000, 'quantize_c_noise': False, 'weighting_config': {'target': 'sgm.modules.diffusionmodules.denoiser_weighting.EpsWeighting'}, 'scaling_config': {'target': 'sgm.modules.diffusionmodules.denoiser_scaling.VideoScaling'}, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 3.0}}}}, 'network_config': {'target': 'dit_video_concat.DiffusionTransformer', 'params': {'time_embed_dim': 512, 'elementwise_affine': True, 'num_frames': 49, 'time_compressed_rate': 4, 'latent_width': 90, 'latent_height': 60, 'num_layers': 30, 'patch_size': 2, 'in_channels': 16, 'out_channels': 16, 'hidden_size': 1920, 'adm_in_channels': 256, 'num_attention_heads': 30, 'transformer_args': {'checkpoint_activations': True, 'vocab_size': 1, 'max_sequence_length': 64, 'layernorm_order': 'pre', 'skip_init': False, 'model_parallel_size': 1, 'is_decoder': False}, 'modules': {'pos_embed_config': {'target': 'dit_video_concat.Basic3DPositionEmbeddingMixin', 'params': {'text_length': 226, 'height_interpolation': 1.875, 'width_interpolation': 1.875}}, 'lora_config': {'target': 'sat.model.finetune.lora2.LoraMixin', 'params': {'r': 128}}, 'patch_embed_config': {'target': 'dit_video_concat.ImagePatchEmbeddingMixin', 'params': {'text_hidden_size': 4096}}, 'adaln_layer_config': {'target': 'dit_video_concat.AdaLNMixin', 'params': {'qk_ln': True}}, 'final_layer_config': {'target': 'dit_video_concat.FinalLayerMixin'}}}}, 'conditioner_config': {'target': 'sgm.modules.GeneralConditioner', 'params': {'emb_models': [{'is_trainable': False, 'input_key': 'txt', 'ucg_rate': 0.1, 'target': 'sgm.modules.encoders.modules.FrozenT5Embedder', 'params': {'model_dir': '/mnt/ceph/develop/jiawei/model_checkpoint/t5-v1_1-xxl', 'max_length': 226}}]}}, 'first_stage_config': {'target': 'vae_modules.autoencoder.VideoAutoencoderInferenceWrapper', 'params': {'cp_size': 1, 'ckpt_path': '/mnt/ceph/develop/jiawei/model_checkpoint/CogVideoX-2b-sat/vae/3d-vae.pt', 'ignore_keys': ['loss'], 'loss_config': {'target': 'torch.nn.Identity'}, 'regularizer_config': {'target': 'vae_modules.regularizers.DiagonalGaussianRegularizer'}, 'encoder_config': {'target': 'vae_modules.cp_enc_dec.ContextParallelEncoder3D', 'params': {'double_z': True, 'z_channels': 16, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 2, 4], 'attn_resolutions': [], 'num_res_blocks': 3, 'dropout': 0.0, 'gather_norm': True}}, 'decoder_config': {'target': 'vae_modules.cp_enc_dec.ContextParallelDecoder3D', 'params': {'double_z': True, 'z_channels': 16, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 2, 4], 'attn_resolutions': [], 'num_res_blocks': 3, 'dropout': 0.0, 'gather_norm': False}}}}, 'loss_fn_config': {'target': 'sgm.modules.diffusionmodules.loss.VideoDiffusionLoss', 'params': {'offset_noise_level': 0, 'sigma_sampler_config': {'target': 'sgm.modules.diffusionmodules.sigma_sampling.DiscreteSampling', 'params': {'uniform_sampling': True, 'num_idx': 1000, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 3.0}}}}}}, 'sampler_config': {'target': 'sgm.modules.diffusionmodules.sampling.VPSDEDPMPP2MSampler', 'params': {'num_steps': 50, 'verbose': True, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 3.0}}, 'guider_config': {'target': 'sgm.modules.diffusionmodules.guiders.DynamicCFG', 'params': {'scale': 6, 'exp': 5, 'num_steps': 50}}}}}}, {'args': {'checkpoint_activations': True, 'model_parallel_size': 1, 'experiment_name': 'lora-disney', 'mode': 'finetune', 'load': '/mnt/ceph/develop/jiawei/model_checkpoint/CogVideoX-2b-sat/transformer', 'no_load_rng': True, 'train_iters': 1000, 'eval_iters': 1, 'eval_interval': 100, 'eval_batch_size': 1, 'save': '/mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora', 'save_interval': 500, 'log_interval': 20, 'train_data': ['/mnt/ceph/develop/jiawei/lora_dataset/cogvideo_lora_dataset'], 'valid_data': ['/mnt/ceph/develop/jiawei/lora_dataset/cogvideo_lora_dataset'], 'split': '1,0,0', 'num_workers': 8, 'force_train': True, 'only_log_video_latents': False, 'wandb': True, 'wandb_project_name': 'cogv_2b_lora_base_001', 'summary_dir': '/mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora'}, 'data': {'target': 'data_video.SFTDataset', 'params': {'video_size': [480, 720], 'fps': 8, 'max_num_frames': 49, 'skip_frms_num': 3.0}}, 'deepspeed': {'train_micro_batch_size_per_gpu': 2, 'gradient_accumulation_steps': 1, 'steps_per_print': 50, 'gradient_clipping': 0.1, 'zero_optimization': {'stage': 2, 'cpu_offload': False, 'contiguous_gradients': False, 'overlap_comm': True, 'reduce_scatter': True, 'reduce_bucket_size': 1000000000, 'allgather_bucket_size': 1000000000, 'load_from_fp32_weights': False}, 'zero_allow_untested_optimizer': True, 'bf16': {'enabled': False}, 'fp16': {'enabled': True}, 'loss_scale': 0, 'loss_scale_window': 400, 'hysteresis': 2, 'min_loss_scale': 1, 'optimizer': {'type': 'sat.ops.FusedEmaAdam', 'params': {'lr': '1E-3', 'betas': [0.9, 0.95], 'eps': '1e-8', 'weight_decay': '1e-4'}}, 'activation_checkpointing': {'partition_activations': False, 'contiguous_memory_optimization': False}, 'wall_clock_breakdown': False}}]
[2024-09-09 14:22:59,057] [INFO] [RANK 0]   do_train ..................... True
[2024-09-09 14:22:59,057] [INFO] [RANK 0]   val_last_shape ............... []
[2024-09-09 14:22:59,057] [INFO] [RANK 0]   val_drop_number .............. 0
[2024-09-09 14:22:59,057] [INFO] [RANK 0]   do_valid ..................... True
[2024-09-09 14:22:59,057] [INFO] [RANK 0]   do_test ...................... False
[2024-09-09 14:22:59,057] [INFO] [RANK 0]   iteration .................... 0
wandb: Currently logged in as: dmeck. Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.17.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora/wandb/run-20240909_142302-11988cxh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lora-disney-09-09-14-22
wandb: â­ï¸ View project at https://wandb.ai/dmeck/cogv_2b_lora_base_001
wandb: ðŸš€ View run at https://wandb.ai/dmeck/cogv_2b_lora_base_001/runs/11988cxh
[2024-09-09 14:23:38,008] [INFO] [checkpointing.py:541:forward] Activation Checkpointing Information
[2024-09-09 14:23:38,009] [INFO] [checkpointing.py:542:forward] ----Partition Activations False, CPU CHECKPOINTING False
[2024-09-09 14:23:38,013] [INFO] [checkpointing.py:543:forward] ----contiguous Memory Checkpointing False with None total layers
[2024-09-09 14:23:38,013] [INFO] [checkpointing.py:545:forward] ----Synchronization False
[2024-09-09 14:23:38,013] [INFO] [checkpointing.py:546:forward] ----Profiling time in checkpointing False
[2024-09-09 14:23:44,512] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4294967296, reducing to 2147483648
[2024-09-09 14:23:59,457] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2147483648, reducing to 1073741824
[2024-09-09 14:24:14,764] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1073741824, reducing to 536870912
[2024-09-09 14:24:45,518] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 536870912, reducing to 268435456
[2024-09-09 14:25:15,967] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 268435456, reducing to 134217728
[2024-09-09 14:27:32,283] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 134217728, reducing to 67108864
[2024-09-09 14:28:33,074] [INFO] [RANK 0]  iteration       20/    1000 | elapsed time per iteration (ms): 16206.2 | learning rate 5.000E-05 | total loss 2.303818E-01 | loss 2.303818E-01 | loss scale 67108864.0 |speed 7.40 samples/(min*GPU)
[2024-09-09 14:28:33,076] [INFO] [RANK 0] after 20 iterations memory (MB) | allocated: 13975.08984375 | max allocated: 64454.34912109375 | cached: 22772.0 | max cached: 77966.0
[2024-09-09 14:28:33,077] [INFO] [RANK 0] time (ms) | forward: 11315.01 | backward: 4865.35 | allreduce: 0.00 | optimizer: 24.70 | data loader: 186.48
[2024-09-09 14:30:34,298] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 67108864, reducing to 33554432
[2024-09-09 14:33:36,075] [INFO] [RANK 0]  iteration       40/    1000 | elapsed time per iteration (ms): 15150.0 | learning rate 5.000E-05 | total loss 1.918417E-01 | loss 1.918417E-01 | loss scale 33554432.0 |speed 7.92 samples/(min*GPU)
[2024-09-09 14:33:36,076] [INFO] [RANK 0] time (ms) | forward: 10251.75 | backward: 4869.25 | allreduce: 0.00 | optimizer: 27.80 | data loader: 0.33
[2024-09-09 14:36:07,718] [INFO] [logging.py:96:log_dist] [Rank 0] step=50, skipped=7, lr=[5e-05], mom=[[0.9, 0.95]]
[2024-09-09 14:37:54,221] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 33554432, reducing to 16777216
[2024-09-09 14:38:39,574] [INFO] [RANK 0]  iteration       60/    1000 | elapsed time per iteration (ms): 15175.0 | learning rate 5.000E-05 | total loss 1.934054E-01 | loss 1.934054E-01 | loss scale 16777216.0 |speed 7.91 samples/(min*GPU)
[2024-09-09 14:38:39,576] [INFO] [RANK 0] time (ms) | forward: 10264.56 | backward: 4881.17 | allreduce: 0.00 | optimizer: 27.95 | data loader: 0.30
[2024-09-09 14:40:41,872] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16777216, reducing to 8388608
[2024-09-09 14:43:43,068] [INFO] [RANK 0]  iteration       80/    1000 | elapsed time per iteration (ms): 15174.7 | learning rate 5.000E-05 | total loss 2.473795E-01 | loss 2.473795E-01 | loss scale 8388608.0 |speed 7.91 samples/(min*GPU)
[2024-09-09 14:43:43,069] [INFO] [RANK 0] time (ms) | forward: 10276.49 | backward: 4869.85 | allreduce: 0.00 | optimizer: 27.08 | data loader: 0.28
[2024-09-09 14:48:44,813] [INFO] [logging.py:96:log_dist] [Rank 0] step=100, skipped=9, lr=[5e-05], mom=[[0.9, 0.95]]
[2024-09-09 14:48:44,814] [INFO] [RANK 0]  iteration      100/    1000 | elapsed time per iteration (ms): 15087.3 | learning rate 5.000E-05 | total loss 2.256734E-01 | loss 2.256734E-01 | loss scale 8388608.0 |speed 7.95 samples/(min*GPU)
[2024-09-09 14:48:44,815] [INFO] [RANK 0] time (ms) | forward: 10189.74 | backward: 4868.50 | allreduce: 0.00 | optimizer: 27.75 | data loader: 0.31
/mnt/ceph/develop/jiawei/CogVideo/sat/train_video.py:67: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at ../torch/csrc/autograd/init.cpp:733.)
  "dtype": torch.get_autocast_gpu_dtype(),
/mnt/ceph/develop/jiawei/CogVideo/sat/train_video.py:70: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.no_grad(), torch.cuda.amp.autocast(**gpu_autocast_kwargs):
##############################  Sampling setting  ##############################
Sampler: VPSDEDPMPP2MSampler
Discretization: ZeroSNRDDPMDiscretization
Guider: DynamicCFG
Sampling with VPSDEDPMPP2MSampler for 51 steps:   0%|          | 0/51 [00:00<?, ?it/s]Sampling with VPSDEDPMPP2MSampler for 51 steps:   2%|â–         | 1/51 [00:01<01:20,  1.61s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:   4%|â–         | 2/51 [00:03<01:21,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:   6%|â–Œ         | 3/51 [00:04<01:19,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:   8%|â–Š         | 4/51 [00:06<01:17,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  10%|â–‰         | 5/51 [00:08<01:16,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  12%|â–ˆâ–        | 6/51 [00:09<01:14,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  14%|â–ˆâ–Ž        | 7/51 [00:11<01:12,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  16%|â–ˆâ–Œ        | 8/51 [00:13<01:11,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  18%|â–ˆâ–Š        | 9/51 [00:14<01:09,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  20%|â–ˆâ–‰        | 10/51 [00:16<01:07,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  22%|â–ˆâ–ˆâ–       | 11/51 [00:18<01:06,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  24%|â–ˆâ–ˆâ–Ž       | 12/51 [00:19<01:04,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  25%|â–ˆâ–ˆâ–Œ       | 13/51 [00:21<01:02,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  27%|â–ˆâ–ˆâ–‹       | 14/51 [00:23<01:01,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  29%|â–ˆâ–ˆâ–‰       | 15/51 [00:24<00:59,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  31%|â–ˆâ–ˆâ–ˆâ–      | 16/51 [00:26<00:57,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 17/51 [00:28<00:56,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 18/51 [00:29<00:54,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 19/51 [00:31<00:53,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 20/51 [00:33<00:51,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 21/51 [00:34<00:49,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 22/51 [00:36<00:48,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 23/51 [00:38<00:46,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 24/51 [00:39<00:44,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 25/51 [00:41<00:43,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 26/51 [00:43<00:41,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 27/51 [00:44<00:39,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 28/51 [00:46<00:38,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 29/51 [00:48<00:36,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 30/51 [00:49<00:34,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 31/51 [00:51<00:33,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 32/51 [00:53<00:31,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33/51 [00:54<00:29,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 34/51 [00:56<00:28,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 35/51 [00:57<00:26,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 36/51 [00:59<00:24,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 37/51 [01:01<00:23,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/51 [01:02<00:21,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 39/51 [01:04<00:19,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 40/51 [01:06<00:18,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 41/51 [01:07<00:16,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 42/51 [01:09<00:14,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 43/51 [01:11<00:13,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 44/51 [01:12<00:11,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 45/51 [01:14<00:09,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 46/51 [01:16<00:08,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 47/51 [01:17<00:06,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 48/51 [01:19<00:04,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 49/51 [01:21<00:03,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 50/51 [01:22<00:01,  1.65s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 50/51 [01:22<00:01,  1.66s/it]
[2024-09-09 14:50:41,941] [INFO] [RANK 0] ----------------------------------------------------------------------------------------------------
[2024-09-09 14:50:41,941] [INFO] [RANK 0] -----------------------------------------------------------------------------------------------
[2024-09-09 14:50:41,947] [INFO] [RANK 0]  validation loss at iteration 100 | loss: 3.070096E-01 | PPL: 1.359354E+00 loss 3.070096E-01 |
[2024-09-09 14:50:41,947] [INFO] [RANK 0] -----------------------------------------------------------------------------------------------
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 100 is less than current step: 101. Dropping entry: {'Train/valid_ppl': 1.359354028075769, 'Train/valid_loss': 0.30700960755348206, '_timestamp': 1725864641.9510176}).
[2024-09-09 14:54:24,941] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8388608, reducing to 4194304
[2024-09-09 14:55:40,146] [INFO] [RANK 0]  iteration      120/    1000 | elapsed time per iteration (ms): 20766.6 | learning rate 8.990E-04 | total loss 2.551147E-01 | loss 2.551146E-01 | loss scale 4194304.0 |speed 5.78 samples/(min*GPU)
[2024-09-09 14:55:40,147] [INFO] [RANK 0] time (ms) | forward: 10015.41 | backward: 4866.08 | allreduce: 0.00 | optimizer: 27.16 | data loader: 0.31
[2024-09-09 15:00:41,857] [INFO] [RANK 0]  iteration      140/    1000 | elapsed time per iteration (ms): 15085.6 | learning rate 8.790E-04 | total loss 2.107409E-01 | loss 2.107409E-01 | loss scale 4194304.0 |speed 7.95 samples/(min*GPU)
[2024-09-09 15:00:41,859] [INFO] [RANK 0] time (ms) | forward: 10188.85 | backward: 4867.89 | allreduce: 0.00 | optimizer: 27.64 | data loader: 0.32
[2024-09-09 15:03:12,645] [INFO] [logging.py:96:log_dist] [Rank 0] step=150, skipped=10, lr=[0.00087], mom=[[0.9, 0.95]]
[2024-09-09 15:05:43,450] [INFO] [RANK 0]  iteration      160/    1000 | elapsed time per iteration (ms): 15079.6 | learning rate 8.590E-04 | total loss 2.307646E-01 | loss 2.307646E-01 | loss scale 4194304.0 |speed 7.96 samples/(min*GPU)
[2024-09-09 15:05:43,452] [INFO] [RANK 0] time (ms) | forward: 10183.78 | backward: 4866.96 | allreduce: 0.00 | optimizer: 27.62 | data loader: 0.28
[2024-09-09 15:10:44,741] [INFO] [RANK 0]  iteration      180/    1000 | elapsed time per iteration (ms): 15064.5 | learning rate 8.390E-04 | total loss 2.082209E-01 | loss 2.082209E-01 | loss scale 4194304.0 |speed 7.97 samples/(min*GPU)
[2024-09-09 15:10:44,742] [INFO] [RANK 0] time (ms) | forward: 10170.30 | backward: 4865.36 | allreduce: 0.00 | optimizer: 27.72 | data loader: 0.28
[2024-09-09 15:15:46,704] [INFO] [logging.py:96:log_dist] [Rank 0] step=200, skipped=10, lr=[0.00082], mom=[[0.9, 0.95]]
[2024-09-09 15:15:46,705] [INFO] [RANK 0]  iteration      200/    1000 | elapsed time per iteration (ms): 15098.2 | learning rate 8.190E-04 | total loss 2.151483E-01 | loss 2.151483E-01 | loss scale 4194304.0 |speed 7.95 samples/(min*GPU)
[2024-09-09 15:15:46,706] [INFO] [RANK 0] time (ms) | forward: 10202.13 | backward: 4867.30 | allreduce: 0.00 | optimizer: 27.62 | data loader: 0.32
##############################  Sampling setting  ##############################
Sampler: VPSDEDPMPP2MSampler
Discretization: ZeroSNRDDPMDiscretization
Guider: DynamicCFG
Sampling with VPSDEDPMPP2MSampler for 51 steps:   0%|          | 0/51 [00:00<?, ?it/s]Sampling with VPSDEDPMPP2MSampler for 51 steps:   2%|â–         | 1/51 [00:01<01:20,  1.61s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:   4%|â–         | 2/51 [00:03<01:21,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:   6%|â–Œ         | 3/51 [00:04<01:19,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:   8%|â–Š         | 4/51 [00:06<01:17,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  10%|â–‰         | 5/51 [00:08<01:16,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  12%|â–ˆâ–        | 6/51 [00:09<01:14,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  14%|â–ˆâ–Ž        | 7/51 [00:11<01:12,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  16%|â–ˆâ–Œ        | 8/51 [00:13<01:11,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  18%|â–ˆâ–Š        | 9/51 [00:14<01:09,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  20%|â–ˆâ–‰        | 10/51 [00:16<01:07,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  22%|â–ˆâ–ˆâ–       | 11/51 [00:18<01:06,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  24%|â–ˆâ–ˆâ–Ž       | 12/51 [00:19<01:04,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  25%|â–ˆâ–ˆâ–Œ       | 13/51 [00:21<01:02,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  27%|â–ˆâ–ˆâ–‹       | 14/51 [00:23<01:01,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  29%|â–ˆâ–ˆâ–‰       | 15/51 [00:24<00:59,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  31%|â–ˆâ–ˆâ–ˆâ–      | 16/51 [00:26<00:58,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 17/51 [00:28<00:56,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 18/51 [00:29<00:54,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 19/51 [00:31<00:53,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 20/51 [00:33<00:51,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 21/51 [00:34<00:49,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 22/51 [00:36<00:48,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 23/51 [00:38<00:46,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 24/51 [00:39<00:44,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 25/51 [00:41<00:43,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 26/51 [00:43<00:41,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 27/51 [00:44<00:39,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 28/51 [00:46<00:38,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 29/51 [00:48<00:36,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 30/51 [00:49<00:34,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 31/51 [00:51<00:33,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 32/51 [00:53<00:31,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33/51 [00:54<00:29,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 34/51 [00:56<00:28,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 35/51 [00:57<00:26,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 36/51 [00:59<00:24,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 37/51 [01:01<00:23,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/51 [01:02<00:21,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 39/51 [01:04<00:19,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 40/51 [01:06<00:18,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 41/51 [01:07<00:16,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 42/51 [01:09<00:14,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 43/51 [01:11<00:13,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 44/51 [01:12<00:11,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 45/51 [01:14<00:09,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 46/51 [01:16<00:08,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 47/51 [01:17<00:06,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 48/51 [01:19<00:04,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 49/51 [01:21<00:03,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 50/51 [01:22<00:01,  1.64s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 50/51 [01:22<00:01,  1.66s/it]
[2024-09-09 15:17:29,136] [INFO] [RANK 0] ----------------------------------------------------------------------------------------------------
[2024-09-09 15:17:29,136] [INFO] [RANK 0] -----------------------------------------------------------------------------------------------
[2024-09-09 15:17:29,139] [INFO] [RANK 0]  validation loss at iteration 200 | loss: 1.767411E-01 | PPL: 1.193322E+00 loss 1.767411E-01 |
[2024-09-09 15:17:29,139] [INFO] [RANK 0] -----------------------------------------------------------------------------------------------
wandb: WARNING (User provided step: 200 is less than current step: 201. Dropping entry: {'Train/valid_ppl': 1.1933221475040683, 'Train/valid_loss': 0.17674113810062408, '_timestamp': 1725866249.1397157}).
[2024-09-09 15:22:27,559] [INFO] [RANK 0]  iteration      220/    1000 | elapsed time per iteration (ms): 20042.7 | learning rate 7.990E-04 | total loss 2.217396E-01 | loss 2.217396E-01 | loss scale 4194304.0 |speed 5.99 samples/(min*GPU)
[2024-09-09 15:22:27,560] [INFO] [RANK 0] time (ms) | forward: 10023.42 | backward: 4868.92 | allreduce: 0.00 | optimizer: 27.54 | data loader: 0.31
[2024-09-09 15:27:29,632] [INFO] [RANK 0]  iteration      240/    1000 | elapsed time per iteration (ms): 15103.7 | learning rate 7.790E-04 | total loss 2.045119E-01 | loss 2.045119E-01 | loss scale 4194304.0 |speed 7.95 samples/(min*GPU)
[2024-09-09 15:27:29,633] [INFO] [RANK 0] time (ms) | forward: 10207.82 | backward: 4867.11 | allreduce: 0.00 | optimizer: 27.56 | data loader: 0.28
[2024-09-09 15:30:00,460] [INFO] [logging.py:96:log_dist] [Rank 0] step=250, skipped=10, lr=[0.0007700000000000001], mom=[[0.9, 0.95]]
[2024-09-09 15:32:31,413] [INFO] [RANK 0]  iteration      260/    1000 | elapsed time per iteration (ms): 15089.0 | learning rate 7.590E-04 | total loss 2.337183E-01 | loss 2.337183E-01 | loss scale 4194304.0 |speed 7.95 samples/(min*GPU)
[2024-09-09 15:32:31,414] [INFO] [RANK 0] time (ms) | forward: 10191.10 | backward: 4869.25 | allreduce: 0.00 | optimizer: 27.56 | data loader: 0.28
[2024-09-09 15:37:33,359] [INFO] [RANK 0]  iteration      280/    1000 | elapsed time per iteration (ms): 15097.3 | learning rate 7.390E-04 | total loss 2.250969E-01 | loss 2.250969E-01 | loss scale 4194304.0 |speed 7.95 samples/(min*GPU)
[2024-09-09 15:37:33,361] [INFO] [RANK 0] time (ms) | forward: 10200.75 | backward: 4867.89 | allreduce: 0.00 | optimizer: 27.56 | data loader: 0.28
[2024-09-09 15:42:35,221] [INFO] [logging.py:96:log_dist] [Rank 0] step=300, skipped=10, lr=[0.0007199999999999999], mom=[[0.9, 0.95]]
[2024-09-09 15:42:35,222] [INFO] [RANK 0]  iteration      300/    1000 | elapsed time per iteration (ms): 15093.1 | learning rate 7.190E-04 | total loss 2.065954E-01 | loss 2.065954E-01 | loss scale 4194304.0 |speed 7.95 samples/(min*GPU)
[2024-09-09 15:42:35,223] [INFO] [RANK 0] time (ms) | forward: 10196.91 | backward: 4867.33 | allreduce: 0.00 | optimizer: 27.61 | data loader: 0.27
##############################  Sampling setting  ##############################
Sampler: VPSDEDPMPP2MSampler
Discretization: ZeroSNRDDPMDiscretization
Guider: DynamicCFG
Sampling with VPSDEDPMPP2MSampler for 51 steps:   0%|          | 0/51 [00:00<?, ?it/s]Sampling with VPSDEDPMPP2MSampler for 51 steps:   2%|â–         | 1/51 [00:01<01:20,  1.61s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:   4%|â–         | 2/51 [00:03<01:21,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:   6%|â–Œ         | 3/51 [00:04<01:19,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:   8%|â–Š         | 4/51 [00:06<01:17,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  10%|â–‰         | 5/51 [00:08<01:16,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  12%|â–ˆâ–        | 6/51 [00:09<01:14,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  14%|â–ˆâ–Ž        | 7/51 [00:11<01:12,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  16%|â–ˆâ–Œ        | 8/51 [00:13<01:11,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  18%|â–ˆâ–Š        | 9/51 [00:14<01:09,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  20%|â–ˆâ–‰        | 10/51 [00:16<01:07,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  22%|â–ˆâ–ˆâ–       | 11/51 [00:18<01:06,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  24%|â–ˆâ–ˆâ–Ž       | 12/51 [00:19<01:04,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  25%|â–ˆâ–ˆâ–Œ       | 13/51 [00:21<01:02,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  27%|â–ˆâ–ˆâ–‹       | 14/51 [00:23<01:01,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  29%|â–ˆâ–ˆâ–‰       | 15/51 [00:24<00:59,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  31%|â–ˆâ–ˆâ–ˆâ–      | 16/51 [00:26<00:57,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 17/51 [00:28<00:56,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 18/51 [00:29<00:54,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 19/51 [00:31<00:53,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 20/51 [00:33<00:51,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 21/51 [00:34<00:49,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 22/51 [00:36<00:48,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 23/51 [00:38<00:46,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 24/51 [00:39<00:44,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 25/51 [00:41<00:43,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 26/51 [00:43<00:41,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 27/51 [00:44<00:39,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 28/51 [00:46<00:38,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 29/51 [00:48<00:36,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 30/51 [00:49<00:34,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 31/51 [00:51<00:33,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 32/51 [00:53<00:31,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33/51 [00:54<00:29,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 34/51 [00:56<00:28,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 35/51 [00:58<00:26,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 36/51 [00:59<00:24,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 37/51 [01:01<00:23,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/51 [01:02<00:21,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 39/51 [01:04<00:19,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 40/51 [01:06<00:18,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 41/51 [01:07<00:16,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 42/51 [01:09<00:14,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 43/51 [01:11<00:13,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 44/51 [01:12<00:11,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 45/51 [01:14<00:09,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 46/51 [01:16<00:08,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 47/51 [01:17<00:06,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 48/51 [01:19<00:04,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 49/51 [01:21<00:03,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 50/51 [01:22<00:01,  1.65s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 50/51 [01:22<00:01,  1.66s/it]
[2024-09-09 15:44:17,875] [INFO] [RANK 0] ----------------------------------------------------------------------------------------------------
[2024-09-09 15:44:17,876] [INFO] [RANK 0] -----------------------------------------------------------------------------------------------
[2024-09-09 15:44:17,879] [INFO] [RANK 0]  validation loss at iteration 300 | loss: 2.258870E-01 | PPL: 1.253434E+00 loss 2.258870E-01 |
[2024-09-09 15:44:17,879] [INFO] [RANK 0] -----------------------------------------------------------------------------------------------
wandb: WARNING (User provided step: 300 is less than current step: 301. Dropping entry: {'Train/valid_ppl': 1.2534340759669076, 'Train/valid_loss': 0.22588704526424408, '_timestamp': 1725867857.8800726}).
[2024-09-09 15:49:16,661] [INFO] [RANK 0]  iteration      320/    1000 | elapsed time per iteration (ms): 20072.0 | learning rate 6.990E-04 | total loss 2.377671E-01 | loss 2.377671E-01 | loss scale 4194304.0 |speed 5.98 samples/(min*GPU)
[2024-09-09 15:49:16,662] [INFO] [RANK 0] time (ms) | forward: 10043.76 | backward: 4866.61 | allreduce: 0.00 | optimizer: 27.58 | data loader: 0.31
[2024-09-09 15:54:19,303] [INFO] [RANK 0]  iteration      340/    1000 | elapsed time per iteration (ms): 15132.1 | learning rate 6.790E-04 | total loss 2.055446E-01 | loss 2.055446E-01 | loss scale 4194304.0 |speed 7.93 samples/(min*GPU)
[2024-09-09 15:54:19,304] [INFO] [RANK 0] time (ms) | forward: 10234.46 | backward: 4868.43 | allreduce: 0.00 | optimizer: 28.07 | data loader: 0.28
[2024-09-09 15:56:51,586] [INFO] [logging.py:96:log_dist] [Rank 0] step=350, skipped=10, lr=[0.00067], mom=[[0.9, 0.95]]
[2024-09-09 15:59:23,919] [INFO] [RANK 0]  iteration      360/    1000 | elapsed time per iteration (ms): 15230.8 | learning rate 6.590E-04 | total loss 2.158377E-01 | loss 2.158377E-01 | loss scale 4194304.0 |speed 7.88 samples/(min*GPU)
[2024-09-09 15:59:23,920] [INFO] [RANK 0] time (ms) | forward: 10334.17 | backward: 4866.66 | allreduce: 0.00 | optimizer: 28.80 | data loader: 0.32
[2024-09-09 16:04:27,202] [INFO] [RANK 0]  iteration      380/    1000 | elapsed time per iteration (ms): 15164.1 | learning rate 6.390E-04 | total loss 2.162380E-01 | loss 2.162379E-01 | loss scale 4194304.0 |speed 7.91 samples/(min*GPU)
[2024-09-09 16:04:27,203] [INFO] [RANK 0] time (ms) | forward: 10267.19 | backward: 4867.43 | allreduce: 0.00 | optimizer: 28.36 | data loader: 0.29
[2024-09-09 16:09:30,881] [INFO] [logging.py:96:log_dist] [Rank 0] step=400, skipped=10, lr=[0.00062], mom=[[0.9, 0.95]]
[2024-09-09 16:09:30,882] [INFO] [RANK 0]  iteration      400/    1000 | elapsed time per iteration (ms): 15184.0 | learning rate 6.190E-04 | total loss 2.322181E-01 | loss 2.322181E-01 | loss scale 4194304.0 |speed 7.90 samples/(min*GPU)
[2024-09-09 16:09:30,883] [INFO] [RANK 0] time (ms) | forward: 10286.30 | backward: 4868.83 | allreduce: 0.00 | optimizer: 27.69 | data loader: 0.32
##############################  Sampling setting  ##############################
Sampler: VPSDEDPMPP2MSampler
Discretization: ZeroSNRDDPMDiscretization
Guider: DynamicCFG
Sampling with VPSDEDPMPP2MSampler for 51 steps:   0%|          | 0/51 [00:00<?, ?it/s]Sampling with VPSDEDPMPP2MSampler for 51 steps:   2%|â–         | 1/51 [00:01<01:20,  1.61s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:   4%|â–         | 2/51 [00:03<01:21,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:   6%|â–Œ         | 3/51 [00:04<01:19,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:   8%|â–Š         | 4/51 [00:06<01:17,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  10%|â–‰         | 5/51 [00:08<01:16,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  12%|â–ˆâ–        | 6/51 [00:09<01:14,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  14%|â–ˆâ–Ž        | 7/51 [00:11<01:12,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  16%|â–ˆâ–Œ        | 8/51 [00:13<01:11,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  18%|â–ˆâ–Š        | 9/51 [00:14<01:09,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  20%|â–ˆâ–‰        | 10/51 [00:16<01:07,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  22%|â–ˆâ–ˆâ–       | 11/51 [00:18<01:06,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  24%|â–ˆâ–ˆâ–Ž       | 12/51 [00:19<01:04,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  25%|â–ˆâ–ˆâ–Œ       | 13/51 [00:21<01:02,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  27%|â–ˆâ–ˆâ–‹       | 14/51 [00:23<01:01,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  29%|â–ˆâ–ˆâ–‰       | 15/51 [00:24<00:59,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  31%|â–ˆâ–ˆâ–ˆâ–      | 16/51 [00:26<00:58,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 17/51 [00:28<00:56,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 18/51 [00:29<00:54,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 19/51 [00:31<00:53,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 20/51 [00:33<00:51,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 21/51 [00:34<00:49,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 22/51 [00:36<00:48,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 23/51 [00:38<00:46,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 24/51 [00:39<00:44,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 25/51 [00:41<00:43,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 26/51 [00:43<00:41,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 27/51 [00:44<00:39,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 28/51 [00:46<00:38,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 29/51 [00:48<00:36,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 30/51 [00:49<00:34,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 31/51 [00:51<00:33,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 32/51 [00:53<00:31,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33/51 [00:54<00:29,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 34/51 [00:56<00:28,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 35/51 [00:58<00:26,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 36/51 [00:59<00:24,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 37/51 [01:01<00:23,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/51 [01:02<00:21,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 39/51 [01:04<00:19,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 40/51 [01:06<00:18,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 41/51 [01:07<00:16,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 42/51 [01:09<00:14,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 43/51 [01:11<00:13,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 44/51 [01:12<00:11,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 45/51 [01:14<00:09,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 46/51 [01:16<00:08,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 47/51 [01:17<00:06,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 48/51 [01:19<00:04,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 49/51 [01:21<00:03,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 50/51 [01:22<00:01,  1.64s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 50/51 [01:22<00:01,  1.66s/it]
[2024-09-09 16:11:13,588] [INFO] [RANK 0] ----------------------------------------------------------------------------------------------------
[2024-09-09 16:11:13,589] [INFO] [RANK 0] -----------------------------------------------------------------------------------------------
[2024-09-09 16:11:13,595] [INFO] [RANK 0]  validation loss at iteration 400 | loss: 1.267249E-01 | PPL: 1.135105E+00 loss 1.267249E-01 |
[2024-09-09 16:11:13,595] [INFO] [RANK 0] -----------------------------------------------------------------------------------------------
wandb: WARNING (User provided step: 400 is less than current step: 401. Dropping entry: {'Train/valid_ppl': 1.1351047400039747, 'Train/valid_loss': 0.12672492861747742, '_timestamp': 1725869473.5961347}).
[2024-09-09 16:16:13,297] [INFO] [RANK 0]  iteration      420/    1000 | elapsed time per iteration (ms): 20120.8 | learning rate 5.990E-04 | total loss 2.317476E-01 | loss 2.317475E-01 | loss scale 4194304.0 |speed 5.96 samples/(min*GPU)
[2024-09-09 16:16:13,298] [INFO] [RANK 0] time (ms) | forward: 10086.79 | backward: 4868.28 | allreduce: 0.00 | optimizer: 28.86 | data loader: 0.36
[2024-09-09 16:21:17,088] [INFO] [RANK 0]  iteration      440/    1000 | elapsed time per iteration (ms): 15189.5 | learning rate 5.790E-04 | total loss 2.254256E-01 | loss 2.254256E-01 | loss scale 4194304.0 |speed 7.90 samples/(min*GPU)
[2024-09-09 16:21:17,090] [INFO] [RANK 0] time (ms) | forward: 10291.57 | backward: 4868.38 | allreduce: 0.00 | optimizer: 28.45 | data loader: 0.30
[2024-09-09 16:23:49,134] [INFO] [logging.py:96:log_dist] [Rank 0] step=450, skipped=10, lr=[0.00057], mom=[[0.9, 0.95]]
[2024-09-09 16:24:35,459] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4194304, reducing to 2097152
[2024-09-09 16:26:21,511] [INFO] [RANK 0]  iteration      460/    1000 | elapsed time per iteration (ms): 15221.1 | learning rate 5.600E-04 | total loss 2.252915E-01 | loss 2.252915E-01 | loss scale 2097152.0 |speed 7.88 samples/(min*GPU)
[2024-09-09 16:26:21,512] [INFO] [RANK 0] time (ms) | forward: 10322.69 | backward: 4868.70 | allreduce: 0.00 | optimizer: 28.39 | data loader: 0.30
[2024-09-09 16:31:24,639] [INFO] [RANK 0]  iteration      480/    1000 | elapsed time per iteration (ms): 15156.4 | learning rate 5.400E-04 | total loss 2.259105E-01 | loss 2.259105E-01 | loss scale 2097152.0 |speed 7.92 samples/(min*GPU)
[2024-09-09 16:31:24,640] [INFO] [RANK 0] time (ms) | forward: 10257.98 | backward: 4869.33 | allreduce: 0.00 | optimizer: 27.94 | data loader: 0.32
[2024-09-09 16:36:28,286] [INFO] [logging.py:96:log_dist] [Rank 0] step=500, skipped=11, lr=[0.000521], mom=[[0.9, 0.95]]
[2024-09-09 16:36:28,286] [INFO] [RANK 0]  iteration      500/    1000 | elapsed time per iteration (ms): 15182.4 | learning rate 5.200E-04 | total loss 2.274512E-01 | loss 2.274512E-01 | loss scale 2097152.0 |speed 7.90 samples/(min*GPU)
[2024-09-09 16:36:28,288] [INFO] [RANK 0] time (ms) | forward: 10282.57 | backward: 4869.66 | allreduce: 0.00 | optimizer: 28.97 | data loader: 0.31
[2024-09-09 16:36:28,290] [INFO] [RANK 0] Saving Model...
[2024-09-09 16:36:34,650] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora/lora-disney-09-09-14-22/500/mp_rank_00_model_states.pt
[2024-09-09 16:36:34,650] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora/lora-disney-09-09-14-22/500/mp_rank_00_model_states.pt...
[2024-09-09 16:37:29,983] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora/lora-disney-09-09-14-22/500/mp_rank_00_model_states.pt.
[2024-09-09 16:37:30,366] [INFO] [RANK 0] Saving Ema Model...
[2024-09-09 16:37:36,187] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora/lora-disney-09-09-14-22/500-ema/mp_rank_00_model_states.pt
[2024-09-09 16:37:36,188] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora/lora-disney-09-09-14-22/500-ema/mp_rank_00_model_states.pt...
[2024-09-09 16:38:31,020] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora/lora-disney-09-09-14-22/500-ema/mp_rank_00_model_states.pt.
##############################  Sampling setting  ##############################
Sampler: VPSDEDPMPP2MSampler
Discretization: ZeroSNRDDPMDiscretization
Guider: DynamicCFG
Sampling with VPSDEDPMPP2MSampler for 51 steps:   0%|          | 0/51 [00:00<?, ?it/s]Sampling with VPSDEDPMPP2MSampler for 51 steps:   2%|â–         | 1/51 [00:01<01:20,  1.61s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:   4%|â–         | 2/51 [00:03<01:21,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:   6%|â–Œ         | 3/51 [00:04<01:19,  1.65s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:   8%|â–Š         | 4/51 [00:06<01:17,  1.65s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  10%|â–‰         | 5/51 [00:08<01:16,  1.65s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  12%|â–ˆâ–        | 6/51 [00:09<01:14,  1.65s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  14%|â–ˆâ–Ž        | 7/51 [00:11<01:12,  1.65s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  16%|â–ˆâ–Œ        | 8/51 [00:13<01:11,  1.65s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  18%|â–ˆâ–Š        | 9/51 [00:14<01:09,  1.65s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  20%|â–ˆâ–‰        | 10/51 [00:16<01:07,  1.65s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  22%|â–ˆâ–ˆâ–       | 11/51 [00:18<01:06,  1.65s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  24%|â–ˆâ–ˆâ–Ž       | 12/51 [00:19<01:04,  1.65s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  25%|â–ˆâ–ˆâ–Œ       | 13/51 [00:21<01:02,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  27%|â–ˆâ–ˆâ–‹       | 14/51 [00:23<01:01,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  29%|â–ˆâ–ˆâ–‰       | 15/51 [00:24<00:59,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  31%|â–ˆâ–ˆâ–ˆâ–      | 16/51 [00:26<00:57,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 17/51 [00:28<00:56,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 18/51 [00:29<00:54,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 19/51 [00:31<00:53,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 20/51 [00:33<00:51,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 21/51 [00:34<00:49,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 22/51 [00:36<00:48,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 23/51 [00:38<00:46,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 24/51 [00:39<00:44,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 25/51 [00:41<00:43,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 26/51 [00:43<00:41,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 27/51 [00:44<00:39,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 28/51 [00:46<00:38,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 29/51 [00:48<00:36,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 30/51 [00:49<00:34,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 31/51 [00:51<00:33,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 32/51 [00:52<00:31,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33/51 [00:54<00:29,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 34/51 [00:56<00:28,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 35/51 [00:57<00:26,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 36/51 [00:59<00:24,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 37/51 [01:01<00:23,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/51 [01:02<00:21,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 39/51 [01:04<00:19,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 40/51 [01:06<00:18,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 41/51 [01:07<00:16,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 42/51 [01:09<00:14,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 43/51 [01:11<00:13,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 44/51 [01:12<00:11,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 45/51 [01:14<00:09,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 46/51 [01:16<00:08,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 47/51 [01:17<00:06,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 48/51 [01:19<00:04,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 49/51 [01:21<00:03,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 50/51 [01:22<00:01,  1.65s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 50/51 [01:22<00:01,  1.66s/it]
[2024-09-09 16:40:13,957] [INFO] [RANK 0] ----------------------------------------------------------------------------------------------------
[2024-09-09 16:40:13,957] [INFO] [RANK 0] -----------------------------------------------------------------------------------------------
[2024-09-09 16:40:13,960] [INFO] [RANK 0]  validation loss at iteration 500 | loss: 3.080253E-01 | PPL: 1.360735E+00 loss 3.080253E-01 |
[2024-09-09 16:40:13,960] [INFO] [RANK 0] -----------------------------------------------------------------------------------------------
wandb: WARNING (User provided step: 500 is less than current step: 501. Dropping entry: {'Train/valid_ppl': 1.3607354563448821, 'Train/valid_loss': 0.3080253303050995, '_timestamp': 1725871213.9614656}).
[2024-09-09 16:45:14,435] [INFO] [RANK 0]  iteration      520/    1000 | elapsed time per iteration (ms): 26307.4 | learning rate 5.000E-04 | total loss 2.011513E-01 | loss 2.011512E-01 | loss scale 2097152.0 |speed 4.56 samples/(min*GPU)
[2024-09-09 16:45:14,437] [INFO] [RANK 0] time (ms) | forward: 10126.11 | backward: 4868.14 | allreduce: 0.00 | optimizer: 28.32 | data loader: 0.37
[2024-09-09 16:50:18,022] [INFO] [RANK 0]  iteration      540/    1000 | elapsed time per iteration (ms): 15179.3 | learning rate 4.800E-04 | total loss 2.261096E-01 | loss 2.261095E-01 | loss scale 2097152.0 |speed 7.91 samples/(min*GPU)
[2024-09-09 16:50:18,023] [INFO] [RANK 0] time (ms) | forward: 10282.44 | backward: 4868.08 | allreduce: 0.00 | optimizer: 27.53 | data loader: 0.29
[2024-09-09 16:52:49,323] [INFO] [logging.py:96:log_dist] [Rank 0] step=550, skipped=11, lr=[0.000471], mom=[[0.9, 0.95]]
[2024-09-09 16:55:21,111] [INFO] [RANK 0]  iteration      560/    1000 | elapsed time per iteration (ms): 15154.5 | learning rate 4.600E-04 | total loss 2.260857E-01 | loss 2.260858E-01 | loss scale 2097152.0 |speed 7.92 samples/(min*GPU)
[2024-09-09 16:55:21,113] [INFO] [RANK 0] time (ms) | forward: 10256.72 | backward: 4868.52 | allreduce: 0.00 | optimizer: 28.08 | data loader: 0.28
[2024-09-09 17:00:23,683] [INFO] [RANK 0]  iteration      580/    1000 | elapsed time per iteration (ms): 15128.6 | learning rate 4.400E-04 | total loss 2.223263E-01 | loss 2.223262E-01 | loss scale 2097152.0 |speed 7.93 samples/(min*GPU)
[2024-09-09 17:00:23,685] [INFO] [RANK 0] time (ms) | forward: 10229.75 | backward: 4869.72 | allreduce: 0.00 | optimizer: 27.69 | data loader: 0.31
[2024-09-09 17:05:26,923] [INFO] [logging.py:96:log_dist] [Rank 0] step=600, skipped=11, lr=[0.000421], mom=[[0.9, 0.95]]
[2024-09-09 17:05:26,924] [INFO] [RANK 0]  iteration      600/    1000 | elapsed time per iteration (ms): 15162.0 | learning rate 4.200E-04 | total loss 2.516405E-01 | loss 2.516405E-01 | loss scale 2097152.0 |speed 7.91 samples/(min*GPU)
[2024-09-09 17:05:26,925] [INFO] [RANK 0] time (ms) | forward: 10264.51 | backward: 4868.04 | allreduce: 0.00 | optimizer: 28.13 | data loader: 0.29
##############################  Sampling setting  ##############################
Sampler: VPSDEDPMPP2MSampler
Discretization: ZeroSNRDDPMDiscretization
Guider: DynamicCFG
Sampling with VPSDEDPMPP2MSampler for 51 steps:   0%|          | 0/51 [00:00<?, ?it/s]Sampling with VPSDEDPMPP2MSampler for 51 steps:   2%|â–         | 1/51 [00:01<01:20,  1.61s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:   4%|â–         | 2/51 [00:03<01:21,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:   6%|â–Œ         | 3/51 [00:04<01:19,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:   8%|â–Š         | 4/51 [00:06<01:17,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  10%|â–‰         | 5/51 [00:08<01:16,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  12%|â–ˆâ–        | 6/51 [00:09<01:14,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  14%|â–ˆâ–Ž        | 7/51 [00:11<01:12,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  16%|â–ˆâ–Œ        | 8/51 [00:13<01:11,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  18%|â–ˆâ–Š        | 9/51 [00:14<01:09,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  20%|â–ˆâ–‰        | 10/51 [00:16<01:07,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  22%|â–ˆâ–ˆâ–       | 11/51 [00:18<01:06,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  24%|â–ˆâ–ˆâ–Ž       | 12/51 [00:19<01:04,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  25%|â–ˆâ–ˆâ–Œ       | 13/51 [00:21<01:02,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  27%|â–ˆâ–ˆâ–‹       | 14/51 [00:23<01:01,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  29%|â–ˆâ–ˆâ–‰       | 15/51 [00:24<00:59,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  31%|â–ˆâ–ˆâ–ˆâ–      | 16/51 [00:26<00:57,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 17/51 [00:28<00:56,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 18/51 [00:29<00:54,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 19/51 [00:31<00:53,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 20/51 [00:33<00:51,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 21/51 [00:34<00:49,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 22/51 [00:36<00:48,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 23/51 [00:38<00:46,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 24/51 [00:39<00:44,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 25/51 [00:41<00:43,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 26/51 [00:43<00:41,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 27/51 [00:44<00:39,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 28/51 [00:46<00:38,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 29/51 [00:48<00:36,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 30/51 [00:49<00:34,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 31/51 [00:51<00:33,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 32/51 [00:53<00:31,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33/51 [00:54<00:29,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 34/51 [00:56<00:28,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 35/51 [00:57<00:26,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 36/51 [00:59<00:24,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 37/51 [01:01<00:23,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/51 [01:02<00:21,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 39/51 [01:04<00:19,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 40/51 [01:06<00:18,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 41/51 [01:07<00:16,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 42/51 [01:09<00:14,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 43/51 [01:11<00:13,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 44/51 [01:12<00:11,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 45/51 [01:14<00:09,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 46/51 [01:16<00:08,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 47/51 [01:17<00:06,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 48/51 [01:19<00:04,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 49/51 [01:21<00:03,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 50/51 [01:22<00:01,  1.64s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 50/51 [01:22<00:01,  1.66s/it]
[2024-09-09 17:07:09,424] [INFO] [RANK 0] ----------------------------------------------------------------------------------------------------
[2024-09-09 17:07:09,424] [INFO] [RANK 0] -----------------------------------------------------------------------------------------------
[2024-09-09 17:07:09,429] [INFO] [RANK 0]  validation loss at iteration 600 | loss: 1.433257E-01 | PPL: 1.154106E+00 loss 1.433257E-01 |
[2024-09-09 17:07:09,429] [INFO] [RANK 0] -----------------------------------------------------------------------------------------------
wandb: WARNING (User provided step: 600 is less than current step: 601. Dropping entry: {'Train/valid_ppl': 1.154105617030546, 'Train/valid_loss': 0.14332568645477295, '_timestamp': 1725872829.4296327}).
[2024-09-09 17:12:09,562] [INFO] [RANK 0]  iteration      620/    1000 | elapsed time per iteration (ms): 20131.9 | learning rate 4.000E-04 | total loss 2.010019E-01 | loss 2.010019E-01 | loss scale 2097152.0 |speed 5.96 samples/(min*GPU)
[2024-09-09 17:12:09,564] [INFO] [RANK 0] time (ms) | forward: 10108.44 | backward: 4869.31 | allreduce: 0.00 | optimizer: 27.81 | data loader: 0.32
[2024-09-09 17:17:13,727] [INFO] [RANK 0]  iteration      640/    1000 | elapsed time per iteration (ms): 15208.2 | learning rate 3.800E-04 | total loss 2.365720E-01 | loss 2.365720E-01 | loss scale 2097152.0 |speed 7.89 samples/(min*GPU)
[2024-09-09 17:17:13,729] [INFO] [RANK 0] time (ms) | forward: 10308.55 | backward: 4869.89 | allreduce: 0.00 | optimizer: 28.60 | data loader: 0.31
[2024-09-09 17:19:45,174] [INFO] [logging.py:96:log_dist] [Rank 0] step=650, skipped=11, lr=[0.000371], mom=[[0.9, 0.95]]
[2024-09-09 17:22:17,099] [INFO] [RANK 0]  iteration      660/    1000 | elapsed time per iteration (ms): 15168.6 | learning rate 3.600E-04 | total loss 2.153242E-01 | loss 2.153242E-01 | loss scale 2097152.0 |speed 7.91 samples/(min*GPU)
[2024-09-09 17:22:17,100] [INFO] [RANK 0] time (ms) | forward: 10273.09 | backward: 4866.22 | allreduce: 0.00 | optimizer: 28.01 | data loader: 0.30
[2024-09-09 17:27:21,498] [INFO] [RANK 0]  iteration      680/    1000 | elapsed time per iteration (ms): 15219.9 | learning rate 3.400E-04 | total loss 2.292674E-01 | loss 2.292673E-01 | loss scale 2097152.0 |speed 7.88 samples/(min*GPU)
[2024-09-09 17:27:21,499] [INFO] [RANK 0] time (ms) | forward: 10286.27 | backward: 4903.99 | allreduce: 0.00 | optimizer: 28.48 | data loader: 0.29
[2024-09-09 17:32:24,724] [INFO] [logging.py:96:log_dist] [Rank 0] step=700, skipped=11, lr=[0.000321], mom=[[0.9, 0.95]]
[2024-09-09 17:32:24,724] [INFO] [RANK 0]  iteration      700/    1000 | elapsed time per iteration (ms): 15161.3 | learning rate 3.200E-04 | total loss 2.414290E-01 | loss 2.414290E-01 | loss scale 2097152.0 |speed 7.91 samples/(min*GPU)
[2024-09-09 17:32:24,728] [INFO] [RANK 0] time (ms) | forward: 10256.29 | backward: 4875.76 | allreduce: 0.00 | optimizer: 28.12 | data loader: 0.31
##############################  Sampling setting  ##############################
Sampler: VPSDEDPMPP2MSampler
Discretization: ZeroSNRDDPMDiscretization
Guider: DynamicCFG
Sampling with VPSDEDPMPP2MSampler for 51 steps:   0%|          | 0/51 [00:00<?, ?it/s]Sampling with VPSDEDPMPP2MSampler for 51 steps:   2%|â–         | 1/51 [00:01<01:20,  1.61s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:   4%|â–         | 2/51 [00:03<01:21,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:   6%|â–Œ         | 3/51 [00:04<01:19,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:   8%|â–Š         | 4/51 [00:06<01:17,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  10%|â–‰         | 5/51 [00:08<01:16,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  12%|â–ˆâ–        | 6/51 [00:09<01:14,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  14%|â–ˆâ–Ž        | 7/51 [00:11<01:12,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  16%|â–ˆâ–Œ        | 8/51 [00:13<01:11,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  18%|â–ˆâ–Š        | 9/51 [00:14<01:09,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  20%|â–ˆâ–‰        | 10/51 [00:16<01:07,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  22%|â–ˆâ–ˆâ–       | 11/51 [00:18<01:06,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  24%|â–ˆâ–ˆâ–Ž       | 12/51 [00:19<01:04,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  25%|â–ˆâ–ˆâ–Œ       | 13/51 [00:21<01:02,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  27%|â–ˆâ–ˆâ–‹       | 14/51 [00:23<01:01,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  29%|â–ˆâ–ˆâ–‰       | 15/51 [00:24<00:59,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  31%|â–ˆâ–ˆâ–ˆâ–      | 16/51 [00:26<00:58,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 17/51 [00:28<00:56,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 18/51 [00:29<00:54,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 19/51 [00:31<00:53,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 20/51 [00:33<00:51,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 21/51 [00:34<00:49,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 22/51 [00:36<00:48,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 23/51 [00:38<00:46,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 24/51 [00:39<00:44,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 25/51 [00:41<00:43,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 26/51 [00:43<00:41,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 27/51 [00:44<00:39,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 28/51 [00:46<00:38,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 29/51 [00:48<00:36,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 30/51 [00:49<00:34,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 31/51 [00:51<00:33,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 32/51 [00:53<00:31,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33/51 [00:54<00:29,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 34/51 [00:56<00:28,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 35/51 [00:58<00:26,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 36/51 [00:59<00:24,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 37/51 [01:01<00:23,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/51 [01:02<00:21,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 39/51 [01:04<00:19,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 40/51 [01:06<00:18,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 41/51 [01:07<00:16,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 42/51 [01:09<00:14,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 43/51 [01:11<00:13,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 44/51 [01:12<00:11,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 45/51 [01:14<00:09,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 46/51 [01:16<00:08,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 47/51 [01:17<00:06,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 48/51 [01:19<00:04,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 49/51 [01:21<00:03,  1.66s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 50/51 [01:22<00:01,  1.65s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 50/51 [01:22<00:01,  1.66s/it]
[2024-09-09 17:34:07,848] [INFO] [RANK 0] ----------------------------------------------------------------------------------------------------
[2024-09-09 17:34:07,848] [INFO] [RANK 0] -----------------------------------------------------------------------------------------------
[2024-09-09 17:34:07,851] [INFO] [RANK 0]  validation loss at iteration 700 | loss: 1.503229E-01 | PPL: 1.162209E+00 loss 1.503229E-01 |
[2024-09-09 17:34:07,851] [INFO] [RANK 0] -----------------------------------------------------------------------------------------------
wandb: WARNING (User provided step: 700 is less than current step: 701. Dropping entry: {'Train/valid_ppl': 1.162209441358726, 'Train/valid_loss': 0.15032288432121277, '_timestamp': 1725874447.85193}).
[2024-09-09 17:39:07,293] [INFO] [RANK 0]  iteration      720/    1000 | elapsed time per iteration (ms): 20128.4 | learning rate 3.000E-04 | total loss 2.138434E-01 | loss 2.138434E-01 | loss scale 2097152.0 |speed 5.96 samples/(min*GPU)
[2024-09-09 17:39:07,294] [INFO] [RANK 0] time (ms) | forward: 10071.86 | backward: 4871.31 | allreduce: 0.00 | optimizer: 27.77 | data loader: 0.34
[2024-09-09 17:44:09,533] [INFO] [RANK 0]  iteration      740/    1000 | elapsed time per iteration (ms): 15112.0 | learning rate 2.800E-04 | total loss 2.304666E-01 | loss 2.304666E-01 | loss scale 2097152.0 |speed 7.94 samples/(min*GPU)
[2024-09-09 17:44:09,534] [INFO] [RANK 0] time (ms) | forward: 10212.61 | backward: 4869.51 | allreduce: 0.00 | optimizer: 28.60 | data loader: 0.31
[2024-09-09 17:46:41,494] [INFO] [logging.py:96:log_dist] [Rank 0] step=750, skipped=11, lr=[0.00027100000000000003], mom=[[0.9, 0.95]]
[2024-09-09 17:49:13,145] [INFO] [RANK 0]  iteration      760/    1000 | elapsed time per iteration (ms): 15180.6 | learning rate 2.600E-04 | total loss 1.960492E-01 | loss 1.960492E-01 | loss scale 2097152.0 |speed 7.90 samples/(min*GPU)
[2024-09-09 17:49:13,146] [INFO] [RANK 0] time (ms) | forward: 10284.29 | backward: 4866.78 | allreduce: 0.00 | optimizer: 28.37 | data loader: 0.29
E0909 17:51:02.700000 140705735886656 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: -9) local_rank: 0 (pid: 1859078) of binary: /mnt/ceph/develop/jiawei/conda_env/cogvideo/bin/python
Traceback (most recent call last):
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
========================================================
train_video.py FAILED
--------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
--------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-09-09_17:51:02
  host      : nm04-a800-node083
  rank      : 0 (local_rank: 0)
  exitcode  : -9 (pid: 1859078)
  error_file: <N/A>
  traceback : Signal 9 (SIGKILL) received by PID 1859078
========================================================
DONE on nm04-a800-node083
RUN on nm04-a800-node083, CUDA_VISIBLE_DEVICES=0
torchrun --standalone --nproc_per_node=1 train_video.py --base configs/cogvideox_2b_lora.yaml configs/sft.yaml --seed 42
[2024-09-09 18:04:57,350] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
[93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, input, weight, bias=None):
/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/kornia/feature/lightglue.py:44: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)
no module 'xformers'. Processing without...
no module 'xformers'. Processing without...
[2024-09-09 18:05:14,362] [INFO] using world size: 1
[2024-09-09 18:05:14,362] [INFO] Will override arguments with manually specified deepspeed_config!
[2024-09-09 18:05:14,369] [INFO] [RANK 0] > initializing model parallel with size 1
[2024-09-09 18:05:14,369] [INFO] [comm.py:637:init_distributed] cdb=None
Namespace(base=['configs/cogvideox_2b_lora.yaml', 'configs/sft.yaml'], model_parallel_size=1, force_pretrain=False, device=0, debug=False, log_image=True, output_dir='samples', input_dir=None, input_type='cli', input_file='input.txt', final_size=2048, sdedit=False, grid_num_rows=1, force_inference=False, lcm_steps=None, sampling_num_frames=32, sampling_fps=8, only_save_latents=False, only_log_video_latents=False, latent_channels=32, image2video=False, experiment_name='lora-disney', train_iters=1000, batch_size=2, lr=0.0005, mode='finetune', seed=42, zero_stage=0, checkpoint_activations=True, checkpoint_num_layers=1, checkpoint_skip_layers=0, fp16=True, bf16=False, gradient_accumulation_steps=1, profiling=-1, epochs=None, log_interval=20, summary_dir='/mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora', save_args=False, lr_decay_iters=None, lr_decay_style='linear', lr_decay_ratio=0.1, warmup=0.01, weight_decay=5e-06, save='/mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora', load='/mnt/ceph/develop/jiawei/model_checkpoint/CogVideoX-2b-sat/transformer', force_train=True, save_interval=100, no_save_rng=False, no_load_rng=True, resume_dataloader=False, distributed_backend='nccl', local_rank=0, exit_interval=None, wandb=True, wandb_project_name='cogv_2b_lora_base_002', eval_batch_size=1, eval_iters=1, eval_interval=100, strict_eval=False, train_data=['/mnt/ceph/develop/jiawei/lora_dataset/cogvideo_lora_dataset'], train_data_weights=None, iterable_dataset=False, iterable_dataset_eval='', batch_from_same_dataset=False, valid_data=['/mnt/ceph/develop/jiawei/lora_dataset/cogvideo_lora_dataset'], test_data=None, split='1,0,0', num_workers=8, block_size=10000, prefetch_factor=4, deepspeed=True, deepspeed_config={'train_micro_batch_size_per_gpu': 2, 'gradient_accumulation_steps': 1, 'steps_per_print': 50, 'gradient_clipping': 0.1, 'zero_optimization': {'stage': 2, 'cpu_offload': False, 'contiguous_gradients': False, 'overlap_comm': True, 'reduce_scatter': True, 'reduce_bucket_size': 1000000000, 'allgather_bucket_size': 1000000000, 'load_from_fp32_weights': False}, 'zero_allow_untested_optimizer': True, 'bf16': {'enabled': False}, 'fp16': {'enabled': True}, 'loss_scale': 0, 'loss_scale_window': 400, 'hysteresis': 2, 'min_loss_scale': 1, 'optimizer': {'type': 'sat.ops.FusedEmaAdam', 'params': {'lr': 0.0005, 'betas': [0.9, 0.95], 'eps': 5e-08, 'weight_decay': 5e-06}}, 'activation_checkpointing': {'partition_activations': False, 'contiguous_memory_optimization': False}, 'wall_clock_breakdown': False}, deepscale=False, deepscale_config=None, model_config={'scale_factor': 1.15258426, 'disable_first_stage_autocast': True, 'not_trainable_prefixes': ['all'], 'log_keys': ['txt'], 'denoiser_config': {'target': 'sgm.modules.diffusionmodules.denoiser.DiscreteDenoiser', 'params': {'num_idx': 1000, 'quantize_c_noise': False, 'weighting_config': {'target': 'sgm.modules.diffusionmodules.denoiser_weighting.EpsWeighting'}, 'scaling_config': {'target': 'sgm.modules.diffusionmodules.denoiser_scaling.VideoScaling'}, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 3.0}}}}, 'network_config': {'target': 'dit_video_concat.DiffusionTransformer', 'params': {'time_embed_dim': 512, 'elementwise_affine': True, 'num_frames': 49, 'time_compressed_rate': 4, 'latent_width': 90, 'latent_height': 60, 'num_layers': 30, 'patch_size': 2, 'in_channels': 16, 'out_channels': 16, 'hidden_size': 1920, 'adm_in_channels': 256, 'num_attention_heads': 30, 'transformer_args': {'checkpoint_activations': True, 'vocab_size': 1, 'max_sequence_length': 64, 'layernorm_order': 'pre', 'skip_init': False, 'model_parallel_size': 1, 'is_decoder': False}, 'modules': {'pos_embed_config': {'target': 'dit_video_concat.Basic3DPositionEmbeddingMixin', 'params': {'text_length': 226, 'height_interpolation': 1.875, 'width_interpolation': 1.875}}, 'lora_config': {'target': 'sat.model.finetune.lora2.LoraMixin', 'params': {'r': 128}}, 'patch_embed_config': {'target': 'dit_video_concat.ImagePatchEmbeddingMixin', 'params': {'text_hidden_size': 4096}}, 'adaln_layer_config': {'target': 'dit_video_concat.AdaLNMixin', 'params': {'qk_ln': True}}, 'final_layer_config': {'target': 'dit_video_concat.FinalLayerMixin'}}}}, 'conditioner_config': {'target': 'sgm.modules.GeneralConditioner', 'params': {'emb_models': [{'is_trainable': False, 'input_key': 'txt', 'ucg_rate': 0.1, 'target': 'sgm.modules.encoders.modules.FrozenT5Embedder', 'params': {'model_dir': '/mnt/ceph/develop/jiawei/model_checkpoint/t5-v1_1-xxl', 'max_length': 2048}}]}}, 'first_stage_config': {'target': 'vae_modules.autoencoder.VideoAutoencoderInferenceWrapper', 'params': {'cp_size': 1, 'ckpt_path': '/mnt/ceph/develop/jiawei/model_checkpoint/CogVideoX-2b-sat/vae/3d-vae.pt', 'ignore_keys': ['loss'], 'loss_config': {'target': 'torch.nn.Identity'}, 'regularizer_config': {'target': 'vae_modules.regularizers.DiagonalGaussianRegularizer'}, 'encoder_config': {'target': 'vae_modules.cp_enc_dec.ContextParallelEncoder3D', 'params': {'double_z': True, 'z_channels': 16, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 2, 4], 'attn_resolutions': [], 'num_res_blocks': 3, 'dropout': 0.0, 'gather_norm': True}}, 'decoder_config': {'target': 'vae_modules.cp_enc_dec.ContextParallelDecoder3D', 'params': {'double_z': True, 'z_channels': 16, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 2, 4], 'attn_resolutions': [], 'num_res_blocks': 3, 'dropout': 0.0, 'gather_norm': False}}}}, 'loss_fn_config': {'target': 'sgm.modules.diffusionmodules.loss.VideoDiffusionLoss', 'params': {'offset_noise_level': 0, 'sigma_sampler_config': {'target': 'sgm.modules.diffusionmodules.sigma_sampling.DiscreteSampling', 'params': {'uniform_sampling': True, 'num_idx': 1000, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 3.0}}}}}}, 'sampler_config': {'target': 'sgm.modules.diffusionmodules.sampling.VPSDEDPMPP2MSampler', 'params': {'num_steps': 50, 'verbose': True, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 3.0}}, 'guider_config': {'target': 'sgm.modules.diffusionmodules.guiders.DynamicCFG', 'params': {'scale': 6, 'exp': 5, 'num_steps': 50}}}}}, data_config={'target': 'data_video.SFTDataset', 'params': {'video_size': [480, 720], 'fps': 8, 'max_num_frames': 49, 'skip_frms_num': 3.0}}, cuda=True, rank=0, world_size=1, deepspeed_activation_checkpointing=True, master_ip='nm04-a800-node083', master_port='32771', log_config=[{'model': {'scale_factor': 1.15258426, 'disable_first_stage_autocast': True, 'not_trainable_prefixes': ['all'], 'log_keys': ['txt'], 'denoiser_config': {'target': 'sgm.modules.diffusionmodules.denoiser.DiscreteDenoiser', 'params': {'num_idx': 1000, 'quantize_c_noise': False, 'weighting_config': {'target': 'sgm.modules.diffusionmodules.denoiser_weighting.EpsWeighting'}, 'scaling_config': {'target': 'sgm.modules.diffusionmodules.denoiser_scaling.VideoScaling'}, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 3.0}}}}, 'network_config': {'target': 'dit_video_concat.DiffusionTransformer', 'params': {'time_embed_dim': 512, 'elementwise_affine': True, 'num_frames': 49, 'time_compressed_rate': 4, 'latent_width': 90, 'latent_height': 60, 'num_layers': 30, 'patch_size': 2, 'in_channels': 16, 'out_channels': 16, 'hidden_size': 1920, 'adm_in_channels': 256, 'num_attention_heads': 30, 'transformer_args': {'checkpoint_activations': True, 'vocab_size': 1, 'max_sequence_length': 64, 'layernorm_order': 'pre', 'skip_init': False, 'model_parallel_size': 1, 'is_decoder': False}, 'modules': {'pos_embed_config': {'target': 'dit_video_concat.Basic3DPositionEmbeddingMixin', 'params': {'text_length': 226, 'height_interpolation': 1.875, 'width_interpolation': 1.875}}, 'lora_config': {'target': 'sat.model.finetune.lora2.LoraMixin', 'params': {'r': 128}}, 'patch_embed_config': {'target': 'dit_video_concat.ImagePatchEmbeddingMixin', 'params': {'text_hidden_size': 4096}}, 'adaln_layer_config': {'target': 'dit_video_concat.AdaLNMixin', 'params': {'qk_ln': True}}, 'final_layer_config': {'target': 'dit_video_concat.FinalLayerMixin'}}}}, 'conditioner_config': {'target': 'sgm.modules.GeneralConditioner', 'params': {'emb_models': [{'is_trainable': False, 'input_key': 'txt', 'ucg_rate': 0.1, 'target': 'sgm.modules.encoders.modules.FrozenT5Embedder', 'params': {'model_dir': '/mnt/ceph/develop/jiawei/model_checkpoint/t5-v1_1-xxl', 'max_length': 2048}}]}}, 'first_stage_config': {'target': 'vae_modules.autoencoder.VideoAutoencoderInferenceWrapper', 'params': {'cp_size': 1, 'ckpt_path': '/mnt/ceph/develop/jiawei/model_checkpoint/CogVideoX-2b-sat/vae/3d-vae.pt', 'ignore_keys': ['loss'], 'loss_config': {'target': 'torch.nn.Identity'}, 'regularizer_config': {'target': 'vae_modules.regularizers.DiagonalGaussianRegularizer'}, 'encoder_config': {'target': 'vae_modules.cp_enc_dec.ContextParallelEncoder3D', 'params': {'double_z': True, 'z_channels': 16, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 2, 4], 'attn_resolutions': [], 'num_res_blocks': 3, 'dropout': 0.0, 'gather_norm': True}}, 'decoder_config': {'target': 'vae_modules.cp_enc_dec.ContextParallelDecoder3D', 'params': {'double_z': True, 'z_channels': 16, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 2, 4], 'attn_resolutions': [], 'num_res_blocks': 3, 'dropout': 0.0, 'gather_norm': False}}}}, 'loss_fn_config': {'target': 'sgm.modules.diffusionmodules.loss.VideoDiffusionLoss', 'params': {'offset_noise_level': 0, 'sigma_sampler_config': {'target': 'sgm.modules.diffusionmodules.sigma_sampling.DiscreteSampling', 'params': {'uniform_sampling': True, 'num_idx': 1000, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 3.0}}}}}}, 'sampler_config': {'target': 'sgm.modules.diffusionmodules.sampling.VPSDEDPMPP2MSampler', 'params': {'num_steps': 50, 'verbose': True, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 3.0}}, 'guider_config': {'target': 'sgm.modules.diffusionmodules.guiders.DynamicCFG', 'params': {'scale': 6, 'exp': 5, 'num_steps': 50}}}}}}, {'args': {'checkpoint_activations': True, 'model_parallel_size': 1, 'experiment_name': 'lora-disney', 'mode': 'finetune', 'load': '/mnt/ceph/develop/jiawei/model_checkpoint/CogVideoX-2b-sat/transformer', 'no_load_rng': True, 'train_iters': 1000, 'eval_iters': 1, 'eval_interval': 100, 'eval_batch_size': 1, 'save': '/mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora', 'save_interval': 100, 'log_interval': 20, 'train_data': ['/mnt/ceph/develop/jiawei/lora_dataset/cogvideo_lora_dataset'], 'valid_data': ['/mnt/ceph/develop/jiawei/lora_dataset/cogvideo_lora_dataset'], 'split': '1,0,0', 'num_workers': 8, 'force_train': True, 'only_log_video_latents': False, 'wandb': True, 'wandb_project_name': 'cogv_2b_lora_base_002', 'summary_dir': '/mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora'}, 'data': {'target': 'data_video.SFTDataset', 'params': {'video_size': [480, 720], 'fps': 8, 'max_num_frames': 49, 'skip_frms_num': 3.0}}, 'deepspeed': {'train_micro_batch_size_per_gpu': 2, 'gradient_accumulation_steps': 1, 'steps_per_print': 50, 'gradient_clipping': 0.1, 'zero_optimization': {'stage': 2, 'cpu_offload': False, 'contiguous_gradients': False, 'overlap_comm': True, 'reduce_scatter': True, 'reduce_bucket_size': 1000000000, 'allgather_bucket_size': 1000000000, 'load_from_fp32_weights': False}, 'zero_allow_untested_optimizer': True, 'bf16': {'enabled': False}, 'fp16': {'enabled': True}, 'loss_scale': 0, 'loss_scale_window': 400, 'hysteresis': 2, 'min_loss_scale': 1, 'optimizer': {'type': 'sat.ops.FusedEmaAdam', 'params': {'lr': '5E-4', 'betas': [0.9, 0.95], 'eps': '5E-8', 'weight_decay': '5E-6'}}, 'activation_checkpointing': {'partition_activations': False, 'contiguous_memory_optimization': False}, 'wall_clock_breakdown': False}}])
[2024-09-09 18:05:15,758] [INFO] [RANK 0] building SATVideoDiffusionEngine model ...
[2024-09-09 18:05:26,047] [WARNING] [RANK 0] Failed to load bitsandbytes:No module named 'bitsandbytes'
[2024-09-09 18:05:26,047] [INFO] [RANK 0] replacing layer 0 attention with lora
[2024-09-09 18:05:26,077] [INFO] [RANK 0] replacing layer 1 attention with lora
[2024-09-09 18:05:26,101] [INFO] [RANK 0] replacing layer 2 attention with lora
[2024-09-09 18:05:26,123] [INFO] [RANK 0] replacing layer 3 attention with lora
[2024-09-09 18:05:26,141] [INFO] [RANK 0] replacing layer 4 attention with lora
[2024-09-09 18:05:26,161] [INFO] [RANK 0] replacing layer 5 attention with lora
[2024-09-09 18:05:26,180] [INFO] [RANK 0] replacing layer 6 attention with lora
[2024-09-09 18:05:26,199] [INFO] [RANK 0] replacing layer 7 attention with lora
[2024-09-09 18:05:26,218] [INFO] [RANK 0] replacing layer 8 attention with lora
[2024-09-09 18:05:26,237] [INFO] [RANK 0] replacing layer 9 attention with lora
[2024-09-09 18:05:26,255] [INFO] [RANK 0] replacing layer 10 attention with lora
[2024-09-09 18:05:26,274] [INFO] [RANK 0] replacing layer 11 attention with lora
[2024-09-09 18:05:26,292] [INFO] [RANK 0] replacing layer 12 attention with lora
[2024-09-09 18:05:26,310] [INFO] [RANK 0] replacing layer 13 attention with lora
[2024-09-09 18:05:26,329] [INFO] [RANK 0] replacing layer 14 attention with lora
[2024-09-09 18:05:26,346] [INFO] [RANK 0] replacing layer 15 attention with lora
[2024-09-09 18:05:26,364] [INFO] [RANK 0] replacing layer 16 attention with lora
[2024-09-09 18:05:26,381] [INFO] [RANK 0] replacing layer 17 attention with lora
[2024-09-09 18:05:26,400] [INFO] [RANK 0] replacing layer 18 attention with lora
[2024-09-09 18:05:26,417] [INFO] [RANK 0] replacing layer 19 attention with lora
[2024-09-09 18:05:26,440] [INFO] [RANK 0] replacing layer 20 attention with lora
[2024-09-09 18:05:26,462] [INFO] [RANK 0] replacing layer 21 attention with lora
[2024-09-09 18:05:26,482] [INFO] [RANK 0] replacing layer 22 attention with lora
[2024-09-09 18:05:26,499] [INFO] [RANK 0] replacing layer 23 attention with lora
[2024-09-09 18:05:26,516] [INFO] [RANK 0] replacing layer 24 attention with lora
[2024-09-09 18:05:26,533] [INFO] [RANK 0] replacing layer 25 attention with lora
[2024-09-09 18:05:26,551] [INFO] [RANK 0] replacing layer 26 attention with lora
[2024-09-09 18:05:26,567] [INFO] [RANK 0] replacing layer 27 attention with lora
[2024-09-09 18:05:26,582] [INFO] [RANK 0] replacing layer 28 attention with lora
[2024-09-09 18:05:26,595] [INFO] [RANK 0] replacing layer 29 attention with lora
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  1.39it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.50it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.48it/s]
Initialized embedder #0: FrozenT5Embedder with 4762310656 params. Trainable: False
Working with z of shape (1, 16, 32, 32) = 16384 dimensions.
/mnt/ceph/develop/jiawei/CogVideo/sat/vae_modules/autoencoder.py:565: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  sd = torch.load(path, map_location="cpu")["state_dict"]
Deleting key loss.logvar from state_dict.
Deleting key loss.perceptual_loss.scaling_layer.shift from state_dict.
Deleting key loss.perceptual_loss.scaling_layer.scale from state_dict.
Deleting key loss.perceptual_loss.net.slice1.0.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice1.0.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice1.2.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice1.2.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice2.5.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice2.5.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice2.7.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice2.7.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice3.10.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice3.10.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice3.12.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice3.12.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice3.14.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice3.14.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice4.17.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice4.17.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice4.19.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice4.19.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice4.21.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice4.21.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice5.24.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice5.24.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice5.26.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice5.26.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice5.28.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice5.28.bias from state_dict.
Deleting key loss.perceptual_loss.lin0.model.1.weight from state_dict.
Deleting key loss.perceptual_loss.lin1.model.1.weight from state_dict.
Deleting key loss.perceptual_loss.lin2.model.1.weight from state_dict.
Deleting key loss.perceptual_loss.lin3.model.1.weight from state_dict.
Deleting key loss.perceptual_loss.lin4.model.1.weight from state_dict.
Deleting key loss.discriminator.blocks.0.downsample_res.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.0.downsample_res.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.0.net.0.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.0.net.0.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.0.net.2.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.0.net.2.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.0.downsample.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.0.downsample.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.1.downsample_res.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.1.downsample_res.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.1.net.0.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.1.net.0.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.1.net.2.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.1.net.2.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.1.downsample.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.1.downsample.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.2.downsample_res.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.2.downsample_res.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.2.net.0.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.2.net.0.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.2.net.2.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.2.net.2.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.2.downsample.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.2.downsample.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.3.downsample_res.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.3.downsample_res.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.3.net.0.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.3.net.0.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.3.net.2.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.3.net.2.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.3.downsample.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.3.downsample.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.4.0.conv_res.weight from state_dict.
Deleting key loss.discriminator.blocks.4.0.conv_res.bias from state_dict.
Deleting key loss.discriminator.blocks.4.0.net.0.weight from state_dict.
Deleting key loss.discriminator.blocks.4.0.net.0.bias from state_dict.
Deleting key loss.discriminator.blocks.4.0.net.2.weight from state_dict.
Deleting key loss.discriminator.blocks.4.0.net.2.bias from state_dict.
Deleting key loss.discriminator.blocks.4.0.downsample.1.weight from state_dict.
Deleting key loss.discriminator.blocks.4.0.downsample.1.bias from state_dict.
Deleting key loss.discriminator.blocks.4.1.0.fn.norm.gamma from state_dict.
Deleting key loss.discriminator.blocks.4.1.0.fn.attn.to_q.0.weight from state_dict.
Deleting key loss.discriminator.blocks.4.1.0.fn.attn.to_kv.0.weight from state_dict.
Deleting key loss.discriminator.blocks.4.1.0.fn.attn.to_out.0.weight from state_dict.
Deleting key loss.discriminator.blocks.4.1.1.fn.norm.gamma from state_dict.
Deleting key loss.discriminator.blocks.4.1.1.fn.net.0.weight from state_dict.
Deleting key loss.discriminator.blocks.4.1.1.fn.net.0.bias from state_dict.
Deleting key loss.discriminator.blocks.4.1.1.fn.net.2.weight from state_dict.
Deleting key loss.discriminator.blocks.4.1.1.fn.net.2.bias from state_dict.
Deleting key loss.discriminator.blocks.5.0.conv_res.weight from state_dict.
Deleting key loss.discriminator.blocks.5.0.conv_res.bias from state_dict.
Deleting key loss.discriminator.blocks.5.0.net.0.weight from state_dict.
Deleting key loss.discriminator.blocks.5.0.net.0.bias from state_dict.
Deleting key loss.discriminator.blocks.5.0.net.2.weight from state_dict.
Deleting key loss.discriminator.blocks.5.0.net.2.bias from state_dict.
Deleting key loss.discriminator.blocks.5.0.downsample.1.weight from state_dict.
Deleting key loss.discriminator.blocks.5.0.downsample.1.bias from state_dict.
Deleting key loss.discriminator.blocks.5.1.0.fn.norm.gamma from state_dict.
Deleting key loss.discriminator.blocks.5.1.0.fn.attn.to_q.0.weight from state_dict.
Deleting key loss.discriminator.blocks.5.1.0.fn.attn.to_kv.0.weight from state_dict.
Deleting key loss.discriminator.blocks.5.1.0.fn.attn.to_out.0.weight from state_dict.
Deleting key loss.discriminator.blocks.5.1.1.fn.norm.gamma from state_dict.
Deleting key loss.discriminator.blocks.5.1.1.fn.net.0.weight from state_dict.
Deleting key loss.discriminator.blocks.5.1.1.fn.net.0.bias from state_dict.
Deleting key loss.discriminator.blocks.5.1.1.fn.net.2.weight from state_dict.
Deleting key loss.discriminator.blocks.5.1.1.fn.net.2.bias from state_dict.
Deleting key loss.discriminator.blocks.6.0.conv_res.weight from state_dict.
Deleting key loss.discriminator.blocks.6.0.conv_res.bias from state_dict.
Deleting key loss.discriminator.blocks.6.0.net.0.weight from state_dict.
Deleting key loss.discriminator.blocks.6.0.net.0.bias from state_dict.
Deleting key loss.discriminator.blocks.6.0.net.2.weight from state_dict.
Deleting key loss.discriminator.blocks.6.0.net.2.bias from state_dict.
Deleting key loss.discriminator.blocks.6.1.0.fn.norm.gamma from state_dict.
Deleting key loss.discriminator.blocks.6.1.0.fn.attn.to_q.0.weight from state_dict.
Deleting key loss.discriminator.blocks.6.1.0.fn.attn.to_kv.0.weight from state_dict.
Deleting key loss.discriminator.blocks.6.1.0.fn.attn.to_out.0.weight from state_dict.
Deleting key loss.discriminator.blocks.6.1.1.fn.norm.gamma from state_dict.
Deleting key loss.discriminator.blocks.6.1.1.fn.net.0.weight from state_dict.
Deleting key loss.discriminator.blocks.6.1.1.fn.net.0.bias from state_dict.
Deleting key loss.discriminator.blocks.6.1.1.fn.net.2.weight from state_dict.
Deleting key loss.discriminator.blocks.6.1.1.fn.net.2.bias from state_dict.
Deleting key loss.discriminator.to_logits.0.weight from state_dict.
Deleting key loss.discriminator.to_logits.0.bias from state_dict.
Deleting key loss.discriminator.to_logits.3.weight from state_dict.
Deleting key loss.discriminator.to_logits.3.bias from state_dict.
Missing keys:  []
Unexpected keys:  []
Restored from /mnt/ceph/develop/jiawei/model_checkpoint/CogVideoX-2b-sat/vae/3d-vae.pt
[2024-09-09 18:05:30,864] [INFO] [RANK 0]  > number of parameters on model parallel rank 0: 6764790755
[2024-09-09 18:05:36,746] [INFO] [RANK 0] global rank 0 is loading checkpoint /mnt/ceph/develop/jiawei/model_checkpoint/CogVideoX-2b-sat/transformer/1000/mp_rank_00_model_states.pt
/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/sat/training/model_io.py:286: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  sd = torch.load(checkpoint_name, map_location='cpu')
[2024-09-09 18:05:38,720] [INFO] [RANK 0] > successfully loaded /mnt/ceph/develop/jiawei/model_checkpoint/CogVideoX-2b-sat/transformer/1000/mp_rank_00_model_states.pt
[2024-09-09 18:05:39,100] [INFO] [RANK 0] ***** Total trainable parameters: 58982400 *****
[2024-09-09 18:05:39,101] [INFO] [RANK 0] [<class 'sat.ops.layernorm.LayerNorm'>, <class 'torch.nn.modules.normalization.LayerNorm'>, <class 'sat.ops.layernorm.RMSNorm'>] is set to no_weight_decay
[2024-09-09 18:05:39,108] [INFO] [RANK 0] Syncing initialized parameters...
[2024-09-09 18:05:39,166] [INFO] [RANK 0] Finished syncing initialized parameters.
[2024-09-09 18:05:39,166] [INFO] [RANK 0] Using optimizer sat.ops.FusedEmaAdam from sat.
[2024-09-09 18:05:39,169] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.4, git-hash=unknown, git-branch=unknown
[2024-09-09 18:05:39,169] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
[2024-09-09 18:05:39,261] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /root/.cache/torch_extensions/py310_cu121/fused_ema_adam/build.ninja...
/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1965: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module fused_ema_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_ema_adam...
Time to load fused_ema_adam op: 3.4032931327819824 seconds
[2024-09-09 18:05:42,695] [INFO] [logging.py:96:log_dist] [Rank 0] Using client callable to create basic optimizer
[2024-09-09 18:05:42,698] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-09-09 18:05:42,724] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedEmaAdam
[2024-09-09 18:05:42,725] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedEmaAdam type=<class 'sat.ops.fused_ema_adam.FusedEmaAdam'>
[2024-09-09 18:05:42,727] [WARNING] [engine.py:1179:_do_optimizer_sanity_check] **** You are using ZeRO with an untested optimizer, proceed with caution *****
[2024-09-09 18:05:42,727] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2024-09-09 18:05:42,727] [INFO] [stage_1_and_2.py:148:__init__] Reduce bucket size 1000000000
[2024-09-09 18:05:42,727] [INFO] [stage_1_and_2.py:149:__init__] Allgather bucket size 1000000000
[2024-09-09 18:05:42,727] [INFO] [stage_1_and_2.py:150:__init__] CPU Offload: False
[2024-09-09 18:05:42,727] [INFO] [stage_1_and_2.py:151:__init__] Round robin gradient partitioning: False
[2024-09-09 18:05:42,975] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2024-09-09 18:05:42,976] [INFO] [utils.py:782:see_memory_usage] MA 12.86 GB         Max_MA 12.97 GB         CA 13.23 GB         Max_CA 13 GB 
[2024-09-09 18:05:42,978] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 120.46 GB, percent = 6.0%
[2024-09-09 18:05:43,124] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2024-09-09 18:05:43,125] [INFO] [utils.py:782:see_memory_usage] MA 12.86 GB         Max_MA 13.08 GB         CA 13.45 GB         Max_CA 13 GB 
[2024-09-09 18:05:43,127] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 120.46 GB, percent = 6.0%
[2024-09-09 18:05:43,127] [INFO] [stage_1_and_2.py:543:__init__] optimizer state initialized
[2024-09-09 18:05:43,262] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2024-09-09 18:05:43,263] [INFO] [utils.py:782:see_memory_usage] MA 12.86 GB         Max_MA 12.86 GB         CA 13.45 GB         Max_CA 13 GB 
[2024-09-09 18:05:43,265] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 120.46 GB, percent = 6.0%
[2024-09-09 18:05:43,269] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer
[2024-09-09 18:05:43,269] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2024-09-09 18:05:43,272] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2024-09-09 18:05:43,272] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[1.0], mom=[[0.9, 0.95]]
[2024-09-09 18:05:43,274] [INFO] [config.py:997:print] DeepSpeedEngine configuration:
[2024-09-09 18:05:43,275] [INFO] [config.py:1001:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-09-09 18:05:43,275] [INFO] [config.py:1001:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-09-09 18:05:43,275] [INFO] [config.py:1001:print]   amp_enabled .................. False
[2024-09-09 18:05:43,275] [INFO] [config.py:1001:print]   amp_params ................... False
[2024-09-09 18:05:43,275] [INFO] [config.py:1001:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-09-09 18:05:43,275] [INFO] [config.py:1001:print]   bfloat16_enabled ............. False
[2024-09-09 18:05:43,275] [INFO] [config.py:1001:print]   bfloat16_immediate_grad_update  False
[2024-09-09 18:05:43,275] [INFO] [config.py:1001:print]   checkpoint_parallel_write_pipeline  False
[2024-09-09 18:05:43,275] [INFO] [config.py:1001:print]   checkpoint_tag_validation_enabled  True
[2024-09-09 18:05:43,275] [INFO] [config.py:1001:print]   checkpoint_tag_validation_fail  False
[2024-09-09 18:05:43,278] [INFO] [config.py:1001:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f61a875faf0>
[2024-09-09 18:05:43,278] [INFO] [config.py:1001:print]   communication_data_type ...... None
[2024-09-09 18:05:43,278] [INFO] [config.py:1001:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-09-09 18:05:43,278] [INFO] [config.py:1001:print]   curriculum_enabled_legacy .... False
[2024-09-09 18:05:43,278] [INFO] [config.py:1001:print]   curriculum_params_legacy ..... False
[2024-09-09 18:05:43,278] [INFO] [config.py:1001:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-09-09 18:05:43,278] [INFO] [config.py:1001:print]   data_efficiency_enabled ...... False
[2024-09-09 18:05:43,278] [INFO] [config.py:1001:print]   dataloader_drop_last ......... False
[2024-09-09 18:05:43,278] [INFO] [config.py:1001:print]   disable_allgather ............ False
[2024-09-09 18:05:43,278] [INFO] [config.py:1001:print]   dump_state ................... False
[2024-09-09 18:05:43,278] [INFO] [config.py:1001:print]   dynamic_loss_scale_args ...... None
[2024-09-09 18:05:43,278] [INFO] [config.py:1001:print]   eigenvalue_enabled ........... False
[2024-09-09 18:05:43,278] [INFO] [config.py:1001:print]   eigenvalue_gas_boundary_resolution  1
[2024-09-09 18:05:43,278] [INFO] [config.py:1001:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-09-09 18:05:43,278] [INFO] [config.py:1001:print]   eigenvalue_layer_num ......... 0
[2024-09-09 18:05:43,278] [INFO] [config.py:1001:print]   eigenvalue_max_iter .......... 100
[2024-09-09 18:05:43,280] [INFO] [config.py:1001:print]   eigenvalue_stability ......... 1e-06
[2024-09-09 18:05:43,280] [INFO] [config.py:1001:print]   eigenvalue_tol ............... 0.01
[2024-09-09 18:05:43,280] [INFO] [config.py:1001:print]   eigenvalue_verbose ........... False
[2024-09-09 18:05:43,280] [INFO] [config.py:1001:print]   elasticity_enabled ........... False
[2024-09-09 18:05:43,280] [INFO] [config.py:1001:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-09-09 18:05:43,280] [INFO] [config.py:1001:print]   fp16_auto_cast ............... False
[2024-09-09 18:05:43,280] [INFO] [config.py:1001:print]   fp16_enabled ................. True
[2024-09-09 18:05:43,280] [INFO] [config.py:1001:print]   fp16_master_weights_and_gradients  False
[2024-09-09 18:05:43,280] [INFO] [config.py:1001:print]   global_rank .................. 0
[2024-09-09 18:05:43,280] [INFO] [config.py:1001:print]   grad_accum_dtype ............. None
[2024-09-09 18:05:43,280] [INFO] [config.py:1001:print]   gradient_accumulation_steps .. 1
[2024-09-09 18:05:43,280] [INFO] [config.py:1001:print]   gradient_clipping ............ 0.1
[2024-09-09 18:05:43,280] [INFO] [config.py:1001:print]   gradient_predivide_factor .... 1.0
[2024-09-09 18:05:43,282] [INFO] [config.py:1001:print]   graph_harvesting ............. False
[2024-09-09 18:05:43,282] [INFO] [config.py:1001:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-09-09 18:05:43,282] [INFO] [config.py:1001:print]   initial_dynamic_scale ........ 65536
[2024-09-09 18:05:43,282] [INFO] [config.py:1001:print]   load_universal_checkpoint .... False
[2024-09-09 18:05:43,282] [INFO] [config.py:1001:print]   loss_scale ................... 0
[2024-09-09 18:05:43,282] [INFO] [config.py:1001:print]   memory_breakdown ............. False
[2024-09-09 18:05:43,282] [INFO] [config.py:1001:print]   mics_hierarchial_params_gather  False
[2024-09-09 18:05:43,282] [INFO] [config.py:1001:print]   mics_shard_size .............. -1
[2024-09-09 18:05:43,282] [INFO] [config.py:1001:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-09-09 18:05:43,282] [INFO] [config.py:1001:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-09-09 18:05:43,284] [INFO] [config.py:1001:print]   optimizer_legacy_fusion ...... False
[2024-09-09 18:05:43,284] [INFO] [config.py:1001:print]   optimizer_name ............... None
[2024-09-09 18:05:43,284] [INFO] [config.py:1001:print]   optimizer_params ............. None
[2024-09-09 18:05:43,284] [INFO] [config.py:1001:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-09-09 18:05:43,284] [INFO] [config.py:1001:print]   pld_enabled .................. False
[2024-09-09 18:05:43,285] [INFO] [config.py:1001:print]   pld_params ................... False
[2024-09-09 18:05:43,285] [INFO] [config.py:1001:print]   prescale_gradients ........... False
[2024-09-09 18:05:43,285] [INFO] [config.py:1001:print]   scheduler_name ............... None
[2024-09-09 18:05:43,285] [INFO] [config.py:1001:print]   scheduler_params ............. None
[2024-09-09 18:05:43,285] [INFO] [config.py:1001:print]   seq_parallel_communication_data_type  torch.float32
[2024-09-09 18:05:43,285] [INFO] [config.py:1001:print]   sparse_attention ............. None
[2024-09-09 18:05:43,285] [INFO] [config.py:1001:print]   sparse_gradients_enabled ..... False
[2024-09-09 18:05:43,285] [INFO] [config.py:1001:print]   steps_per_print .............. 50
[2024-09-09 18:05:43,285] [INFO] [config.py:1001:print]   timers_config ................ enabled=True synchronized=True
[2024-09-09 18:05:43,285] [INFO] [config.py:1001:print]   train_batch_size ............. 2
[2024-09-09 18:05:43,285] [INFO] [config.py:1001:print]   train_micro_batch_size_per_gpu  2
[2024-09-09 18:05:43,287] [INFO] [config.py:1001:print]   use_data_before_expert_parallel_  False
[2024-09-09 18:05:43,287] [INFO] [config.py:1001:print]   use_node_local_storage ....... False
[2024-09-09 18:05:43,287] [INFO] [config.py:1001:print]   wall_clock_breakdown ......... False
[2024-09-09 18:05:43,287] [INFO] [config.py:1001:print]   weight_quantization_config ... None
[2024-09-09 18:05:43,287] [INFO] [config.py:1001:print]   world_size ................... 1
[2024-09-09 18:05:43,287] [INFO] [config.py:1001:print]   zero_allow_untested_optimizer  True
[2024-09-09 18:05:43,287] [INFO] [config.py:1001:print]   zero_config .................. stage=2 contiguous_gradients=False reduce_scatter=True reduce_bucket_size=1000000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=1000000000 overlap_comm=True load_from_fp32_weights=False elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-09-09 18:05:43,287] [INFO] [config.py:1001:print]   zero_enabled ................. True
[2024-09-09 18:05:43,287] [INFO] [config.py:1001:print]   zero_force_ds_cpu_optimizer .. True
[2024-09-09 18:05:43,287] [INFO] [config.py:1001:print]   zero_optimization_stage ...... 2
[2024-09-09 18:05:43,287] [INFO] [config.py:987:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 2, 
    "gradient_accumulation_steps": 1, 
    "steps_per_print": 50, 
    "gradient_clipping": 0.1, 
    "zero_optimization": {
        "stage": 2, 
        "cpu_offload": false, 
        "contiguous_gradients": false, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 1.000000e+09, 
        "allgather_bucket_size": 1.000000e+09, 
        "load_from_fp32_weights": false
    }, 
    "zero_allow_untested_optimizer": true, 
    "bf16": {
        "enabled": false
    }, 
    "fp16": {
        "enabled": true
    }, 
    "loss_scale": 0, 
    "loss_scale_window": 400, 
    "hysteresis": 2, 
    "min_loss_scale": 1, 
    "activation_checkpointing": {
        "partition_activations": false, 
        "contiguous_memory_optimization": false
    }, 
    "wall_clock_breakdown": false
}
[2024-09-09 18:05:43,292] [INFO] [RANK 0] learning rate decaying style linear, ratio 10.0
[2024-09-09 18:05:43,292] [INFO] [RANK 0] Finetuning Model...
[2024-09-09 18:05:43,293] [INFO] [RANK 0] arguments:
[2024-09-09 18:05:43,293] [INFO] [RANK 0]   base ......................... ['configs/cogvideox_2b_lora.yaml', 'configs/sft.yaml']
[2024-09-09 18:05:43,293] [INFO] [RANK 0]   model_parallel_size .......... 1
[2024-09-09 18:05:43,293] [INFO] [RANK 0]   force_pretrain ............... False
[2024-09-09 18:05:43,293] [INFO] [RANK 0]   device ....................... 0
[2024-09-09 18:05:43,293] [INFO] [RANK 0]   debug ........................ False
[2024-09-09 18:05:43,296] [INFO] [RANK 0]   log_image .................... True
[2024-09-09 18:05:43,296] [INFO] [RANK 0]   output_dir ................... samples
[2024-09-09 18:05:43,296] [INFO] [RANK 0]   input_dir .................... None
[2024-09-09 18:05:43,296] [INFO] [RANK 0]   input_type ................... cli
[2024-09-09 18:05:43,296] [INFO] [RANK 0]   input_file ................... input.txt
[2024-09-09 18:05:43,296] [INFO] [RANK 0]   final_size ................... 2048
[2024-09-09 18:05:43,296] [INFO] [RANK 0]   sdedit ....................... False
[2024-09-09 18:05:43,296] [INFO] [RANK 0]   grid_num_rows ................ 1
[2024-09-09 18:05:43,296] [INFO] [RANK 0]   force_inference .............. False
[2024-09-09 18:05:43,296] [INFO] [RANK 0]   lcm_steps .................... None
[2024-09-09 18:05:43,296] [INFO] [RANK 0]   sampling_num_frames .......... 32
[2024-09-09 18:05:43,296] [INFO] [RANK 0]   sampling_fps ................. 8
[2024-09-09 18:05:43,296] [INFO] [RANK 0]   only_save_latents ............ False
[2024-09-09 18:05:43,296] [INFO] [RANK 0]   only_log_video_latents ....... False
[2024-09-09 18:05:43,296] [INFO] [RANK 0]   latent_channels .............. 32
[2024-09-09 18:05:43,296] [INFO] [RANK 0]   image2video .................. False
[2024-09-09 18:05:43,296] [INFO] [RANK 0]   experiment_name .............. lora-disney-09-09-18-05
[2024-09-09 18:05:43,299] [INFO] [RANK 0]   train_iters .................. 1000
[2024-09-09 18:05:43,299] [INFO] [RANK 0]   batch_size ................... 2
[2024-09-09 18:05:43,299] [INFO] [RANK 0]   lr ........................... 0.0005
[2024-09-09 18:05:43,299] [INFO] [RANK 0]   mode ......................... finetune
[2024-09-09 18:05:43,299] [INFO] [RANK 0]   seed ......................... 42
[2024-09-09 18:05:43,299] [INFO] [RANK 0]   zero_stage ................... 0
[2024-09-09 18:05:43,299] [INFO] [RANK 0]   checkpoint_activations ....... True
[2024-09-09 18:05:43,299] [INFO] [RANK 0]   checkpoint_num_layers ........ 1
[2024-09-09 18:05:43,299] [INFO] [RANK 0]   checkpoint_skip_layers ....... 0
[2024-09-09 18:05:43,299] [INFO] [RANK 0]   fp16 ......................... True
[2024-09-09 18:05:43,303] [INFO] [RANK 0]   bf16 ......................... False
[2024-09-09 18:05:43,303] [INFO] [RANK 0]   gradient_accumulation_steps .. 1
[2024-09-09 18:05:43,303] [INFO] [RANK 0]   profiling .................... -1
[2024-09-09 18:05:43,303] [INFO] [RANK 0]   epochs ....................... None
[2024-09-09 18:05:43,303] [INFO] [RANK 0]   log_interval ................. 20
[2024-09-09 18:05:43,303] [INFO] [RANK 0]   summary_dir .................. /mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora
[2024-09-09 18:05:43,303] [INFO] [RANK 0]   save_args .................... False
[2024-09-09 18:05:43,303] [INFO] [RANK 0]   lr_decay_iters ............... None
[2024-09-09 18:05:43,303] [INFO] [RANK 0]   lr_decay_style ............... linear
[2024-09-09 18:05:43,303] [INFO] [RANK 0]   lr_decay_ratio ............... 0.1
[2024-09-09 18:05:43,303] [INFO] [RANK 0]   warmup ....................... 0.01
[2024-09-09 18:05:43,303] [INFO] [RANK 0]   weight_decay ................. 5e-06
[2024-09-09 18:05:43,303] [INFO] [RANK 0]   save ......................... /mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora/lora-disney-09-09-18-05
[2024-09-09 18:05:43,303] [INFO] [RANK 0]   load ......................... /mnt/ceph/develop/jiawei/model_checkpoint/CogVideoX-2b-sat/transformer
[2024-09-09 18:05:43,305] [INFO] [RANK 0]   force_train .................. True
[2024-09-09 18:05:43,305] [INFO] [RANK 0]   save_interval ................ 100
[2024-09-09 18:05:43,305] [INFO] [RANK 0]   no_save_rng .................. False
[2024-09-09 18:05:43,305] [INFO] [RANK 0]   no_load_rng .................. True
[2024-09-09 18:05:43,305] [INFO] [RANK 0]   resume_dataloader ............ False
[2024-09-09 18:05:43,305] [INFO] [RANK 0]   distributed_backend .......... nccl
[2024-09-09 18:05:43,305] [INFO] [RANK 0]   local_rank ................... 0
[2024-09-09 18:05:43,305] [INFO] [RANK 0]   exit_interval ................ None
[2024-09-09 18:05:43,305] [INFO] [RANK 0]   wandb ........................ True
[2024-09-09 18:05:43,306] [INFO] [RANK 0]   wandb_project_name ........... cogv_2b_lora_base_002
[2024-09-09 18:05:43,306] [INFO] [RANK 0]   eval_batch_size .............. 1
[2024-09-09 18:05:43,306] [INFO] [RANK 0]   eval_iters ................... 1
[2024-09-09 18:05:43,306] [INFO] [RANK 0]   eval_interval ................ 100
[2024-09-09 18:05:43,306] [INFO] [RANK 0]   strict_eval .................. False
[2024-09-09 18:05:43,306] [INFO] [RANK 0]   train_data ................... ['/mnt/ceph/develop/jiawei/lora_dataset/cogvideo_lora_dataset']
[2024-09-09 18:05:43,309] [INFO] [RANK 0]   train_data_weights ........... None
[2024-09-09 18:05:43,309] [INFO] [RANK 0]   iterable_dataset ............. False
[2024-09-09 18:05:43,309] [INFO] [RANK 0]   iterable_dataset_eval ........ 
[2024-09-09 18:05:43,309] [INFO] [RANK 0]   batch_from_same_dataset ...... False
[2024-09-09 18:05:43,309] [INFO] [RANK 0]   valid_data ................... ['/mnt/ceph/develop/jiawei/lora_dataset/cogvideo_lora_dataset']
[2024-09-09 18:05:43,309] [INFO] [RANK 0]   test_data .................... None
[2024-09-09 18:05:43,309] [INFO] [RANK 0]   split ........................ 1,0,0
[2024-09-09 18:05:43,309] [INFO] [RANK 0]   num_workers .................. 8
[2024-09-09 18:05:43,309] [INFO] [RANK 0]   block_size ................... 10000
[2024-09-09 18:05:43,309] [INFO] [RANK 0]   prefetch_factor .............. 4
[2024-09-09 18:05:43,309] [INFO] [RANK 0]   deepspeed .................... True
[2024-09-09 18:05:43,309] [INFO] [RANK 0]   deepspeed_config ............. {'train_micro_batch_size_per_gpu': 2, 'gradient_accumulation_steps': 1, 'steps_per_print': 50, 'gradient_clipping': 0.1, 'zero_optimization': {'stage': 2, 'cpu_offload': False, 'contiguous_gradients': False, 'overlap_comm': True, 'reduce_scatter': True, 'reduce_bucket_size': 1000000000, 'allgather_bucket_size': 1000000000, 'load_from_fp32_weights': False}, 'zero_allow_untested_optimizer': True, 'bf16': {'enabled': False}, 'fp16': {'enabled': True}, 'loss_scale': 0, 'loss_scale_window': 400, 'hysteresis': 2, 'min_loss_scale': 1, 'activation_checkpointing': {'partition_activations': False, 'contiguous_memory_optimization': False}, 'wall_clock_breakdown': False}
[2024-09-09 18:05:43,309] [INFO] [RANK 0]   deepscale .................... False
[2024-09-09 18:05:43,309] [INFO] [RANK 0]   deepscale_config ............. None
[2024-09-09 18:05:43,309] [INFO] [RANK 0]   model_config ................. {'scale_factor': 1.15258426, 'disable_first_stage_autocast': True, 'not_trainable_prefixes': ['all'], 'log_keys': ['txt'], 'denoiser_config': {'target': 'sgm.modules.diffusionmodules.denoiser.DiscreteDenoiser', 'params': {'num_idx': 1000, 'quantize_c_noise': False, 'weighting_config': {'target': 'sgm.modules.diffusionmodules.denoiser_weighting.EpsWeighting'}, 'scaling_config': {'target': 'sgm.modules.diffusionmodules.denoiser_scaling.VideoScaling'}, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 3.0}}}}, 'network_config': {'target': 'dit_video_concat.DiffusionTransformer', 'params': {'time_embed_dim': 512, 'elementwise_affine': True, 'num_frames': 49, 'time_compressed_rate': 4, 'latent_width': 90, 'latent_height': 60, 'num_layers': 30, 'patch_size': 2, 'in_channels': 16, 'out_channels': 16, 'hidden_size': 1920, 'adm_in_channels': 256, 'num_attention_heads': 30, 'transformer_args': {'checkpoint_activations': True, 'vocab_size': 1, 'max_sequence_length': 64, 'layernorm_order': 'pre', 'skip_init': False, 'model_parallel_size': 1, 'is_decoder': False, 'num_layers': 30, 'hidden_size': 1920, 'num_attention_heads': 30, 'parallel_output': True}, 'modules': {'pos_embed_config': {'target': 'dit_video_concat.Basic3DPositionEmbeddingMixin', 'params': {'text_length': 226, 'height_interpolation': 1.875, 'width_interpolation': 1.875}}, 'lora_config': {'target': 'sat.model.finetune.lora2.LoraMixin', 'params': {'r': 128}}, 'patch_embed_config': {'target': 'dit_video_concat.ImagePatchEmbeddingMixin', 'params': {'text_hidden_size': 4096}}, 'adaln_layer_config': {'target': 'dit_video_concat.AdaLNMixin', 'params': {'qk_ln': True}}, 'final_layer_config': {'target': 'dit_video_concat.FinalLayerMixin'}}, 'dtype': 'fp16'}}, 'conditioner_config': {'target': 'sgm.modules.GeneralConditioner', 'params': {'emb_models': [{'is_trainable': False, 'input_key': 'txt', 'ucg_rate': 0.1, 'target': 'sgm.modules.encoders.modules.FrozenT5Embedder', 'params': {'model_dir': '/mnt/ceph/develop/jiawei/model_checkpoint/t5-v1_1-xxl', 'max_length': 2048}}]}}, 'first_stage_config': {'target': 'vae_modules.autoencoder.VideoAutoencoderInferenceWrapper', 'params': {'cp_size': 1, 'ckpt_path': '/mnt/ceph/develop/jiawei/model_checkpoint/CogVideoX-2b-sat/vae/3d-vae.pt', 'ignore_keys': ['loss'], 'loss_config': {'target': 'torch.nn.Identity'}, 'regularizer_config': {'target': 'vae_modules.regularizers.DiagonalGaussianRegularizer'}, 'encoder_config': {'target': 'vae_modules.cp_enc_dec.ContextParallelEncoder3D', 'params': {'double_z': True, 'z_channels': 16, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 2, 4], 'attn_resolutions': [], 'num_res_blocks': 3, 'dropout': 0.0, 'gather_norm': True}}, 'decoder_config': {'target': 'vae_modules.cp_enc_dec.ContextParallelDecoder3D', 'params': {'double_z': True, 'z_channels': 16, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 2, 4], 'attn_resolutions': [], 'num_res_blocks': 3, 'dropout': 0.0, 'gather_norm': False}}}}, 'loss_fn_config': {'target': 'sgm.modules.diffusionmodules.loss.VideoDiffusionLoss', 'params': {'offset_noise_level': 0, 'sigma_sampler_config': {'target': 'sgm.modules.diffusionmodules.sigma_sampling.DiscreteSampling', 'params': {'uniform_sampling': True, 'num_idx': 1000, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 3.0}}}}}}, 'sampler_config': {'target': 'sgm.modules.diffusionmodules.sampling.VPSDEDPMPP2MSampler', 'params': {'num_steps': 50, 'verbose': True, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 3.0}}, 'guider_config': {'target': 'sgm.modules.diffusionmodules.guiders.DynamicCFG', 'params': {'scale': 6, 'exp': 5, 'num_steps': 50}}}}}
[2024-09-09 18:05:43,312] [INFO] [RANK 0]   data_config .................. {'target': 'data_video.SFTDataset', 'params': {'video_size': [480, 720], 'fps': 8, 'max_num_frames': 49, 'skip_frms_num': 3.0}}
[2024-09-09 18:05:43,312] [INFO] [RANK 0]   cuda ......................... True
[2024-09-09 18:05:43,312] [INFO] [RANK 0]   rank ......................... 0
[2024-09-09 18:05:43,312] [INFO] [RANK 0]   world_size ................... 1
[2024-09-09 18:05:43,312] [INFO] [RANK 0]   deepspeed_activation_checkpointing  True
[2024-09-09 18:05:43,312] [INFO] [RANK 0]   master_ip .................... nm04-a800-node083
[2024-09-09 18:05:43,312] [INFO] [RANK 0]   master_port .................. 32771
[2024-09-09 18:05:43,312] [INFO] [RANK 0]   log_config ................... [{'model': {'scale_factor': 1.15258426, 'disable_first_stage_autocast': True, 'not_trainable_prefixes': ['all'], 'log_keys': ['txt'], 'denoiser_config': {'target': 'sgm.modules.diffusionmodules.denoiser.DiscreteDenoiser', 'params': {'num_idx': 1000, 'quantize_c_noise': False, 'weighting_config': {'target': 'sgm.modules.diffusionmodules.denoiser_weighting.EpsWeighting'}, 'scaling_config': {'target': 'sgm.modules.diffusionmodules.denoiser_scaling.VideoScaling'}, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 3.0}}}}, 'network_config': {'target': 'dit_video_concat.DiffusionTransformer', 'params': {'time_embed_dim': 512, 'elementwise_affine': True, 'num_frames': 49, 'time_compressed_rate': 4, 'latent_width': 90, 'latent_height': 60, 'num_layers': 30, 'patch_size': 2, 'in_channels': 16, 'out_channels': 16, 'hidden_size': 1920, 'adm_in_channels': 256, 'num_attention_heads': 30, 'transformer_args': {'checkpoint_activations': True, 'vocab_size': 1, 'max_sequence_length': 64, 'layernorm_order': 'pre', 'skip_init': False, 'model_parallel_size': 1, 'is_decoder': False}, 'modules': {'pos_embed_config': {'target': 'dit_video_concat.Basic3DPositionEmbeddingMixin', 'params': {'text_length': 226, 'height_interpolation': 1.875, 'width_interpolation': 1.875}}, 'lora_config': {'target': 'sat.model.finetune.lora2.LoraMixin', 'params': {'r': 128}}, 'patch_embed_config': {'target': 'dit_video_concat.ImagePatchEmbeddingMixin', 'params': {'text_hidden_size': 4096}}, 'adaln_layer_config': {'target': 'dit_video_concat.AdaLNMixin', 'params': {'qk_ln': True}}, 'final_layer_config': {'target': 'dit_video_concat.FinalLayerMixin'}}}}, 'conditioner_config': {'target': 'sgm.modules.GeneralConditioner', 'params': {'emb_models': [{'is_trainable': False, 'input_key': 'txt', 'ucg_rate': 0.1, 'target': 'sgm.modules.encoders.modules.FrozenT5Embedder', 'params': {'model_dir': '/mnt/ceph/develop/jiawei/model_checkpoint/t5-v1_1-xxl', 'max_length': 2048}}]}}, 'first_stage_config': {'target': 'vae_modules.autoencoder.VideoAutoencoderInferenceWrapper', 'params': {'cp_size': 1, 'ckpt_path': '/mnt/ceph/develop/jiawei/model_checkpoint/CogVideoX-2b-sat/vae/3d-vae.pt', 'ignore_keys': ['loss'], 'loss_config': {'target': 'torch.nn.Identity'}, 'regularizer_config': {'target': 'vae_modules.regularizers.DiagonalGaussianRegularizer'}, 'encoder_config': {'target': 'vae_modules.cp_enc_dec.ContextParallelEncoder3D', 'params': {'double_z': True, 'z_channels': 16, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 2, 4], 'attn_resolutions': [], 'num_res_blocks': 3, 'dropout': 0.0, 'gather_norm': True}}, 'decoder_config': {'target': 'vae_modules.cp_enc_dec.ContextParallelDecoder3D', 'params': {'double_z': True, 'z_channels': 16, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 2, 4], 'attn_resolutions': [], 'num_res_blocks': 3, 'dropout': 0.0, 'gather_norm': False}}}}, 'loss_fn_config': {'target': 'sgm.modules.diffusionmodules.loss.VideoDiffusionLoss', 'params': {'offset_noise_level': 0, 'sigma_sampler_config': {'target': 'sgm.modules.diffusionmodules.sigma_sampling.DiscreteSampling', 'params': {'uniform_sampling': True, 'num_idx': 1000, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 3.0}}}}}}, 'sampler_config': {'target': 'sgm.modules.diffusionmodules.sampling.VPSDEDPMPP2MSampler', 'params': {'num_steps': 50, 'verbose': True, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 3.0}}, 'guider_config': {'target': 'sgm.modules.diffusionmodules.guiders.DynamicCFG', 'params': {'scale': 6, 'exp': 5, 'num_steps': 50}}}}}}, {'args': {'checkpoint_activations': True, 'model_parallel_size': 1, 'experiment_name': 'lora-disney', 'mode': 'finetune', 'load': '/mnt/ceph/develop/jiawei/model_checkpoint/CogVideoX-2b-sat/transformer', 'no_load_rng': True, 'train_iters': 1000, 'eval_iters': 1, 'eval_interval': 100, 'eval_batch_size': 1, 'save': '/mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora', 'save_interval': 100, 'log_interval': 20, 'train_data': ['/mnt/ceph/develop/jiawei/lora_dataset/cogvideo_lora_dataset'], 'valid_data': ['/mnt/ceph/develop/jiawei/lora_dataset/cogvideo_lora_dataset'], 'split': '1,0,0', 'num_workers': 8, 'force_train': True, 'only_log_video_latents': False, 'wandb': True, 'wandb_project_name': 'cogv_2b_lora_base_002', 'summary_dir': '/mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora'}, 'data': {'target': 'data_video.SFTDataset', 'params': {'video_size': [480, 720], 'fps': 8, 'max_num_frames': 49, 'skip_frms_num': 3.0}}, 'deepspeed': {'train_micro_batch_size_per_gpu': 2, 'gradient_accumulation_steps': 1, 'steps_per_print': 50, 'gradient_clipping': 0.1, 'zero_optimization': {'stage': 2, 'cpu_offload': False, 'contiguous_gradients': False, 'overlap_comm': True, 'reduce_scatter': True, 'reduce_bucket_size': 1000000000, 'allgather_bucket_size': 1000000000, 'load_from_fp32_weights': False}, 'zero_allow_untested_optimizer': True, 'bf16': {'enabled': False}, 'fp16': {'enabled': True}, 'loss_scale': 0, 'loss_scale_window': 400, 'hysteresis': 2, 'min_loss_scale': 1, 'optimizer': {'type': 'sat.ops.FusedEmaAdam', 'params': {'lr': '5E-4', 'betas': [0.9, 0.95], 'eps': '5E-8', 'weight_decay': '5E-6'}}, 'activation_checkpointing': {'partition_activations': False, 'contiguous_memory_optimization': False}, 'wall_clock_breakdown': False}}]
[2024-09-09 18:05:43,314] [INFO] [RANK 0]   do_train ..................... True
[2024-09-09 18:05:43,314] [INFO] [RANK 0]   val_last_shape ............... []
[2024-09-09 18:05:43,314] [INFO] [RANK 0]   val_drop_number .............. 0
[2024-09-09 18:05:43,314] [INFO] [RANK 0]   do_valid ..................... True
[2024-09-09 18:05:43,314] [INFO] [RANK 0]   do_test ...................... False
[2024-09-09 18:05:43,314] [INFO] [RANK 0]   iteration .................... 0
wandb: Currently logged in as: dmeck. Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.17.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora/wandb/run-20240909_180546-a1w4r7cm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lora-disney-09-09-18-05
wandb: â­ï¸ View project at https://wandb.ai/dmeck/cogv_2b_lora_base_002
wandb: ðŸš€ View run at https://wandb.ai/dmeck/cogv_2b_lora_base_002/runs/a1w4r7cm
Traceback (most recent call last):
  File "/mnt/ceph/develop/jiawei/CogVideo/sat/train_video.py", line 223, in <module>
    training_main(
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/sat/training/deepspeed_training.py", line 157, in training_main
    iteration, skipped = train(model, optimizer,
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/sat/training/deepspeed_training.py", line 359, in train
    lm_loss, skipped_iter, metrics = train_step(train_data_iterator,
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/sat/training/deepspeed_training.py", line 443, in train_step
    forward_ret = forward_step(data_iterator, model, args, timers, **kwargs)
  File "/mnt/ceph/develop/jiawei/CogVideo/sat/train_video.py", line 195, in forward_step
    loss, loss_dict = model.shared_step(batch)
  File "/mnt/ceph/develop/jiawei/CogVideo/sat/diffusion_video.py", line 147, in shared_step
    loss, loss_dict = self(x, batch)
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/ceph/develop/jiawei/CogVideo/sat/diffusion_video.py", line 128, in forward
    loss = self.loss_fn(self.model, self.denoiser, self.conditioner, x, batch)
  File "/mnt/ceph/develop/jiawei/CogVideo/sat/sgm/modules/diffusionmodules/loss.py", line 110, in __call__
    model_output = denoiser(network, noised_input, alphas_cumprod_sqrt, cond, **additional_model_inputs)
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/ceph/develop/jiawei/CogVideo/sat/sgm/modules/diffusionmodules/denoiser.py", line 38, in forward
    return network(input * c_in, c_noise, cond, **additional_model_inputs) * c_out + input * c_skip
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/ceph/develop/jiawei/CogVideo/sat/sgm/modules/diffusionmodules/wrappers.py", line 35, in forward
    return self.diffusion_model(
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/ceph/develop/jiawei/CogVideo/sat/dit_video_concat.py", line 770, in forward
    output = super().forward(**kwargs)[0]
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/sat/model/base_model.py", line 138, in forward
    return self.transformer(*args, **kwargs)
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/sat/model/transformer.py", line 608, in forward
    hidden_states = hidden_states + position_embeddings
RuntimeError: The size of tensor a (19598) must match the size of tensor b (17776) at non-singleton dimension 1
[rank0]: Traceback (most recent call last):
[rank0]:   File "/mnt/ceph/develop/jiawei/CogVideo/sat/train_video.py", line 223, in <module>
[rank0]:     training_main(
[rank0]:   File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/sat/training/deepspeed_training.py", line 157, in training_main
[rank0]:     iteration, skipped = train(model, optimizer,
[rank0]:   File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/sat/training/deepspeed_training.py", line 359, in train
[rank0]:     lm_loss, skipped_iter, metrics = train_step(train_data_iterator,
[rank0]:   File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/sat/training/deepspeed_training.py", line 443, in train_step
[rank0]:     forward_ret = forward_step(data_iterator, model, args, timers, **kwargs)
[rank0]:   File "/mnt/ceph/develop/jiawei/CogVideo/sat/train_video.py", line 195, in forward_step
[rank0]:     loss, loss_dict = model.shared_step(batch)
[rank0]:   File "/mnt/ceph/develop/jiawei/CogVideo/sat/diffusion_video.py", line 147, in shared_step
[rank0]:     loss, loss_dict = self(x, batch)
[rank0]:   File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/mnt/ceph/develop/jiawei/CogVideo/sat/diffusion_video.py", line 128, in forward
[rank0]:     loss = self.loss_fn(self.model, self.denoiser, self.conditioner, x, batch)
[rank0]:   File "/mnt/ceph/develop/jiawei/CogVideo/sat/sgm/modules/diffusionmodules/loss.py", line 110, in __call__
[rank0]:     model_output = denoiser(network, noised_input, alphas_cumprod_sqrt, cond, **additional_model_inputs)
[rank0]:   File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/mnt/ceph/develop/jiawei/CogVideo/sat/sgm/modules/diffusionmodules/denoiser.py", line 38, in forward
[rank0]:     return network(input * c_in, c_noise, cond, **additional_model_inputs) * c_out + input * c_skip
[rank0]:   File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/mnt/ceph/develop/jiawei/CogVideo/sat/sgm/modules/diffusionmodules/wrappers.py", line 35, in forward
[rank0]:     return self.diffusion_model(
[rank0]:   File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/mnt/ceph/develop/jiawei/CogVideo/sat/dit_video_concat.py", line 770, in forward
[rank0]:     output = super().forward(**kwargs)[0]
[rank0]:   File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/sat/model/base_model.py", line 138, in forward
[rank0]:     return self.transformer(*args, **kwargs)
[rank0]:   File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/sat/model/transformer.py", line 608, in forward
[rank0]:     hidden_states = hidden_states + position_embeddings
[rank0]: RuntimeError: The size of tensor a (19598) must match the size of tensor b (17776) at non-singleton dimension 1
wandb: - 0.015 MB of 0.015 MB uploadedwandb: \ 0.015 MB of 0.015 MB uploadedwandb: | 0.015 MB of 0.015 MB uploadedwandb: / 0.015 MB of 0.015 MB uploadedwandb: - 0.015 MB of 0.015 MB uploadedwandb: \ 0.018 MB of 0.032 MB uploadedwandb: | 0.021 MB of 0.032 MB uploadedwandb: / 0.032 MB of 0.032 MB uploadedwandb: ðŸš€ View run lora-disney-09-09-18-05 at: https://wandb.ai/dmeck/cogv_2b_lora_base_002/runs/a1w4r7cm
wandb: â­ï¸ View project at: https://wandb.ai/dmeck/cogv_2b_lora_base_002
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: /mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora/wandb/run-20240909_180546-a1w4r7cm/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
E0909 18:06:40.456000 140394264336192 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 0 (pid: 2029735) of binary: /mnt/ceph/develop/jiawei/conda_env/cogvideo/bin/python
Traceback (most recent call last):
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train_video.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-09-09_18:06:40
  host      : nm04-a800-node083
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 2029735)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
DONE on nm04-a800-node083
RUN on nm04-a800-node083, CUDA_VISIBLE_DEVICES=0
torchrun --standalone --nproc_per_node=1 train_video.py --base configs/cogvideox_2b_lora.yaml configs/sft.yaml --seed 42
[2024-09-09 18:07:54,416] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
[93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, input, weight, bias=None):
/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/kornia/feature/lightglue.py:44: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)
no module 'xformers'. Processing without...
no module 'xformers'. Processing without...
[2024-09-09 18:08:17,483] [INFO] using world size: 1
[2024-09-09 18:08:17,483] [INFO] Will override arguments with manually specified deepspeed_config!
[2024-09-09 18:08:17,492] [INFO] [RANK 0] > initializing model parallel with size 1
[2024-09-09 18:08:17,493] [INFO] [comm.py:637:init_distributed] cdb=None
Namespace(base=['configs/cogvideox_2b_lora.yaml', 'configs/sft.yaml'], model_parallel_size=1, force_pretrain=False, device=0, debug=False, log_image=True, output_dir='samples', input_dir=None, input_type='cli', input_file='input.txt', final_size=2048, sdedit=False, grid_num_rows=1, force_inference=False, lcm_steps=None, sampling_num_frames=32, sampling_fps=8, only_save_latents=False, only_log_video_latents=False, latent_channels=32, image2video=False, experiment_name='lora-disney', train_iters=1000, batch_size=2, lr=0.0005, mode='finetune', seed=42, zero_stage=0, checkpoint_activations=True, checkpoint_num_layers=1, checkpoint_skip_layers=0, fp16=True, bf16=False, gradient_accumulation_steps=1, profiling=-1, epochs=None, log_interval=20, summary_dir='/mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora', save_args=False, lr_decay_iters=None, lr_decay_style='linear', lr_decay_ratio=0.1, warmup=0.01, weight_decay=5e-06, save='/mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora', load='/mnt/ceph/develop/jiawei/model_checkpoint/CogVideoX-2b-sat/transformer', force_train=True, save_interval=100, no_save_rng=False, no_load_rng=True, resume_dataloader=False, distributed_backend='nccl', local_rank=0, exit_interval=None, wandb=True, wandb_project_name='cogv_2b_lora_base_002', eval_batch_size=1, eval_iters=1, eval_interval=100, strict_eval=False, train_data=['/mnt/ceph/develop/jiawei/lora_dataset/cogvideo_lora_dataset'], train_data_weights=None, iterable_dataset=False, iterable_dataset_eval='', batch_from_same_dataset=False, valid_data=['/mnt/ceph/develop/jiawei/lora_dataset/cogvideo_lora_dataset'], test_data=None, split='1,0,0', num_workers=8, block_size=10000, prefetch_factor=4, deepspeed=True, deepspeed_config={'train_micro_batch_size_per_gpu': 2, 'gradient_accumulation_steps': 1, 'steps_per_print': 50, 'gradient_clipping': 0.1, 'zero_optimization': {'stage': 2, 'cpu_offload': False, 'contiguous_gradients': False, 'overlap_comm': True, 'reduce_scatter': True, 'reduce_bucket_size': 1000000000, 'allgather_bucket_size': 1000000000, 'load_from_fp32_weights': False}, 'zero_allow_untested_optimizer': True, 'bf16': {'enabled': False}, 'fp16': {'enabled': True}, 'loss_scale': 0, 'loss_scale_window': 400, 'hysteresis': 2, 'min_loss_scale': 1, 'optimizer': {'type': 'sat.ops.FusedEmaAdam', 'params': {'lr': 0.0005, 'betas': [0.9, 0.95], 'eps': 5e-08, 'weight_decay': 5e-06}}, 'activation_checkpointing': {'partition_activations': False, 'contiguous_memory_optimization': False}, 'wall_clock_breakdown': False}, deepscale=False, deepscale_config=None, model_config={'scale_factor': 1.15258426, 'disable_first_stage_autocast': True, 'not_trainable_prefixes': ['all'], 'log_keys': ['txt'], 'denoiser_config': {'target': 'sgm.modules.diffusionmodules.denoiser.DiscreteDenoiser', 'params': {'num_idx': 1000, 'quantize_c_noise': False, 'weighting_config': {'target': 'sgm.modules.diffusionmodules.denoiser_weighting.EpsWeighting'}, 'scaling_config': {'target': 'sgm.modules.diffusionmodules.denoiser_scaling.VideoScaling'}, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 3.0}}}}, 'network_config': {'target': 'dit_video_concat.DiffusionTransformer', 'params': {'time_embed_dim': 512, 'elementwise_affine': True, 'num_frames': 49, 'time_compressed_rate': 4, 'latent_width': 90, 'latent_height': 60, 'num_layers': 30, 'patch_size': 2, 'in_channels': 16, 'out_channels': 16, 'hidden_size': 1920, 'adm_in_channels': 256, 'num_attention_heads': 30, 'transformer_args': {'checkpoint_activations': True, 'vocab_size': 1, 'max_sequence_length': 64, 'layernorm_order': 'pre', 'skip_init': False, 'model_parallel_size': 1, 'is_decoder': False}, 'modules': {'pos_embed_config': {'target': 'dit_video_concat.Basic3DPositionEmbeddingMixin', 'params': {'text_length': 226, 'height_interpolation': 1.875, 'width_interpolation': 1.875}}, 'lora_config': {'target': 'sat.model.finetune.lora2.LoraMixin', 'params': {'r': 128}}, 'patch_embed_config': {'target': 'dit_video_concat.ImagePatchEmbeddingMixin', 'params': {'text_hidden_size': 4096}}, 'adaln_layer_config': {'target': 'dit_video_concat.AdaLNMixin', 'params': {'qk_ln': True}}, 'final_layer_config': {'target': 'dit_video_concat.FinalLayerMixin'}}}}, 'conditioner_config': {'target': 'sgm.modules.GeneralConditioner', 'params': {'emb_models': [{'is_trainable': False, 'input_key': 'txt', 'ucg_rate': 0.1, 'target': 'sgm.modules.encoders.modules.FrozenT5Embedder', 'params': {'model_dir': '/mnt/ceph/develop/jiawei/model_checkpoint/t5-v1_1-xxl', 'max_length': 2048}}]}}, 'first_stage_config': {'target': 'vae_modules.autoencoder.VideoAutoencoderInferenceWrapper', 'params': {'cp_size': 1, 'ckpt_path': '/mnt/ceph/develop/jiawei/model_checkpoint/CogVideoX-2b-sat/vae/3d-vae.pt', 'ignore_keys': ['loss'], 'loss_config': {'target': 'torch.nn.Identity'}, 'regularizer_config': {'target': 'vae_modules.regularizers.DiagonalGaussianRegularizer'}, 'encoder_config': {'target': 'vae_modules.cp_enc_dec.ContextParallelEncoder3D', 'params': {'double_z': True, 'z_channels': 16, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 2, 4], 'attn_resolutions': [], 'num_res_blocks': 3, 'dropout': 0.0, 'gather_norm': True}}, 'decoder_config': {'target': 'vae_modules.cp_enc_dec.ContextParallelDecoder3D', 'params': {'double_z': True, 'z_channels': 16, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 2, 4], 'attn_resolutions': [], 'num_res_blocks': 3, 'dropout': 0.0, 'gather_norm': False}}}}, 'loss_fn_config': {'target': 'sgm.modules.diffusionmodules.loss.VideoDiffusionLoss', 'params': {'offset_noise_level': 0, 'sigma_sampler_config': {'target': 'sgm.modules.diffusionmodules.sigma_sampling.DiscreteSampling', 'params': {'uniform_sampling': True, 'num_idx': 1000, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 3.0}}}}}}, 'sampler_config': {'target': 'sgm.modules.diffusionmodules.sampling.VPSDEDPMPP2MSampler', 'params': {'num_steps': 50, 'verbose': True, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 3.0}}, 'guider_config': {'target': 'sgm.modules.diffusionmodules.guiders.DynamicCFG', 'params': {'scale': 6, 'exp': 5, 'num_steps': 50}}}}}, data_config={'target': 'data_video.SFTDataset', 'params': {'video_size': [480, 720], 'fps': 8, 'max_num_frames': 49, 'skip_frms_num': 3.0}}, cuda=True, rank=0, world_size=1, deepspeed_activation_checkpointing=True, master_ip='nm04-a800-node083', master_port='57281', log_config=[{'model': {'scale_factor': 1.15258426, 'disable_first_stage_autocast': True, 'not_trainable_prefixes': ['all'], 'log_keys': ['txt'], 'denoiser_config': {'target': 'sgm.modules.diffusionmodules.denoiser.DiscreteDenoiser', 'params': {'num_idx': 1000, 'quantize_c_noise': False, 'weighting_config': {'target': 'sgm.modules.diffusionmodules.denoiser_weighting.EpsWeighting'}, 'scaling_config': {'target': 'sgm.modules.diffusionmodules.denoiser_scaling.VideoScaling'}, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 3.0}}}}, 'network_config': {'target': 'dit_video_concat.DiffusionTransformer', 'params': {'time_embed_dim': 512, 'elementwise_affine': True, 'num_frames': 49, 'time_compressed_rate': 4, 'latent_width': 90, 'latent_height': 60, 'num_layers': 30, 'patch_size': 2, 'in_channels': 16, 'out_channels': 16, 'hidden_size': 1920, 'adm_in_channels': 256, 'num_attention_heads': 30, 'transformer_args': {'checkpoint_activations': True, 'vocab_size': 1, 'max_sequence_length': 64, 'layernorm_order': 'pre', 'skip_init': False, 'model_parallel_size': 1, 'is_decoder': False}, 'modules': {'pos_embed_config': {'target': 'dit_video_concat.Basic3DPositionEmbeddingMixin', 'params': {'text_length': 226, 'height_interpolation': 1.875, 'width_interpolation': 1.875}}, 'lora_config': {'target': 'sat.model.finetune.lora2.LoraMixin', 'params': {'r': 128}}, 'patch_embed_config': {'target': 'dit_video_concat.ImagePatchEmbeddingMixin', 'params': {'text_hidden_size': 4096}}, 'adaln_layer_config': {'target': 'dit_video_concat.AdaLNMixin', 'params': {'qk_ln': True}}, 'final_layer_config': {'target': 'dit_video_concat.FinalLayerMixin'}}}}, 'conditioner_config': {'target': 'sgm.modules.GeneralConditioner', 'params': {'emb_models': [{'is_trainable': False, 'input_key': 'txt', 'ucg_rate': 0.1, 'target': 'sgm.modules.encoders.modules.FrozenT5Embedder', 'params': {'model_dir': '/mnt/ceph/develop/jiawei/model_checkpoint/t5-v1_1-xxl', 'max_length': 2048}}]}}, 'first_stage_config': {'target': 'vae_modules.autoencoder.VideoAutoencoderInferenceWrapper', 'params': {'cp_size': 1, 'ckpt_path': '/mnt/ceph/develop/jiawei/model_checkpoint/CogVideoX-2b-sat/vae/3d-vae.pt', 'ignore_keys': ['loss'], 'loss_config': {'target': 'torch.nn.Identity'}, 'regularizer_config': {'target': 'vae_modules.regularizers.DiagonalGaussianRegularizer'}, 'encoder_config': {'target': 'vae_modules.cp_enc_dec.ContextParallelEncoder3D', 'params': {'double_z': True, 'z_channels': 16, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 2, 4], 'attn_resolutions': [], 'num_res_blocks': 3, 'dropout': 0.0, 'gather_norm': True}}, 'decoder_config': {'target': 'vae_modules.cp_enc_dec.ContextParallelDecoder3D', 'params': {'double_z': True, 'z_channels': 16, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 2, 4], 'attn_resolutions': [], 'num_res_blocks': 3, 'dropout': 0.0, 'gather_norm': False}}}}, 'loss_fn_config': {'target': 'sgm.modules.diffusionmodules.loss.VideoDiffusionLoss', 'params': {'offset_noise_level': 0, 'sigma_sampler_config': {'target': 'sgm.modules.diffusionmodules.sigma_sampling.DiscreteSampling', 'params': {'uniform_sampling': True, 'num_idx': 1000, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 3.0}}}}}}, 'sampler_config': {'target': 'sgm.modules.diffusionmodules.sampling.VPSDEDPMPP2MSampler', 'params': {'num_steps': 50, 'verbose': True, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 3.0}}, 'guider_config': {'target': 'sgm.modules.diffusionmodules.guiders.DynamicCFG', 'params': {'scale': 6, 'exp': 5, 'num_steps': 50}}}}}}, {'args': {'checkpoint_activations': True, 'model_parallel_size': 1, 'experiment_name': 'lora-disney', 'mode': 'finetune', 'load': '/mnt/ceph/develop/jiawei/model_checkpoint/CogVideoX-2b-sat/transformer', 'no_load_rng': True, 'train_iters': 1000, 'eval_iters': 1, 'eval_interval': 100, 'eval_batch_size': 1, 'save': '/mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora', 'save_interval': 100, 'log_interval': 20, 'train_data': ['/mnt/ceph/develop/jiawei/lora_dataset/cogvideo_lora_dataset'], 'valid_data': ['/mnt/ceph/develop/jiawei/lora_dataset/cogvideo_lora_dataset'], 'split': '1,0,0', 'num_workers': 8, 'force_train': True, 'only_log_video_latents': False, 'wandb': True, 'wandb_project_name': 'cogv_2b_lora_base_002', 'summary_dir': '/mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora'}, 'data': {'target': 'data_video.SFTDataset', 'params': {'video_size': [480, 720], 'fps': 8, 'max_num_frames': 49, 'skip_frms_num': 3.0}}, 'deepspeed': {'train_micro_batch_size_per_gpu': 2, 'gradient_accumulation_steps': 1, 'steps_per_print': 50, 'gradient_clipping': 0.1, 'zero_optimization': {'stage': 2, 'cpu_offload': False, 'contiguous_gradients': False, 'overlap_comm': True, 'reduce_scatter': True, 'reduce_bucket_size': 1000000000, 'allgather_bucket_size': 1000000000, 'load_from_fp32_weights': False}, 'zero_allow_untested_optimizer': True, 'bf16': {'enabled': False}, 'fp16': {'enabled': True}, 'loss_scale': 0, 'loss_scale_window': 400, 'hysteresis': 2, 'min_loss_scale': 1, 'optimizer': {'type': 'sat.ops.FusedEmaAdam', 'params': {'lr': '5E-4', 'betas': [0.9, 0.95], 'eps': '5E-8', 'weight_decay': '5E-6'}}, 'activation_checkpointing': {'partition_activations': False, 'contiguous_memory_optimization': False}, 'wall_clock_breakdown': False}}])
[2024-09-09 18:08:18,738] [INFO] [RANK 0] building SATVideoDiffusionEngine model ...
[2024-09-09 18:08:29,294] [WARNING] [RANK 0] Failed to load bitsandbytes:No module named 'bitsandbytes'
[2024-09-09 18:08:29,295] [INFO] [RANK 0] replacing layer 0 attention with lora
[2024-09-09 18:08:29,325] [INFO] [RANK 0] replacing layer 1 attention with lora
[2024-09-09 18:08:29,348] [INFO] [RANK 0] replacing layer 2 attention with lora
[2024-09-09 18:08:29,372] [INFO] [RANK 0] replacing layer 3 attention with lora
[2024-09-09 18:08:29,394] [INFO] [RANK 0] replacing layer 4 attention with lora
[2024-09-09 18:08:29,417] [INFO] [RANK 0] replacing layer 5 attention with lora
[2024-09-09 18:08:29,440] [INFO] [RANK 0] replacing layer 6 attention with lora
[2024-09-09 18:08:29,463] [INFO] [RANK 0] replacing layer 7 attention with lora
[2024-09-09 18:08:29,485] [INFO] [RANK 0] replacing layer 8 attention with lora
[2024-09-09 18:08:29,509] [INFO] [RANK 0] replacing layer 9 attention with lora
[2024-09-09 18:08:29,533] [INFO] [RANK 0] replacing layer 10 attention with lora
[2024-09-09 18:08:29,557] [INFO] [RANK 0] replacing layer 11 attention with lora
[2024-09-09 18:08:29,579] [INFO] [RANK 0] replacing layer 12 attention with lora
[2024-09-09 18:08:29,602] [INFO] [RANK 0] replacing layer 13 attention with lora
[2024-09-09 18:08:29,625] [INFO] [RANK 0] replacing layer 14 attention with lora
[2024-09-09 18:08:29,647] [INFO] [RANK 0] replacing layer 15 attention with lora
[2024-09-09 18:08:29,670] [INFO] [RANK 0] replacing layer 16 attention with lora
[2024-09-09 18:08:29,693] [INFO] [RANK 0] replacing layer 17 attention with lora
[2024-09-09 18:08:29,715] [INFO] [RANK 0] replacing layer 18 attention with lora
[2024-09-09 18:08:29,737] [INFO] [RANK 0] replacing layer 19 attention with lora
[2024-09-09 18:08:29,761] [INFO] [RANK 0] replacing layer 20 attention with lora
[2024-09-09 18:08:29,783] [INFO] [RANK 0] replacing layer 21 attention with lora
[2024-09-09 18:08:29,806] [INFO] [RANK 0] replacing layer 22 attention with lora
[2024-09-09 18:08:29,827] [INFO] [RANK 0] replacing layer 23 attention with lora
[2024-09-09 18:08:29,849] [INFO] [RANK 0] replacing layer 24 attention with lora
[2024-09-09 18:08:29,871] [INFO] [RANK 0] replacing layer 25 attention with lora
[2024-09-09 18:08:29,893] [INFO] [RANK 0] replacing layer 26 attention with lora
[2024-09-09 18:08:29,915] [INFO] [RANK 0] replacing layer 27 attention with lora
[2024-09-09 18:08:29,937] [INFO] [RANK 0] replacing layer 28 attention with lora
[2024-09-09 18:08:29,959] [INFO] [RANK 0] replacing layer 29 attention with lora
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  1.50it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.60it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.59it/s]
Initialized embedder #0: FrozenT5Embedder with 4762310656 params. Trainable: False
Working with z of shape (1, 16, 32, 32) = 16384 dimensions.
/mnt/ceph/develop/jiawei/CogVideo/sat/vae_modules/autoencoder.py:565: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  sd = torch.load(path, map_location="cpu")["state_dict"]
Deleting key loss.logvar from state_dict.
Deleting key loss.perceptual_loss.scaling_layer.shift from state_dict.
Deleting key loss.perceptual_loss.scaling_layer.scale from state_dict.
Deleting key loss.perceptual_loss.net.slice1.0.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice1.0.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice1.2.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice1.2.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice2.5.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice2.5.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice2.7.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice2.7.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice3.10.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice3.10.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice3.12.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice3.12.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice3.14.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice3.14.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice4.17.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice4.17.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice4.19.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice4.19.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice4.21.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice4.21.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice5.24.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice5.24.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice5.26.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice5.26.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice5.28.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice5.28.bias from state_dict.
Deleting key loss.perceptual_loss.lin0.model.1.weight from state_dict.
Deleting key loss.perceptual_loss.lin1.model.1.weight from state_dict.
Deleting key loss.perceptual_loss.lin2.model.1.weight from state_dict.
Deleting key loss.perceptual_loss.lin3.model.1.weight from state_dict.
Deleting key loss.perceptual_loss.lin4.model.1.weight from state_dict.
Deleting key loss.discriminator.blocks.0.downsample_res.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.0.downsample_res.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.0.net.0.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.0.net.0.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.0.net.2.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.0.net.2.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.0.downsample.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.0.downsample.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.1.downsample_res.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.1.downsample_res.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.1.net.0.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.1.net.0.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.1.net.2.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.1.net.2.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.1.downsample.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.1.downsample.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.2.downsample_res.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.2.downsample_res.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.2.net.0.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.2.net.0.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.2.net.2.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.2.net.2.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.2.downsample.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.2.downsample.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.3.downsample_res.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.3.downsample_res.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.3.net.0.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.3.net.0.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.3.net.2.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.3.net.2.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.3.downsample.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.3.downsample.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.4.0.conv_res.weight from state_dict.
Deleting key loss.discriminator.blocks.4.0.conv_res.bias from state_dict.
Deleting key loss.discriminator.blocks.4.0.net.0.weight from state_dict.
Deleting key loss.discriminator.blocks.4.0.net.0.bias from state_dict.
Deleting key loss.discriminator.blocks.4.0.net.2.weight from state_dict.
Deleting key loss.discriminator.blocks.4.0.net.2.bias from state_dict.
Deleting key loss.discriminator.blocks.4.0.downsample.1.weight from state_dict.
Deleting key loss.discriminator.blocks.4.0.downsample.1.bias from state_dict.
Deleting key loss.discriminator.blocks.4.1.0.fn.norm.gamma from state_dict.
Deleting key loss.discriminator.blocks.4.1.0.fn.attn.to_q.0.weight from state_dict.
Deleting key loss.discriminator.blocks.4.1.0.fn.attn.to_kv.0.weight from state_dict.
Deleting key loss.discriminator.blocks.4.1.0.fn.attn.to_out.0.weight from state_dict.
Deleting key loss.discriminator.blocks.4.1.1.fn.norm.gamma from state_dict.
Deleting key loss.discriminator.blocks.4.1.1.fn.net.0.weight from state_dict.
Deleting key loss.discriminator.blocks.4.1.1.fn.net.0.bias from state_dict.
Deleting key loss.discriminator.blocks.4.1.1.fn.net.2.weight from state_dict.
Deleting key loss.discriminator.blocks.4.1.1.fn.net.2.bias from state_dict.
Deleting key loss.discriminator.blocks.5.0.conv_res.weight from state_dict.
Deleting key loss.discriminator.blocks.5.0.conv_res.bias from state_dict.
Deleting key loss.discriminator.blocks.5.0.net.0.weight from state_dict.
Deleting key loss.discriminator.blocks.5.0.net.0.bias from state_dict.
Deleting key loss.discriminator.blocks.5.0.net.2.weight from state_dict.
Deleting key loss.discriminator.blocks.5.0.net.2.bias from state_dict.
Deleting key loss.discriminator.blocks.5.0.downsample.1.weight from state_dict.
Deleting key loss.discriminator.blocks.5.0.downsample.1.bias from state_dict.
Deleting key loss.discriminator.blocks.5.1.0.fn.norm.gamma from state_dict.
Deleting key loss.discriminator.blocks.5.1.0.fn.attn.to_q.0.weight from state_dict.
Deleting key loss.discriminator.blocks.5.1.0.fn.attn.to_kv.0.weight from state_dict.
Deleting key loss.discriminator.blocks.5.1.0.fn.attn.to_out.0.weight from state_dict.
Deleting key loss.discriminator.blocks.5.1.1.fn.norm.gamma from state_dict.
Deleting key loss.discriminator.blocks.5.1.1.fn.net.0.weight from state_dict.
Deleting key loss.discriminator.blocks.5.1.1.fn.net.0.bias from state_dict.
Deleting key loss.discriminator.blocks.5.1.1.fn.net.2.weight from state_dict.
Deleting key loss.discriminator.blocks.5.1.1.fn.net.2.bias from state_dict.
Deleting key loss.discriminator.blocks.6.0.conv_res.weight from state_dict.
Deleting key loss.discriminator.blocks.6.0.conv_res.bias from state_dict.
Deleting key loss.discriminator.blocks.6.0.net.0.weight from state_dict.
Deleting key loss.discriminator.blocks.6.0.net.0.bias from state_dict.
Deleting key loss.discriminator.blocks.6.0.net.2.weight from state_dict.
Deleting key loss.discriminator.blocks.6.0.net.2.bias from state_dict.
Deleting key loss.discriminator.blocks.6.1.0.fn.norm.gamma from state_dict.
Deleting key loss.discriminator.blocks.6.1.0.fn.attn.to_q.0.weight from state_dict.
Deleting key loss.discriminator.blocks.6.1.0.fn.attn.to_kv.0.weight from state_dict.
Deleting key loss.discriminator.blocks.6.1.0.fn.attn.to_out.0.weight from state_dict.
Deleting key loss.discriminator.blocks.6.1.1.fn.norm.gamma from state_dict.
Deleting key loss.discriminator.blocks.6.1.1.fn.net.0.weight from state_dict.
Deleting key loss.discriminator.blocks.6.1.1.fn.net.0.bias from state_dict.
Deleting key loss.discriminator.blocks.6.1.1.fn.net.2.weight from state_dict.
Deleting key loss.discriminator.blocks.6.1.1.fn.net.2.bias from state_dict.
Deleting key loss.discriminator.to_logits.0.weight from state_dict.
Deleting key loss.discriminator.to_logits.0.bias from state_dict.
Deleting key loss.discriminator.to_logits.3.weight from state_dict.
Deleting key loss.discriminator.to_logits.3.bias from state_dict.
Missing keys:  []
Unexpected keys:  []
Restored from /mnt/ceph/develop/jiawei/model_checkpoint/CogVideoX-2b-sat/vae/3d-vae.pt
[2024-09-09 18:08:34,183] [INFO] [RANK 0]  > number of parameters on model parallel rank 0: 6764790755
[2024-09-09 18:08:42,197] [INFO] [RANK 0] global rank 0 is loading checkpoint /mnt/ceph/develop/jiawei/model_checkpoint/CogVideoX-2b-sat/transformer/1000/mp_rank_00_model_states.pt
/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/sat/training/model_io.py:286: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  sd = torch.load(checkpoint_name, map_location='cpu')
[2024-09-09 18:08:44,963] [INFO] [RANK 0] > successfully loaded /mnt/ceph/develop/jiawei/model_checkpoint/CogVideoX-2b-sat/transformer/1000/mp_rank_00_model_states.pt
[2024-09-09 18:08:45,466] [INFO] [RANK 0] ***** Total trainable parameters: 58982400 *****
[2024-09-09 18:08:45,466] [INFO] [RANK 0] [<class 'sat.ops.layernorm.LayerNorm'>, <class 'torch.nn.modules.normalization.LayerNorm'>, <class 'sat.ops.layernorm.RMSNorm'>] is set to no_weight_decay
[2024-09-09 18:08:45,474] [INFO] [RANK 0] Syncing initialized parameters...
[2024-09-09 18:08:45,542] [INFO] [RANK 0] Finished syncing initialized parameters.
[2024-09-09 18:08:45,543] [INFO] [RANK 0] Using optimizer sat.ops.FusedEmaAdam from sat.
[2024-09-09 18:08:45,545] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.4, git-hash=unknown, git-branch=unknown
[2024-09-09 18:08:45,546] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
[2024-09-09 18:08:45,637] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /root/.cache/torch_extensions/py310_cu121/fused_ema_adam/build.ninja...
/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1965: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module fused_ema_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_ema_adam...
Time to load fused_ema_adam op: 3.496824264526367 seconds
[2024-09-09 18:08:49,163] [INFO] [logging.py:96:log_dist] [Rank 0] Using client callable to create basic optimizer
[2024-09-09 18:08:49,165] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-09-09 18:08:49,195] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedEmaAdam
[2024-09-09 18:08:49,195] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedEmaAdam type=<class 'sat.ops.fused_ema_adam.FusedEmaAdam'>
[2024-09-09 18:08:49,199] [WARNING] [engine.py:1179:_do_optimizer_sanity_check] **** You are using ZeRO with an untested optimizer, proceed with caution *****
[2024-09-09 18:08:49,199] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2024-09-09 18:08:49,199] [INFO] [stage_1_and_2.py:148:__init__] Reduce bucket size 1000000000
[2024-09-09 18:08:49,199] [INFO] [stage_1_and_2.py:149:__init__] Allgather bucket size 1000000000
[2024-09-09 18:08:49,199] [INFO] [stage_1_and_2.py:150:__init__] CPU Offload: False
[2024-09-09 18:08:49,199] [INFO] [stage_1_and_2.py:151:__init__] Round robin gradient partitioning: False
[2024-09-09 18:08:49,462] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2024-09-09 18:08:49,463] [INFO] [utils.py:782:see_memory_usage] MA 12.86 GB         Max_MA 12.97 GB         CA 13.23 GB         Max_CA 13 GB 
[2024-09-09 18:08:49,465] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 121.27 GB, percent = 6.0%
[2024-09-09 18:08:49,607] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2024-09-09 18:08:49,608] [INFO] [utils.py:782:see_memory_usage] MA 12.86 GB         Max_MA 13.08 GB         CA 13.45 GB         Max_CA 13 GB 
[2024-09-09 18:08:49,610] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 121.26 GB, percent = 6.0%
[2024-09-09 18:08:49,610] [INFO] [stage_1_and_2.py:543:__init__] optimizer state initialized
[2024-09-09 18:08:49,749] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2024-09-09 18:08:49,750] [INFO] [utils.py:782:see_memory_usage] MA 12.86 GB         Max_MA 12.86 GB         CA 13.45 GB         Max_CA 13 GB 
[2024-09-09 18:08:49,753] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 121.26 GB, percent = 6.0%
[2024-09-09 18:08:49,757] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer
[2024-09-09 18:08:49,757] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2024-09-09 18:08:49,763] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2024-09-09 18:08:49,763] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[1.0], mom=[[0.9, 0.95]]
[2024-09-09 18:08:49,765] [INFO] [config.py:997:print] DeepSpeedEngine configuration:
[2024-09-09 18:08:49,765] [INFO] [config.py:1001:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-09-09 18:08:49,768] [INFO] [config.py:1001:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-09-09 18:08:49,768] [INFO] [config.py:1001:print]   amp_enabled .................. False
[2024-09-09 18:08:49,768] [INFO] [config.py:1001:print]   amp_params ................... False
[2024-09-09 18:08:49,768] [INFO] [config.py:1001:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-09-09 18:08:49,768] [INFO] [config.py:1001:print]   bfloat16_enabled ............. False
[2024-09-09 18:08:49,770] [INFO] [config.py:1001:print]   bfloat16_immediate_grad_update  False
[2024-09-09 18:08:49,770] [INFO] [config.py:1001:print]   checkpoint_parallel_write_pipeline  False
[2024-09-09 18:08:49,770] [INFO] [config.py:1001:print]   checkpoint_tag_validation_enabled  True
[2024-09-09 18:08:49,770] [INFO] [config.py:1001:print]   checkpoint_tag_validation_fail  False
[2024-09-09 18:08:49,770] [INFO] [config.py:1001:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f6ff8377ac0>
[2024-09-09 18:08:49,770] [INFO] [config.py:1001:print]   communication_data_type ...... None
[2024-09-09 18:08:49,771] [INFO] [config.py:1001:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-09-09 18:08:49,771] [INFO] [config.py:1001:print]   curriculum_enabled_legacy .... False
[2024-09-09 18:08:49,771] [INFO] [config.py:1001:print]   curriculum_params_legacy ..... False
[2024-09-09 18:08:49,771] [INFO] [config.py:1001:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-09-09 18:08:49,771] [INFO] [config.py:1001:print]   data_efficiency_enabled ...... False
[2024-09-09 18:08:49,771] [INFO] [config.py:1001:print]   dataloader_drop_last ......... False
[2024-09-09 18:08:49,773] [INFO] [config.py:1001:print]   disable_allgather ............ False
[2024-09-09 18:08:49,773] [INFO] [config.py:1001:print]   dump_state ................... False
[2024-09-09 18:08:49,773] [INFO] [config.py:1001:print]   dynamic_loss_scale_args ...... None
[2024-09-09 18:08:49,773] [INFO] [config.py:1001:print]   eigenvalue_enabled ........... False
[2024-09-09 18:08:49,773] [INFO] [config.py:1001:print]   eigenvalue_gas_boundary_resolution  1
[2024-09-09 18:08:49,773] [INFO] [config.py:1001:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-09-09 18:08:49,773] [INFO] [config.py:1001:print]   eigenvalue_layer_num ......... 0
[2024-09-09 18:08:49,773] [INFO] [config.py:1001:print]   eigenvalue_max_iter .......... 100
[2024-09-09 18:08:49,773] [INFO] [config.py:1001:print]   eigenvalue_stability ......... 1e-06
[2024-09-09 18:08:49,773] [INFO] [config.py:1001:print]   eigenvalue_tol ............... 0.01
[2024-09-09 18:08:49,773] [INFO] [config.py:1001:print]   eigenvalue_verbose ........... False
[2024-09-09 18:08:49,773] [INFO] [config.py:1001:print]   elasticity_enabled ........... False
[2024-09-09 18:08:49,773] [INFO] [config.py:1001:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-09-09 18:08:49,775] [INFO] [config.py:1001:print]   fp16_auto_cast ............... False
[2024-09-09 18:08:49,775] [INFO] [config.py:1001:print]   fp16_enabled ................. True
[2024-09-09 18:08:49,775] [INFO] [config.py:1001:print]   fp16_master_weights_and_gradients  False
[2024-09-09 18:08:49,775] [INFO] [config.py:1001:print]   global_rank .................. 0
[2024-09-09 18:08:49,775] [INFO] [config.py:1001:print]   grad_accum_dtype ............. None
[2024-09-09 18:08:49,775] [INFO] [config.py:1001:print]   gradient_accumulation_steps .. 1
[2024-09-09 18:08:49,775] [INFO] [config.py:1001:print]   gradient_clipping ............ 0.1
[2024-09-09 18:08:49,775] [INFO] [config.py:1001:print]   gradient_predivide_factor .... 1.0
[2024-09-09 18:08:49,775] [INFO] [config.py:1001:print]   graph_harvesting ............. False
[2024-09-09 18:08:49,775] [INFO] [config.py:1001:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-09-09 18:08:49,775] [INFO] [config.py:1001:print]   initial_dynamic_scale ........ 65536
[2024-09-09 18:08:49,775] [INFO] [config.py:1001:print]   load_universal_checkpoint .... False
[2024-09-09 18:08:49,775] [INFO] [config.py:1001:print]   loss_scale ................... 0
[2024-09-09 18:08:49,775] [INFO] [config.py:1001:print]   memory_breakdown ............. False
[2024-09-09 18:08:49,775] [INFO] [config.py:1001:print]   mics_hierarchial_params_gather  False
[2024-09-09 18:08:49,775] [INFO] [config.py:1001:print]   mics_shard_size .............. -1
[2024-09-09 18:08:49,775] [INFO] [config.py:1001:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-09-09 18:08:49,775] [INFO] [config.py:1001:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-09-09 18:08:49,775] [INFO] [config.py:1001:print]   optimizer_legacy_fusion ...... False
[2024-09-09 18:08:49,775] [INFO] [config.py:1001:print]   optimizer_name ............... None
[2024-09-09 18:08:49,775] [INFO] [config.py:1001:print]   optimizer_params ............. None
[2024-09-09 18:08:49,775] [INFO] [config.py:1001:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-09-09 18:08:49,777] [INFO] [config.py:1001:print]   pld_enabled .................. False
[2024-09-09 18:08:49,777] [INFO] [config.py:1001:print]   pld_params ................... False
[2024-09-09 18:08:49,777] [INFO] [config.py:1001:print]   prescale_gradients ........... False
[2024-09-09 18:08:49,777] [INFO] [config.py:1001:print]   scheduler_name ............... None
[2024-09-09 18:08:49,777] [INFO] [config.py:1001:print]   scheduler_params ............. None
[2024-09-09 18:08:49,777] [INFO] [config.py:1001:print]   seq_parallel_communication_data_type  torch.float32
[2024-09-09 18:08:49,778] [INFO] [config.py:1001:print]   sparse_attention ............. None
[2024-09-09 18:08:49,778] [INFO] [config.py:1001:print]   sparse_gradients_enabled ..... False
[2024-09-09 18:08:49,778] [INFO] [config.py:1001:print]   steps_per_print .............. 50
[2024-09-09 18:08:49,778] [INFO] [config.py:1001:print]   timers_config ................ enabled=True synchronized=True
[2024-09-09 18:08:49,778] [INFO] [config.py:1001:print]   train_batch_size ............. 2
[2024-09-09 18:08:49,778] [INFO] [config.py:1001:print]   train_micro_batch_size_per_gpu  2
[2024-09-09 18:08:49,778] [INFO] [config.py:1001:print]   use_data_before_expert_parallel_  False
[2024-09-09 18:08:49,778] [INFO] [config.py:1001:print]   use_node_local_storage ....... False
[2024-09-09 18:08:49,778] [INFO] [config.py:1001:print]   wall_clock_breakdown ......... False
[2024-09-09 18:08:49,780] [INFO] [config.py:1001:print]   weight_quantization_config ... None
[2024-09-09 18:08:49,780] [INFO] [config.py:1001:print]   world_size ................... 1
[2024-09-09 18:08:49,780] [INFO] [config.py:1001:print]   zero_allow_untested_optimizer  True
[2024-09-09 18:08:49,780] [INFO] [config.py:1001:print]   zero_config .................. stage=2 contiguous_gradients=False reduce_scatter=True reduce_bucket_size=1000000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=1000000000 overlap_comm=True load_from_fp32_weights=False elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-09-09 18:08:49,780] [INFO] [config.py:1001:print]   zero_enabled ................. True
[2024-09-09 18:08:49,780] [INFO] [config.py:1001:print]   zero_force_ds_cpu_optimizer .. True
[2024-09-09 18:08:49,780] [INFO] [config.py:1001:print]   zero_optimization_stage ...... 2
[2024-09-09 18:08:49,780] [INFO] [config.py:987:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 2, 
    "gradient_accumulation_steps": 1, 
    "steps_per_print": 50, 
    "gradient_clipping": 0.1, 
    "zero_optimization": {
        "stage": 2, 
        "cpu_offload": false, 
        "contiguous_gradients": false, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 1.000000e+09, 
        "allgather_bucket_size": 1.000000e+09, 
        "load_from_fp32_weights": false
    }, 
    "zero_allow_untested_optimizer": true, 
    "bf16": {
        "enabled": false
    }, 
    "fp16": {
        "enabled": true
    }, 
    "loss_scale": 0, 
    "loss_scale_window": 400, 
    "hysteresis": 2, 
    "min_loss_scale": 1, 
    "activation_checkpointing": {
        "partition_activations": false, 
        "contiguous_memory_optimization": false
    }, 
    "wall_clock_breakdown": false
}
[2024-09-09 18:08:49,780] [INFO] [RANK 0] learning rate decaying style linear, ratio 10.0
[2024-09-09 18:08:49,782] [INFO] [RANK 0] Finetuning Model...
[2024-09-09 18:08:49,782] [INFO] [RANK 0] arguments:
[2024-09-09 18:08:49,782] [INFO] [RANK 0]   base ......................... ['configs/cogvideox_2b_lora.yaml', 'configs/sft.yaml']
[2024-09-09 18:08:49,783] [INFO] [RANK 0]   model_parallel_size .......... 1
[2024-09-09 18:08:49,783] [INFO] [RANK 0]   force_pretrain ............... False
[2024-09-09 18:08:49,783] [INFO] [RANK 0]   device ....................... 0
[2024-09-09 18:08:49,783] [INFO] [RANK 0]   debug ........................ False
[2024-09-09 18:08:49,783] [INFO] [RANK 0]   log_image .................... True
[2024-09-09 18:08:49,783] [INFO] [RANK 0]   output_dir ................... samples
[2024-09-09 18:08:49,783] [INFO] [RANK 0]   input_dir .................... None
[2024-09-09 18:08:49,785] [INFO] [RANK 0]   input_type ................... cli
[2024-09-09 18:08:49,785] [INFO] [RANK 0]   input_file ................... input.txt
[2024-09-09 18:08:49,785] [INFO] [RANK 0]   final_size ................... 2048
[2024-09-09 18:08:49,785] [INFO] [RANK 0]   sdedit ....................... False
[2024-09-09 18:08:49,785] [INFO] [RANK 0]   grid_num_rows ................ 1
[2024-09-09 18:08:49,785] [INFO] [RANK 0]   force_inference .............. False
[2024-09-09 18:08:49,785] [INFO] [RANK 0]   lcm_steps .................... None
[2024-09-09 18:08:49,785] [INFO] [RANK 0]   sampling_num_frames .......... 32
[2024-09-09 18:08:49,785] [INFO] [RANK 0]   sampling_fps ................. 8
[2024-09-09 18:08:49,785] [INFO] [RANK 0]   only_save_latents ............ False
[2024-09-09 18:08:49,785] [INFO] [RANK 0]   only_log_video_latents ....... False
[2024-09-09 18:08:49,785] [INFO] [RANK 0]   latent_channels .............. 32
[2024-09-09 18:08:49,787] [INFO] [RANK 0]   image2video .................. False
[2024-09-09 18:08:49,788] [INFO] [RANK 0]   experiment_name .............. lora-disney-09-09-18-08
[2024-09-09 18:08:49,788] [INFO] [RANK 0]   train_iters .................. 1000
[2024-09-09 18:08:49,788] [INFO] [RANK 0]   batch_size ................... 2
[2024-09-09 18:08:49,788] [INFO] [RANK 0]   lr ........................... 0.0005
[2024-09-09 18:08:49,788] [INFO] [RANK 0]   mode ......................... finetune
[2024-09-09 18:08:49,788] [INFO] [RANK 0]   seed ......................... 42
[2024-09-09 18:08:49,788] [INFO] [RANK 0]   zero_stage ................... 0
[2024-09-09 18:08:49,788] [INFO] [RANK 0]   checkpoint_activations ....... True
[2024-09-09 18:08:49,788] [INFO] [RANK 0]   checkpoint_num_layers ........ 1
[2024-09-09 18:08:49,788] [INFO] [RANK 0]   checkpoint_skip_layers ....... 0
[2024-09-09 18:08:49,790] [INFO] [RANK 0]   fp16 ......................... True
[2024-09-09 18:08:49,790] [INFO] [RANK 0]   bf16 ......................... False
[2024-09-09 18:08:49,790] [INFO] [RANK 0]   gradient_accumulation_steps .. 1
[2024-09-09 18:08:49,790] [INFO] [RANK 0]   profiling .................... -1
[2024-09-09 18:08:49,790] [INFO] [RANK 0]   epochs ....................... None
[2024-09-09 18:08:49,790] [INFO] [RANK 0]   log_interval ................. 20
[2024-09-09 18:08:49,790] [INFO] [RANK 0]   summary_dir .................. /mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora
[2024-09-09 18:08:49,790] [INFO] [RANK 0]   save_args .................... False
[2024-09-09 18:08:49,790] [INFO] [RANK 0]   lr_decay_iters ............... None
[2024-09-09 18:08:49,790] [INFO] [RANK 0]   lr_decay_style ............... linear
[2024-09-09 18:08:49,790] [INFO] [RANK 0]   lr_decay_ratio ............... 0.1
[2024-09-09 18:08:49,790] [INFO] [RANK 0]   warmup ....................... 0.01
[2024-09-09 18:08:49,790] [INFO] [RANK 0]   weight_decay ................. 5e-06
[2024-09-09 18:08:49,793] [INFO] [RANK 0]   save ......................... /mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora/lora-disney-09-09-18-08
[2024-09-09 18:08:49,793] [INFO] [RANK 0]   load ......................... /mnt/ceph/develop/jiawei/model_checkpoint/CogVideoX-2b-sat/transformer
[2024-09-09 18:08:49,793] [INFO] [RANK 0]   force_train .................. True
[2024-09-09 18:08:49,793] [INFO] [RANK 0]   save_interval ................ 100
[2024-09-09 18:08:49,793] [INFO] [RANK 0]   no_save_rng .................. False
[2024-09-09 18:08:49,793] [INFO] [RANK 0]   no_load_rng .................. True
[2024-09-09 18:08:49,793] [INFO] [RANK 0]   resume_dataloader ............ False
[2024-09-09 18:08:49,794] [INFO] [RANK 0]   distributed_backend .......... nccl
[2024-09-09 18:08:49,794] [INFO] [RANK 0]   local_rank ................... 0
[2024-09-09 18:08:49,794] [INFO] [RANK 0]   exit_interval ................ None
[2024-09-09 18:08:49,794] [INFO] [RANK 0]   wandb ........................ True
[2024-09-09 18:08:49,794] [INFO] [RANK 0]   wandb_project_name ........... cogv_2b_lora_base_002
[2024-09-09 18:08:49,796] [INFO] [RANK 0]   eval_batch_size .............. 1
[2024-09-09 18:08:49,796] [INFO] [RANK 0]   eval_iters ................... 1
[2024-09-09 18:08:49,796] [INFO] [RANK 0]   eval_interval ................ 100
[2024-09-09 18:08:49,796] [INFO] [RANK 0]   strict_eval .................. False
[2024-09-09 18:08:49,796] [INFO] [RANK 0]   train_data ................... ['/mnt/ceph/develop/jiawei/lora_dataset/cogvideo_lora_dataset']
[2024-09-09 18:08:49,796] [INFO] [RANK 0]   train_data_weights ........... None
[2024-09-09 18:08:49,796] [INFO] [RANK 0]   iterable_dataset ............. False
[2024-09-09 18:08:49,796] [INFO] [RANK 0]   iterable_dataset_eval ........ 
[2024-09-09 18:08:49,796] [INFO] [RANK 0]   batch_from_same_dataset ...... False
[2024-09-09 18:08:49,796] [INFO] [RANK 0]   valid_data ................... ['/mnt/ceph/develop/jiawei/lora_dataset/cogvideo_lora_dataset']
[2024-09-09 18:08:49,798] [INFO] [RANK 0]   test_data .................... None
[2024-09-09 18:08:49,798] [INFO] [RANK 0]   split ........................ 1,0,0
[2024-09-09 18:08:49,798] [INFO] [RANK 0]   num_workers .................. 8
[2024-09-09 18:08:49,798] [INFO] [RANK 0]   block_size ................... 10000
[2024-09-09 18:08:49,798] [INFO] [RANK 0]   prefetch_factor .............. 4
[2024-09-09 18:08:49,798] [INFO] [RANK 0]   deepspeed .................... True
[2024-09-09 18:08:49,798] [INFO] [RANK 0]   deepspeed_config ............. {'train_micro_batch_size_per_gpu': 2, 'gradient_accumulation_steps': 1, 'steps_per_print': 50, 'gradient_clipping': 0.1, 'zero_optimization': {'stage': 2, 'cpu_offload': False, 'contiguous_gradients': False, 'overlap_comm': True, 'reduce_scatter': True, 'reduce_bucket_size': 1000000000, 'allgather_bucket_size': 1000000000, 'load_from_fp32_weights': False}, 'zero_allow_untested_optimizer': True, 'bf16': {'enabled': False}, 'fp16': {'enabled': True}, 'loss_scale': 0, 'loss_scale_window': 400, 'hysteresis': 2, 'min_loss_scale': 1, 'activation_checkpointing': {'partition_activations': False, 'contiguous_memory_optimization': False}, 'wall_clock_breakdown': False}
[2024-09-09 18:08:49,798] [INFO] [RANK 0]   deepscale .................... False
[2024-09-09 18:08:49,798] [INFO] [RANK 0]   deepscale_config ............. None
[2024-09-09 18:08:49,800] [INFO] [RANK 0]   model_config ................. {'scale_factor': 1.15258426, 'disable_first_stage_autocast': True, 'not_trainable_prefixes': ['all'], 'log_keys': ['txt'], 'denoiser_config': {'target': 'sgm.modules.diffusionmodules.denoiser.DiscreteDenoiser', 'params': {'num_idx': 1000, 'quantize_c_noise': False, 'weighting_config': {'target': 'sgm.modules.diffusionmodules.denoiser_weighting.EpsWeighting'}, 'scaling_config': {'target': 'sgm.modules.diffusionmodules.denoiser_scaling.VideoScaling'}, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 3.0}}}}, 'network_config': {'target': 'dit_video_concat.DiffusionTransformer', 'params': {'time_embed_dim': 512, 'elementwise_affine': True, 'num_frames': 49, 'time_compressed_rate': 4, 'latent_width': 90, 'latent_height': 60, 'num_layers': 30, 'patch_size': 2, 'in_channels': 16, 'out_channels': 16, 'hidden_size': 1920, 'adm_in_channels': 256, 'num_attention_heads': 30, 'transformer_args': {'checkpoint_activations': True, 'vocab_size': 1, 'max_sequence_length': 64, 'layernorm_order': 'pre', 'skip_init': False, 'model_parallel_size': 1, 'is_decoder': False, 'num_layers': 30, 'hidden_size': 1920, 'num_attention_heads': 30, 'parallel_output': True}, 'modules': {'pos_embed_config': {'target': 'dit_video_concat.Basic3DPositionEmbeddingMixin', 'params': {'text_length': 226, 'height_interpolation': 1.875, 'width_interpolation': 1.875}}, 'lora_config': {'target': 'sat.model.finetune.lora2.LoraMixin', 'params': {'r': 128}}, 'patch_embed_config': {'target': 'dit_video_concat.ImagePatchEmbeddingMixin', 'params': {'text_hidden_size': 4096}}, 'adaln_layer_config': {'target': 'dit_video_concat.AdaLNMixin', 'params': {'qk_ln': True}}, 'final_layer_config': {'target': 'dit_video_concat.FinalLayerMixin'}}, 'dtype': 'fp16'}}, 'conditioner_config': {'target': 'sgm.modules.GeneralConditioner', 'params': {'emb_models': [{'is_trainable': False, 'input_key': 'txt', 'ucg_rate': 0.1, 'target': 'sgm.modules.encoders.modules.FrozenT5Embedder', 'params': {'model_dir': '/mnt/ceph/develop/jiawei/model_checkpoint/t5-v1_1-xxl', 'max_length': 2048}}]}}, 'first_stage_config': {'target': 'vae_modules.autoencoder.VideoAutoencoderInferenceWrapper', 'params': {'cp_size': 1, 'ckpt_path': '/mnt/ceph/develop/jiawei/model_checkpoint/CogVideoX-2b-sat/vae/3d-vae.pt', 'ignore_keys': ['loss'], 'loss_config': {'target': 'torch.nn.Identity'}, 'regularizer_config': {'target': 'vae_modules.regularizers.DiagonalGaussianRegularizer'}, 'encoder_config': {'target': 'vae_modules.cp_enc_dec.ContextParallelEncoder3D', 'params': {'double_z': True, 'z_channels': 16, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 2, 4], 'attn_resolutions': [], 'num_res_blocks': 3, 'dropout': 0.0, 'gather_norm': True}}, 'decoder_config': {'target': 'vae_modules.cp_enc_dec.ContextParallelDecoder3D', 'params': {'double_z': True, 'z_channels': 16, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 2, 4], 'attn_resolutions': [], 'num_res_blocks': 3, 'dropout': 0.0, 'gather_norm': False}}}}, 'loss_fn_config': {'target': 'sgm.modules.diffusionmodules.loss.VideoDiffusionLoss', 'params': {'offset_noise_level': 0, 'sigma_sampler_config': {'target': 'sgm.modules.diffusionmodules.sigma_sampling.DiscreteSampling', 'params': {'uniform_sampling': True, 'num_idx': 1000, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 3.0}}}}}}, 'sampler_config': {'target': 'sgm.modules.diffusionmodules.sampling.VPSDEDPMPP2MSampler', 'params': {'num_steps': 50, 'verbose': True, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 3.0}}, 'guider_config': {'target': 'sgm.modules.diffusionmodules.guiders.DynamicCFG', 'params': {'scale': 6, 'exp': 5, 'num_steps': 50}}}}}
[2024-09-09 18:08:49,800] [INFO] [RANK 0]   data_config .................. {'target': 'data_video.SFTDataset', 'params': {'video_size': [480, 720], 'fps': 8, 'max_num_frames': 49, 'skip_frms_num': 3.0}}
[2024-09-09 18:08:49,800] [INFO] [RANK 0]   cuda ......................... True
[2024-09-09 18:08:49,800] [INFO] [RANK 0]   rank ......................... 0
[2024-09-09 18:08:49,801] [INFO] [RANK 0]   world_size ................... 1
[2024-09-09 18:08:49,801] [INFO] [RANK 0]   deepspeed_activation_checkpointing  True
[2024-09-09 18:08:49,801] [INFO] [RANK 0]   master_ip .................... nm04-a800-node083
[2024-09-09 18:08:49,801] [INFO] [RANK 0]   master_port .................. 57281
[2024-09-09 18:08:49,803] [INFO] [RANK 0]   log_config ................... [{'model': {'scale_factor': 1.15258426, 'disable_first_stage_autocast': True, 'not_trainable_prefixes': ['all'], 'log_keys': ['txt'], 'denoiser_config': {'target': 'sgm.modules.diffusionmodules.denoiser.DiscreteDenoiser', 'params': {'num_idx': 1000, 'quantize_c_noise': False, 'weighting_config': {'target': 'sgm.modules.diffusionmodules.denoiser_weighting.EpsWeighting'}, 'scaling_config': {'target': 'sgm.modules.diffusionmodules.denoiser_scaling.VideoScaling'}, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 3.0}}}}, 'network_config': {'target': 'dit_video_concat.DiffusionTransformer', 'params': {'time_embed_dim': 512, 'elementwise_affine': True, 'num_frames': 49, 'time_compressed_rate': 4, 'latent_width': 90, 'latent_height': 60, 'num_layers': 30, 'patch_size': 2, 'in_channels': 16, 'out_channels': 16, 'hidden_size': 1920, 'adm_in_channels': 256, 'num_attention_heads': 30, 'transformer_args': {'checkpoint_activations': True, 'vocab_size': 1, 'max_sequence_length': 64, 'layernorm_order': 'pre', 'skip_init': False, 'model_parallel_size': 1, 'is_decoder': False}, 'modules': {'pos_embed_config': {'target': 'dit_video_concat.Basic3DPositionEmbeddingMixin', 'params': {'text_length': 226, 'height_interpolation': 1.875, 'width_interpolation': 1.875}}, 'lora_config': {'target': 'sat.model.finetune.lora2.LoraMixin', 'params': {'r': 128}}, 'patch_embed_config': {'target': 'dit_video_concat.ImagePatchEmbeddingMixin', 'params': {'text_hidden_size': 4096}}, 'adaln_layer_config': {'target': 'dit_video_concat.AdaLNMixin', 'params': {'qk_ln': True}}, 'final_layer_config': {'target': 'dit_video_concat.FinalLayerMixin'}}}}, 'conditioner_config': {'target': 'sgm.modules.GeneralConditioner', 'params': {'emb_models': [{'is_trainable': False, 'input_key': 'txt', 'ucg_rate': 0.1, 'target': 'sgm.modules.encoders.modules.FrozenT5Embedder', 'params': {'model_dir': '/mnt/ceph/develop/jiawei/model_checkpoint/t5-v1_1-xxl', 'max_length': 2048}}]}}, 'first_stage_config': {'target': 'vae_modules.autoencoder.VideoAutoencoderInferenceWrapper', 'params': {'cp_size': 1, 'ckpt_path': '/mnt/ceph/develop/jiawei/model_checkpoint/CogVideoX-2b-sat/vae/3d-vae.pt', 'ignore_keys': ['loss'], 'loss_config': {'target': 'torch.nn.Identity'}, 'regularizer_config': {'target': 'vae_modules.regularizers.DiagonalGaussianRegularizer'}, 'encoder_config': {'target': 'vae_modules.cp_enc_dec.ContextParallelEncoder3D', 'params': {'double_z': True, 'z_channels': 16, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 2, 4], 'attn_resolutions': [], 'num_res_blocks': 3, 'dropout': 0.0, 'gather_norm': True}}, 'decoder_config': {'target': 'vae_modules.cp_enc_dec.ContextParallelDecoder3D', 'params': {'double_z': True, 'z_channels': 16, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 2, 4], 'attn_resolutions': [], 'num_res_blocks': 3, 'dropout': 0.0, 'gather_norm': False}}}}, 'loss_fn_config': {'target': 'sgm.modules.diffusionmodules.loss.VideoDiffusionLoss', 'params': {'offset_noise_level': 0, 'sigma_sampler_config': {'target': 'sgm.modules.diffusionmodules.sigma_sampling.DiscreteSampling', 'params': {'uniform_sampling': True, 'num_idx': 1000, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 3.0}}}}}}, 'sampler_config': {'target': 'sgm.modules.diffusionmodules.sampling.VPSDEDPMPP2MSampler', 'params': {'num_steps': 50, 'verbose': True, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 3.0}}, 'guider_config': {'target': 'sgm.modules.diffusionmodules.guiders.DynamicCFG', 'params': {'scale': 6, 'exp': 5, 'num_steps': 50}}}}}}, {'args': {'checkpoint_activations': True, 'model_parallel_size': 1, 'experiment_name': 'lora-disney', 'mode': 'finetune', 'load': '/mnt/ceph/develop/jiawei/model_checkpoint/CogVideoX-2b-sat/transformer', 'no_load_rng': True, 'train_iters': 1000, 'eval_iters': 1, 'eval_interval': 100, 'eval_batch_size': 1, 'save': '/mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora', 'save_interval': 100, 'log_interval': 20, 'train_data': ['/mnt/ceph/develop/jiawei/lora_dataset/cogvideo_lora_dataset'], 'valid_data': ['/mnt/ceph/develop/jiawei/lora_dataset/cogvideo_lora_dataset'], 'split': '1,0,0', 'num_workers': 8, 'force_train': True, 'only_log_video_latents': False, 'wandb': True, 'wandb_project_name': 'cogv_2b_lora_base_002', 'summary_dir': '/mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora'}, 'data': {'target': 'data_video.SFTDataset', 'params': {'video_size': [480, 720], 'fps': 8, 'max_num_frames': 49, 'skip_frms_num': 3.0}}, 'deepspeed': {'train_micro_batch_size_per_gpu': 2, 'gradient_accumulation_steps': 1, 'steps_per_print': 50, 'gradient_clipping': 0.1, 'zero_optimization': {'stage': 2, 'cpu_offload': False, 'contiguous_gradients': False, 'overlap_comm': True, 'reduce_scatter': True, 'reduce_bucket_size': 1000000000, 'allgather_bucket_size': 1000000000, 'load_from_fp32_weights': False}, 'zero_allow_untested_optimizer': True, 'bf16': {'enabled': False}, 'fp16': {'enabled': True}, 'loss_scale': 0, 'loss_scale_window': 400, 'hysteresis': 2, 'min_loss_scale': 1, 'optimizer': {'type': 'sat.ops.FusedEmaAdam', 'params': {'lr': '5E-4', 'betas': [0.9, 0.95], 'eps': '5E-8', 'weight_decay': '5E-6'}}, 'activation_checkpointing': {'partition_activations': False, 'contiguous_memory_optimization': False}, 'wall_clock_breakdown': False}}]
[2024-09-09 18:08:49,803] [INFO] [RANK 0]   do_train ..................... True
[2024-09-09 18:08:49,803] [INFO] [RANK 0]   val_last_shape ............... []
[2024-09-09 18:08:49,803] [INFO] [RANK 0]   val_drop_number .............. 0
[2024-09-09 18:08:49,803] [INFO] [RANK 0]   do_valid ..................... True
[2024-09-09 18:08:49,803] [INFO] [RANK 0]   do_test ...................... False
[2024-09-09 18:08:49,806] [INFO] [RANK 0]   iteration .................... 0
wandb: Currently logged in as: dmeck. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora/wandb/run-20240909_180853-c3qk8axv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lora-disney-09-09-18-08
wandb: â­ï¸ View project at https://wandb.ai/dmeck/cogv_2b_lora_base_002
wandb: ðŸš€ View run at https://wandb.ai/dmeck/cogv_2b_lora_base_002/runs/c3qk8axv
Traceback (most recent call last):
  File "/mnt/ceph/develop/jiawei/CogVideo/sat/train_video.py", line 223, in <module>
    training_main(
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/sat/training/deepspeed_training.py", line 157, in training_main
    iteration, skipped = train(model, optimizer,
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/sat/training/deepspeed_training.py", line 359, in train
    lm_loss, skipped_iter, metrics = train_step(train_data_iterator,
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/sat/training/deepspeed_training.py", line 443, in train_step
    forward_ret = forward_step(data_iterator, model, args, timers, **kwargs)
  File "/mnt/ceph/develop/jiawei/CogVideo/sat/train_video.py", line 195, in forward_step
    loss, loss_dict = model.shared_step(batch)
  File "/mnt/ceph/develop/jiawei/CogVideo/sat/diffusion_video.py", line 147, in shared_step
    loss, loss_dict = self(x, batch)
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/ceph/develop/jiawei/CogVideo/sat/diffusion_video.py", line 128, in forward
    loss = self.loss_fn(self.model, self.denoiser, self.conditioner, x, batch)
  File "/mnt/ceph/develop/jiawei/CogVideo/sat/sgm/modules/diffusionmodules/loss.py", line 110, in __call__
    model_output = denoiser(network, noised_input, alphas_cumprod_sqrt, cond, **additional_model_inputs)
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/ceph/develop/jiawei/CogVideo/sat/sgm/modules/diffusionmodules/denoiser.py", line 38, in forward
    return network(input * c_in, c_noise, cond, **additional_model_inputs) * c_out + input * c_skip
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/ceph/develop/jiawei/CogVideo/sat/sgm/modules/diffusionmodules/wrappers.py", line 35, in forward
    return self.diffusion_model(
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/ceph/develop/jiawei/CogVideo/sat/dit_video_concat.py", line 770, in forward
    output = super().forward(**kwargs)[0]
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/sat/model/base_model.py", line 138, in forward
    return self.transformer(*args, **kwargs)
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/sat/model/transformer.py", line 608, in forward
    hidden_states = hidden_states + position_embeddings
RuntimeError: The size of tensor a (19598) must match the size of tensor b (17776) at non-singleton dimension 1
[rank0]: Traceback (most recent call last):
[rank0]:   File "/mnt/ceph/develop/jiawei/CogVideo/sat/train_video.py", line 223, in <module>
[rank0]:     training_main(
[rank0]:   File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/sat/training/deepspeed_training.py", line 157, in training_main
[rank0]:     iteration, skipped = train(model, optimizer,
[rank0]:   File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/sat/training/deepspeed_training.py", line 359, in train
[rank0]:     lm_loss, skipped_iter, metrics = train_step(train_data_iterator,
[rank0]:   File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/sat/training/deepspeed_training.py", line 443, in train_step
[rank0]:     forward_ret = forward_step(data_iterator, model, args, timers, **kwargs)
[rank0]:   File "/mnt/ceph/develop/jiawei/CogVideo/sat/train_video.py", line 195, in forward_step
[rank0]:     loss, loss_dict = model.shared_step(batch)
[rank0]:   File "/mnt/ceph/develop/jiawei/CogVideo/sat/diffusion_video.py", line 147, in shared_step
[rank0]:     loss, loss_dict = self(x, batch)
[rank0]:   File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/mnt/ceph/develop/jiawei/CogVideo/sat/diffusion_video.py", line 128, in forward
[rank0]:     loss = self.loss_fn(self.model, self.denoiser, self.conditioner, x, batch)
[rank0]:   File "/mnt/ceph/develop/jiawei/CogVideo/sat/sgm/modules/diffusionmodules/loss.py", line 110, in __call__
[rank0]:     model_output = denoiser(network, noised_input, alphas_cumprod_sqrt, cond, **additional_model_inputs)
[rank0]:   File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/mnt/ceph/develop/jiawei/CogVideo/sat/sgm/modules/diffusionmodules/denoiser.py", line 38, in forward
[rank0]:     return network(input * c_in, c_noise, cond, **additional_model_inputs) * c_out + input * c_skip
[rank0]:   File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/mnt/ceph/develop/jiawei/CogVideo/sat/sgm/modules/diffusionmodules/wrappers.py", line 35, in forward
[rank0]:     return self.diffusion_model(
[rank0]:   File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/mnt/ceph/develop/jiawei/CogVideo/sat/dit_video_concat.py", line 770, in forward
[rank0]:     output = super().forward(**kwargs)[0]
[rank0]:   File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/sat/model/base_model.py", line 138, in forward
[rank0]:     return self.transformer(*args, **kwargs)
[rank0]:   File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/sat/model/transformer.py", line 608, in forward
[rank0]:     hidden_states = hidden_states + position_embeddings
[rank0]: RuntimeError: The size of tensor a (19598) must match the size of tensor b (17776) at non-singleton dimension 1
wandb: - 0.015 MB of 0.015 MB uploadedwandb: \ 0.015 MB of 0.015 MB uploadedwandb: | 0.015 MB of 0.027 MB uploadedwandb: / 0.018 MB of 0.029 MB uploadedwandb: - 0.029 MB of 0.029 MB uploadedwandb: ðŸš€ View run lora-disney-09-09-18-08 at: https://wandb.ai/dmeck/cogv_2b_lora_base_002/runs/c3qk8axv
wandb: â­ï¸ View project at: https://wandb.ai/dmeck/cogv_2b_lora_base_002
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora/wandb/run-20240909_180853-c3qk8axv/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
E0909 18:09:44.496000 140178878019392 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 0 (pid: 2034490) of binary: /mnt/ceph/develop/jiawei/conda_env/cogvideo/bin/python
Traceback (most recent call last):
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train_video.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-09-09_18:09:44
  host      : nm04-a800-node083
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 2034490)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
DONE on nm04-a800-node083
RUN on nm04-a800-node083, CUDA_VISIBLE_DEVICES=0
torchrun --standalone --nproc_per_node=1 train_video.py --base configs/cogvideox_2b_lora.yaml configs/sft.yaml --seed 42
[2024-09-09 18:21:21,144] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
[93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, input, weight, bias=None):
/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/kornia/feature/lightglue.py:44: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)
no module 'xformers'. Processing without...
no module 'xformers'. Processing without...
[2024-09-09 18:21:37,836] [INFO] using world size: 1
[2024-09-09 18:21:37,836] [INFO] Will override arguments with manually specified deepspeed_config!
[2024-09-09 18:21:37,843] [INFO] [RANK 0] > initializing model parallel with size 1
[2024-09-09 18:21:37,843] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-09-09 18:21:38,981] [INFO] [RANK 0] building SATVideoDiffusionEngine model ...
[2024-09-09 18:21:49,273] [WARNING] [RANK 0] Failed to load bitsandbytes:No module named 'bitsandbytes'
[2024-09-09 18:21:49,273] [INFO] [RANK 0] replacing layer 0 attention with lora
[2024-09-09 18:21:49,299] [INFO] [RANK 0] replacing layer 1 attention with lora
[2024-09-09 18:21:49,322] [INFO] [RANK 0] replacing layer 2 attention with lora
[2024-09-09 18:21:49,345] [INFO] [RANK 0] replacing layer 3 attention with lora
[2024-09-09 18:21:49,369] [INFO] [RANK 0] replacing layer 4 attention with lora
[2024-09-09 18:21:49,393] [INFO] [RANK 0] replacing layer 5 attention with lora
[2024-09-09 18:21:49,417] [INFO] [RANK 0] replacing layer 6 attention with lora
[2024-09-09 18:21:49,441] [INFO] [RANK 0] replacing layer 7 attention with lora
[2024-09-09 18:21:49,464] [INFO] [RANK 0] replacing layer 8 attention with lora
[2024-09-09 18:21:49,487] [INFO] [RANK 0] replacing layer 9 attention with lora
[2024-09-09 18:21:49,509] [INFO] [RANK 0] replacing layer 10 attention with lora
[2024-09-09 18:21:49,532] [INFO] [RANK 0] replacing layer 11 attention with lora
[2024-09-09 18:21:49,555] [INFO] [RANK 0] replacing layer 12 attention with lora
[2024-09-09 18:21:49,577] [INFO] [RANK 0] replacing layer 13 attention with lora
[2024-09-09 18:21:49,599] [INFO] [RANK 0] replacing layer 14 attention with lora
[2024-09-09 18:21:49,620] [INFO] [RANK 0] replacing layer 15 attention with lora
[2024-09-09 18:21:49,643] [INFO] [RANK 0] replacing layer 16 attention with lora
[2024-09-09 18:21:49,665] [INFO] [RANK 0] replacing layer 17 attention with lora
[2024-09-09 18:21:49,686] [INFO] [RANK 0] replacing layer 18 attention with lora
[2024-09-09 18:21:49,708] [INFO] [RANK 0] replacing layer 19 attention with lora
[2024-09-09 18:21:49,729] [INFO] [RANK 0] replacing layer 20 attention with lora
[2024-09-09 18:21:49,752] [INFO] [RANK 0] replacing layer 21 attention with lora
[2024-09-09 18:21:49,774] [INFO] [RANK 0] replacing layer 22 attention with lora
[2024-09-09 18:21:49,795] [INFO] [RANK 0] replacing layer 23 attention with lora
[2024-09-09 18:21:49,817] [INFO] [RANK 0] replacing layer 24 attention with lora
[2024-09-09 18:21:49,838] [INFO] [RANK 0] replacing layer 25 attention with lora
[2024-09-09 18:21:49,860] [INFO] [RANK 0] replacing layer 26 attention with lora
[2024-09-09 18:21:49,880] [INFO] [RANK 0] replacing layer 27 attention with lora
[2024-09-09 18:21:49,901] [INFO] [RANK 0] replacing layer 28 attention with lora
[2024-09-09 18:21:50,124] [INFO] [RANK 0] replacing layer 29 attention with lora
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  1.47it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.60it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.58it/s]
Initialized embedder #0: FrozenT5Embedder with 4762310656 params. Trainable: False
Working with z of shape (1, 16, 32, 32) = 16384 dimensions.
/mnt/ceph/develop/jiawei/CogVideo/sat/vae_modules/autoencoder.py:565: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  sd = torch.load(path, map_location="cpu")["state_dict"]
Deleting key loss.logvar from state_dict.
Deleting key loss.perceptual_loss.scaling_layer.shift from state_dict.
Deleting key loss.perceptual_loss.scaling_layer.scale from state_dict.
Deleting key loss.perceptual_loss.net.slice1.0.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice1.0.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice1.2.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice1.2.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice2.5.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice2.5.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice2.7.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice2.7.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice3.10.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice3.10.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice3.12.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice3.12.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice3.14.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice3.14.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice4.17.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice4.17.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice4.19.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice4.19.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice4.21.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice4.21.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice5.24.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice5.24.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice5.26.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice5.26.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice5.28.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice5.28.bias from state_dict.
Deleting key loss.perceptual_loss.lin0.model.1.weight from state_dict.
Deleting key loss.perceptual_loss.lin1.model.1.weight from state_dict.
Deleting key loss.perceptual_loss.lin2.model.1.weight from state_dict.
Deleting key loss.perceptual_loss.lin3.model.1.weight from state_dict.
Deleting key loss.perceptual_loss.lin4.model.1.weight from state_dict.
Deleting key loss.discriminator.blocks.0.downsample_res.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.0.downsample_res.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.0.net.0.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.0.net.0.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.0.net.2.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.0.net.2.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.0.downsample.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.0.downsample.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.1.downsample_res.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.1.downsample_res.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.1.net.0.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.1.net.0.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.1.net.2.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.1.net.2.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.1.downsample.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.1.downsample.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.2.downsample_res.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.2.downsample_res.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.2.net.0.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.2.net.0.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.2.net.2.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.2.net.2.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.2.downsample.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.2.downsample.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.3.downsample_res.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.3.downsample_res.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.3.net.0.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.3.net.0.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.3.net.2.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.3.net.2.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.3.downsample.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.3.downsample.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.4.0.conv_res.weight from state_dict.
Deleting key loss.discriminator.blocks.4.0.conv_res.bias from state_dict.
Deleting key loss.discriminator.blocks.4.0.net.0.weight from state_dict.
Deleting key loss.discriminator.blocks.4.0.net.0.bias from state_dict.
Deleting key loss.discriminator.blocks.4.0.net.2.weight from state_dict.
Deleting key loss.discriminator.blocks.4.0.net.2.bias from state_dict.
Deleting key loss.discriminator.blocks.4.0.downsample.1.weight from state_dict.
Deleting key loss.discriminator.blocks.4.0.downsample.1.bias from state_dict.
Deleting key loss.discriminator.blocks.4.1.0.fn.norm.gamma from state_dict.
Deleting key loss.discriminator.blocks.4.1.0.fn.attn.to_q.0.weight from state_dict.
Deleting key loss.discriminator.blocks.4.1.0.fn.attn.to_kv.0.weight from state_dict.
Deleting key loss.discriminator.blocks.4.1.0.fn.attn.to_out.0.weight from state_dict.
Deleting key loss.discriminator.blocks.4.1.1.fn.norm.gamma from state_dict.
Deleting key loss.discriminator.blocks.4.1.1.fn.net.0.weight from state_dict.
Deleting key loss.discriminator.blocks.4.1.1.fn.net.0.bias from state_dict.
Deleting key loss.discriminator.blocks.4.1.1.fn.net.2.weight from state_dict.
Deleting key loss.discriminator.blocks.4.1.1.fn.net.2.bias from state_dict.
Deleting key loss.discriminator.blocks.5.0.conv_res.weight from state_dict.
Deleting key loss.discriminator.blocks.5.0.conv_res.bias from state_dict.
Deleting key loss.discriminator.blocks.5.0.net.0.weight from state_dict.
Deleting key loss.discriminator.blocks.5.0.net.0.bias from state_dict.
Deleting key loss.discriminator.blocks.5.0.net.2.weight from state_dict.
Deleting key loss.discriminator.blocks.5.0.net.2.bias from state_dict.
Deleting key loss.discriminator.blocks.5.0.downsample.1.weight from state_dict.
Deleting key loss.discriminator.blocks.5.0.downsample.1.bias from state_dict.
Deleting key loss.discriminator.blocks.5.1.0.fn.norm.gamma from state_dict.
Deleting key loss.discriminator.blocks.5.1.0.fn.attn.to_q.0.weight from state_dict.
Deleting key loss.discriminator.blocks.5.1.0.fn.attn.to_kv.0.weight from state_dict.
Deleting key loss.discriminator.blocks.5.1.0.fn.attn.to_out.0.weight from state_dict.
Deleting key loss.discriminator.blocks.5.1.1.fn.norm.gamma from state_dict.
Deleting key loss.discriminator.blocks.5.1.1.fn.net.0.weight from state_dict.
Deleting key loss.discriminator.blocks.5.1.1.fn.net.0.bias from state_dict.
Deleting key loss.discriminator.blocks.5.1.1.fn.net.2.weight from state_dict.
Deleting key loss.discriminator.blocks.5.1.1.fn.net.2.bias from state_dict.
Deleting key loss.discriminator.blocks.6.0.conv_res.weight from state_dict.
Deleting key loss.discriminator.blocks.6.0.conv_res.bias from state_dict.
Deleting key loss.discriminator.blocks.6.0.net.0.weight from state_dict.
Deleting key loss.discriminator.blocks.6.0.net.0.bias from state_dict.
Deleting key loss.discriminator.blocks.6.0.net.2.weight from state_dict.
Deleting key loss.discriminator.blocks.6.0.net.2.bias from state_dict.
Deleting key loss.discriminator.blocks.6.1.0.fn.norm.gamma from state_dict.
Deleting key loss.discriminator.blocks.6.1.0.fn.attn.to_q.0.weight from state_dict.
Deleting key loss.discriminator.blocks.6.1.0.fn.attn.to_kv.0.weight from state_dict.
Deleting key loss.discriminator.blocks.6.1.0.fn.attn.to_out.0.weight from state_dict.
Deleting key loss.discriminator.blocks.6.1.1.fn.norm.gamma from state_dict.
Deleting key loss.discriminator.blocks.6.1.1.fn.net.0.weight from state_dict.
Deleting key loss.discriminator.blocks.6.1.1.fn.net.0.bias from state_dict.
Deleting key loss.discriminator.blocks.6.1.1.fn.net.2.weight from state_dict.
Deleting key loss.discriminator.blocks.6.1.1.fn.net.2.bias from state_dict.
Deleting key loss.discriminator.to_logits.0.weight from state_dict.
Deleting key loss.discriminator.to_logits.0.bias from state_dict.
Deleting key loss.discriminator.to_logits.3.weight from state_dict.
Deleting key loss.discriminator.to_logits.3.bias from state_dict.
Missing keys:  []
Unexpected keys:  []
Restored from /mnt/ceph/develop/jiawei/model_checkpoint/CogVideoX-2b-sat/vae/3d-vae.pt
[2024-09-09 18:21:54,318] [INFO] [RANK 0]  > number of parameters on model parallel rank 0: 6768288995
[2024-09-09 18:22:01,024] [INFO] [RANK 0] global rank 0 is loading checkpoint /mnt/ceph/develop/jiawei/model_checkpoint/CogVideoX-2b-sat/transformer/1000/mp_rank_00_model_states.pt
/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/sat/training/model_io.py:286: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  sd = torch.load(checkpoint_name, map_location='cpu')
[2024-09-09 18:22:03,013] [INFO] [RANK 0] > successfully loaded /mnt/ceph/develop/jiawei/model_checkpoint/CogVideoX-2b-sat/transformer/1000/mp_rank_00_model_states.pt
[2024-09-09 18:22:03,376] [INFO] [RANK 0] ***** Total trainable parameters: 58982400 *****
[2024-09-09 18:22:03,376] [INFO] [RANK 0] [<class 'sat.ops.layernorm.LayerNorm'>, <class 'torch.nn.modules.normalization.LayerNorm'>, <class 'sat.ops.layernorm.RMSNorm'>] is set to no_weight_decay
[2024-09-09 18:22:03,472] [INFO] [RANK 0] Syncing initialized parameters...
[2024-09-09 18:22:03,534] [INFO] [RANK 0] Finished syncing initialized parameters.
[2024-09-09 18:22:03,535] [INFO] [RANK 0] Using optimizer sat.ops.FusedEmaAdam from sat.
[2024-09-09 18:22:03,538] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.4, git-hash=unknown, git-branch=unknown
[2024-09-09 18:22:03,539] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
[2024-09-09 18:22:03,629] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /root/.cache/torch_extensions/py310_cu121/fused_ema_adam/build.ninja...
/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1965: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module fused_ema_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_ema_adam...
Time to load fused_ema_adam op: 3.2965760231018066 seconds
[2024-09-09 18:22:06,962] [INFO] [logging.py:96:log_dist] [Rank 0] Using client callable to create basic optimizer
[2024-09-09 18:22:06,964] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-09-09 18:22:06,991] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedEmaAdam
[2024-09-09 18:22:06,991] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedEmaAdam type=<class 'sat.ops.fused_ema_adam.FusedEmaAdam'>
[2024-09-09 18:22:06,994] [WARNING] [engine.py:1179:_do_optimizer_sanity_check] **** You are using ZeRO with an untested optimizer, proceed with caution *****
[2024-09-09 18:22:06,994] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2024-09-09 18:22:06,994] [INFO] [stage_1_and_2.py:148:__init__] Reduce bucket size 1000000000
[2024-09-09 18:22:06,994] [INFO] [stage_1_and_2.py:149:__init__] Allgather bucket size 1000000000
[2024-09-09 18:22:06,994] [INFO] [stage_1_and_2.py:150:__init__] CPU Offload: False
[2024-09-09 18:22:06,994] [INFO] [stage_1_and_2.py:151:__init__] Round robin gradient partitioning: False
[2024-09-09 18:22:07,251] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2024-09-09 18:22:07,251] [INFO] [utils.py:782:see_memory_usage] MA 12.87 GB         Max_MA 12.98 GB         CA 13.23 GB         Max_CA 13 GB 
[2024-09-09 18:22:07,256] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 120.81 GB, percent = 6.0%
[2024-09-09 18:22:07,397] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2024-09-09 18:22:07,397] [INFO] [utils.py:782:see_memory_usage] MA 12.87 GB         Max_MA 13.09 GB         CA 13.45 GB         Max_CA 13 GB 
[2024-09-09 18:22:07,399] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 120.81 GB, percent = 6.0%
[2024-09-09 18:22:07,400] [INFO] [stage_1_and_2.py:543:__init__] optimizer state initialized
[2024-09-09 18:22:07,542] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2024-09-09 18:22:07,543] [INFO] [utils.py:782:see_memory_usage] MA 12.87 GB         Max_MA 12.87 GB         CA 13.45 GB         Max_CA 13 GB 
[2024-09-09 18:22:07,545] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 120.81 GB, percent = 6.0%
[2024-09-09 18:22:07,549] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer
[2024-09-09 18:22:07,549] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2024-09-09 18:22:07,552] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2024-09-09 18:22:07,552] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[1.0], mom=[[0.9, 0.95]]
[2024-09-09 18:22:07,554] [INFO] [config.py:997:print] DeepSpeedEngine configuration:
[2024-09-09 18:22:07,555] [INFO] [config.py:1001:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-09-09 18:22:07,557] [INFO] [config.py:1001:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-09-09 18:22:07,557] [INFO] [config.py:1001:print]   amp_enabled .................. False
[2024-09-09 18:22:07,557] [INFO] [config.py:1001:print]   amp_params ................... False
[2024-09-09 18:22:07,557] [INFO] [config.py:1001:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-09-09 18:22:07,557] [INFO] [config.py:1001:print]   bfloat16_enabled ............. False
[2024-09-09 18:22:07,558] [INFO] [config.py:1001:print]   bfloat16_immediate_grad_update  False
[2024-09-09 18:22:07,558] [INFO] [config.py:1001:print]   checkpoint_parallel_write_pipeline  False
[2024-09-09 18:22:07,558] [INFO] [config.py:1001:print]   checkpoint_tag_validation_enabled  True
[2024-09-09 18:22:07,558] [INFO] [config.py:1001:print]   checkpoint_tag_validation_fail  False
[2024-09-09 18:22:07,558] [INFO] [config.py:1001:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f3ca6d97b20>
[2024-09-09 18:22:07,558] [INFO] [config.py:1001:print]   communication_data_type ...... None
[2024-09-09 18:22:07,558] [INFO] [config.py:1001:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-09-09 18:22:07,559] [INFO] [config.py:1001:print]   curriculum_enabled_legacy .... False
[2024-09-09 18:22:07,559] [INFO] [config.py:1001:print]   curriculum_params_legacy ..... False
[2024-09-09 18:22:07,559] [INFO] [config.py:1001:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-09-09 18:22:07,559] [INFO] [config.py:1001:print]   data_efficiency_enabled ...... False
[2024-09-09 18:22:07,559] [INFO] [config.py:1001:print]   dataloader_drop_last ......... False
[2024-09-09 18:22:07,559] [INFO] [config.py:1001:print]   disable_allgather ............ False
[2024-09-09 18:22:07,561] [INFO] [config.py:1001:print]   dump_state ................... False
[2024-09-09 18:22:07,561] [INFO] [config.py:1001:print]   dynamic_loss_scale_args ...... None
[2024-09-09 18:22:07,561] [INFO] [config.py:1001:print]   eigenvalue_enabled ........... False
[2024-09-09 18:22:07,561] [INFO] [config.py:1001:print]   eigenvalue_gas_boundary_resolution  1
[2024-09-09 18:22:07,561] [INFO] [config.py:1001:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-09-09 18:22:07,561] [INFO] [config.py:1001:print]   eigenvalue_layer_num ......... 0
[2024-09-09 18:22:07,561] [INFO] [config.py:1001:print]   eigenvalue_max_iter .......... 100
[2024-09-09 18:22:07,561] [INFO] [config.py:1001:print]   eigenvalue_stability ......... 1e-06
[2024-09-09 18:22:07,561] [INFO] [config.py:1001:print]   eigenvalue_tol ............... 0.01
[2024-09-09 18:22:07,561] [INFO] [config.py:1001:print]   eigenvalue_verbose ........... False
[2024-09-09 18:22:07,561] [INFO] [config.py:1001:print]   elasticity_enabled ........... False
[2024-09-09 18:22:07,561] [INFO] [config.py:1001:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-09-09 18:22:07,563] [INFO] [config.py:1001:print]   fp16_auto_cast ............... False
[2024-09-09 18:22:07,563] [INFO] [config.py:1001:print]   fp16_enabled ................. True
[2024-09-09 18:22:07,563] [INFO] [config.py:1001:print]   fp16_master_weights_and_gradients  False
[2024-09-09 18:22:07,563] [INFO] [config.py:1001:print]   global_rank .................. 0
[2024-09-09 18:22:07,563] [INFO] [config.py:1001:print]   grad_accum_dtype ............. None
[2024-09-09 18:22:07,563] [INFO] [config.py:1001:print]   gradient_accumulation_steps .. 1
[2024-09-09 18:22:07,563] [INFO] [config.py:1001:print]   gradient_clipping ............ 0.1
[2024-09-09 18:22:07,563] [INFO] [config.py:1001:print]   gradient_predivide_factor .... 1.0
[2024-09-09 18:22:07,563] [INFO] [config.py:1001:print]   graph_harvesting ............. False
[2024-09-09 18:22:07,563] [INFO] [config.py:1001:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-09-09 18:22:07,564] [INFO] [config.py:1001:print]   initial_dynamic_scale ........ 65536
[2024-09-09 18:22:07,564] [INFO] [config.py:1001:print]   load_universal_checkpoint .... False
[2024-09-09 18:22:07,564] [INFO] [config.py:1001:print]   loss_scale ................... 0
[2024-09-09 18:22:07,564] [INFO] [config.py:1001:print]   memory_breakdown ............. False
[2024-09-09 18:22:07,564] [INFO] [config.py:1001:print]   mics_hierarchial_params_gather  False
[2024-09-09 18:22:07,564] [INFO] [config.py:1001:print]   mics_shard_size .............. -1
[2024-09-09 18:22:07,564] [INFO] [config.py:1001:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-09-09 18:22:07,564] [INFO] [config.py:1001:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-09-09 18:22:07,564] [INFO] [config.py:1001:print]   optimizer_legacy_fusion ...... False
[2024-09-09 18:22:07,564] [INFO] [config.py:1001:print]   optimizer_name ............... None
[2024-09-09 18:22:07,564] [INFO] [config.py:1001:print]   optimizer_params ............. None
[2024-09-09 18:22:07,564] [INFO] [config.py:1001:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-09-09 18:22:07,564] [INFO] [config.py:1001:print]   pld_enabled .................. False
[2024-09-09 18:22:07,568] [INFO] [config.py:1001:print]   pld_params ................... False
[2024-09-09 18:22:07,568] [INFO] [config.py:1001:print]   prescale_gradients ........... False
[2024-09-09 18:22:07,568] [INFO] [config.py:1001:print]   scheduler_name ............... None
[2024-09-09 18:22:07,568] [INFO] [config.py:1001:print]   scheduler_params ............. None
[2024-09-09 18:22:07,568] [INFO] [config.py:1001:print]   seq_parallel_communication_data_type  torch.float32
[2024-09-09 18:22:07,568] [INFO] [config.py:1001:print]   sparse_attention ............. None
[2024-09-09 18:22:07,568] [INFO] [config.py:1001:print]   sparse_gradients_enabled ..... False
[2024-09-09 18:22:07,568] [INFO] [config.py:1001:print]   steps_per_print .............. 50
[2024-09-09 18:22:07,568] [INFO] [config.py:1001:print]   timers_config ................ enabled=True synchronized=True
[2024-09-09 18:22:07,568] [INFO] [config.py:1001:print]   train_batch_size ............. 2
[2024-09-09 18:22:07,568] [INFO] [config.py:1001:print]   train_micro_batch_size_per_gpu  2
[2024-09-09 18:22:07,568] [INFO] [config.py:1001:print]   use_data_before_expert_parallel_  False
[2024-09-09 18:22:07,568] [INFO] [config.py:1001:print]   use_node_local_storage ....... False
[2024-09-09 18:22:07,568] [INFO] [config.py:1001:print]   wall_clock_breakdown ......... False
[2024-09-09 18:22:07,568] [INFO] [config.py:1001:print]   weight_quantization_config ... None
[2024-09-09 18:22:07,568] [INFO] [config.py:1001:print]   world_size ................... 1
[2024-09-09 18:22:07,568] [INFO] [config.py:1001:print]   zero_allow_untested_optimizer  True
[2024-09-09 18:22:07,570] [INFO] [config.py:1001:print]   zero_config .................. stage=2 contiguous_gradients=False reduce_scatter=True reduce_bucket_size=1000000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=1000000000 overlap_comm=True load_from_fp32_weights=False elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-09-09 18:22:07,570] [INFO] [config.py:1001:print]   zero_enabled ................. True
[2024-09-09 18:22:07,570] [INFO] [config.py:1001:print]   zero_force_ds_cpu_optimizer .. True
[2024-09-09 18:22:07,570] [INFO] [config.py:1001:print]   zero_optimization_stage ...... 2
[2024-09-09 18:22:07,570] [INFO] [config.py:987:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 2, 
    "gradient_accumulation_steps": 1, 
    "steps_per_print": 50, 
    "gradient_clipping": 0.1, 
    "zero_optimization": {
        "stage": 2, 
        "cpu_offload": false, 
        "contiguous_gradients": false, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 1.000000e+09, 
        "allgather_bucket_size": 1.000000e+09, 
        "load_from_fp32_weights": false
    }, 
    "zero_allow_untested_optimizer": true, 
    "bf16": {
        "enabled": false
    }, 
    "fp16": {
        "enabled": true
    }, 
    "loss_scale": 0, 
    "loss_scale_window": 400, 
    "hysteresis": 2, 
    "min_loss_scale": 1, 
    "activation_checkpointing": {
        "partition_activations": false, 
        "contiguous_memory_optimization": false
    }, 
    "wall_clock_breakdown": false
}
[2024-09-09 18:22:07,570] [INFO] [RANK 0] learning rate decaying style linear, ratio 10.0
[2024-09-09 18:22:07,570] [INFO] [RANK 0] Finetuning Model...
[2024-09-09 18:22:07,570] [INFO] [RANK 0] arguments:
[2024-09-09 18:22:07,570] [INFO] [RANK 0]   base ......................... ['configs/cogvideox_2b_lora.yaml', 'configs/sft.yaml']
[2024-09-09 18:22:07,572] [INFO] [RANK 0]   model_parallel_size .......... 1
[2024-09-09 18:22:07,572] [INFO] [RANK 0]   force_pretrain ............... False
[2024-09-09 18:22:07,572] [INFO] [RANK 0]   device ....................... 0
[2024-09-09 18:22:07,572] [INFO] [RANK 0]   debug ........................ False
[2024-09-09 18:22:07,572] [INFO] [RANK 0]   log_image .................... True
[2024-09-09 18:22:07,572] [INFO] [RANK 0]   output_dir ................... samples
[2024-09-09 18:22:07,572] [INFO] [RANK 0]   input_dir .................... None
[2024-09-09 18:22:07,572] [INFO] [RANK 0]   input_type ................... cli
[2024-09-09 18:22:07,572] [INFO] [RANK 0]   input_file ................... input.txt
[2024-09-09 18:22:07,572] [INFO] [RANK 0]   final_size ................... 2048
[2024-09-09 18:22:07,572] [INFO] [RANK 0]   sdedit ....................... False
[2024-09-09 18:22:07,572] [INFO] [RANK 0]   grid_num_rows ................ 1
[2024-09-09 18:22:07,574] [INFO] [RANK 0]   force_inference .............. False
[2024-09-09 18:22:07,574] [INFO] [RANK 0]   lcm_steps .................... None
[2024-09-09 18:22:07,574] [INFO] [RANK 0]   sampling_num_frames .......... 32
[2024-09-09 18:22:07,574] [INFO] [RANK 0]   sampling_fps ................. 8
[2024-09-09 18:22:07,574] [INFO] [RANK 0]   only_save_latents ............ False
[2024-09-09 18:22:07,574] [INFO] [RANK 0]   only_log_video_latents ....... False
[2024-09-09 18:22:07,574] [INFO] [RANK 0]   latent_channels .............. 32
[2024-09-09 18:22:07,574] [INFO] [RANK 0]   image2video .................. False
[2024-09-09 18:22:07,574] [INFO] [RANK 0]   experiment_name .............. lora-disney-09-09-18-21
[2024-09-09 18:22:07,574] [INFO] [RANK 0]   train_iters .................. 1000
[2024-09-09 18:22:07,574] [INFO] [RANK 0]   batch_size ................... 2
[2024-09-09 18:22:07,574] [INFO] [RANK 0]   lr ........................... 0.0005
[2024-09-09 18:22:07,574] [INFO] [RANK 0]   mode ......................... finetune
[2024-09-09 18:22:07,574] [INFO] [RANK 0]   seed ......................... 42
[2024-09-09 18:22:07,574] [INFO] [RANK 0]   zero_stage ................... 0
[2024-09-09 18:22:07,574] [INFO] [RANK 0]   checkpoint_activations ....... True
[2024-09-09 18:22:07,574] [INFO] [RANK 0]   checkpoint_num_layers ........ 1
[2024-09-09 18:22:07,576] [INFO] [RANK 0]   checkpoint_skip_layers ....... 0
[2024-09-09 18:22:07,576] [INFO] [RANK 0]   fp16 ......................... True
[2024-09-09 18:22:07,576] [INFO] [RANK 0]   bf16 ......................... False
[2024-09-09 18:22:07,577] [INFO] [RANK 0]   gradient_accumulation_steps .. 1
[2024-09-09 18:22:07,577] [INFO] [RANK 0]   profiling .................... -1
[2024-09-09 18:22:07,577] [INFO] [RANK 0]   epochs ....................... None
[2024-09-09 18:22:07,577] [INFO] [RANK 0]   log_interval ................. 20
[2024-09-09 18:22:07,577] [INFO] [RANK 0]   summary_dir .................. /mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora
[2024-09-09 18:22:07,577] [INFO] [RANK 0]   save_args .................... False
[2024-09-09 18:22:07,577] [INFO] [RANK 0]   lr_decay_iters ............... None
[2024-09-09 18:22:07,577] [INFO] [RANK 0]   lr_decay_style ............... linear
[2024-09-09 18:22:07,577] [INFO] [RANK 0]   lr_decay_ratio ............... 0.1
[2024-09-09 18:22:07,577] [INFO] [RANK 0]   warmup ....................... 0.01
[2024-09-09 18:22:07,580] [INFO] [RANK 0]   weight_decay ................. 5e-06
[2024-09-09 18:22:07,580] [INFO] [RANK 0]   save ......................... /mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora/lora-disney-09-09-18-21
[2024-09-09 18:22:07,580] [INFO] [RANK 0]   load ......................... /mnt/ceph/develop/jiawei/model_checkpoint/CogVideoX-2b-sat/transformer
[2024-09-09 18:22:07,580] [INFO] [RANK 0]   force_train .................. True
[2024-09-09 18:22:07,580] [INFO] [RANK 0]   save_interval ................ 100
[2024-09-09 18:22:07,580] [INFO] [RANK 0]   no_save_rng .................. False
[2024-09-09 18:22:07,580] [INFO] [RANK 0]   no_load_rng .................. True
[2024-09-09 18:22:07,580] [INFO] [RANK 0]   resume_dataloader ............ False
[2024-09-09 18:22:07,580] [INFO] [RANK 0]   distributed_backend .......... nccl
[2024-09-09 18:22:07,580] [INFO] [RANK 0]   local_rank ................... 0
[2024-09-09 18:22:07,580] [INFO] [RANK 0]   exit_interval ................ None
[2024-09-09 18:22:07,582] [INFO] [RANK 0]   wandb ........................ True
[2024-09-09 18:22:07,582] [INFO] [RANK 0]   wandb_project_name ........... cogv_2b_lora_base_002
[2024-09-09 18:22:07,582] [INFO] [RANK 0]   eval_batch_size .............. 1
[2024-09-09 18:22:07,582] [INFO] [RANK 0]   eval_iters ................... 1
[2024-09-09 18:22:07,582] [INFO] [RANK 0]   eval_interval ................ 100
[2024-09-09 18:22:07,582] [INFO] [RANK 0]   strict_eval .................. False
[2024-09-09 18:22:07,582] [INFO] [RANK 0]   train_data ................... ['/mnt/ceph/develop/jiawei/lora_dataset/cogvideo_lora_dataset']
[2024-09-09 18:22:07,583] [INFO] [RANK 0]   train_data_weights ........... None
[2024-09-09 18:22:07,583] [INFO] [RANK 0]   iterable_dataset ............. False
[2024-09-09 18:22:07,583] [INFO] [RANK 0]   iterable_dataset_eval ........ 
[2024-09-09 18:22:07,583] [INFO] [RANK 0]   batch_from_same_dataset ...... False
[2024-09-09 18:22:07,583] [INFO] [RANK 0]   valid_data ................... ['/mnt/ceph/develop/jiawei/lora_dataset/cogvideo_lora_dataset']
[2024-09-09 18:22:07,584] [INFO] [RANK 0]   test_data .................... None
[2024-09-09 18:22:07,584] [INFO] [RANK 0]   split ........................ 1,0,0
[2024-09-09 18:22:07,584] [INFO] [RANK 0]   num_workers .................. 8
[2024-09-09 18:22:07,584] [INFO] [RANK 0]   block_size ................... 10000
[2024-09-09 18:22:07,584] [INFO] [RANK 0]   prefetch_factor .............. 4
[2024-09-09 18:22:07,584] [INFO] [RANK 0]   deepspeed .................... True
[2024-09-09 18:22:07,584] [INFO] [RANK 0]   deepspeed_config ............. {'train_micro_batch_size_per_gpu': 2, 'gradient_accumulation_steps': 1, 'steps_per_print': 50, 'gradient_clipping': 0.1, 'zero_optimization': {'stage': 2, 'cpu_offload': False, 'contiguous_gradients': False, 'overlap_comm': True, 'reduce_scatter': True, 'reduce_bucket_size': 1000000000, 'allgather_bucket_size': 1000000000, 'load_from_fp32_weights': False}, 'zero_allow_untested_optimizer': True, 'bf16': {'enabled': False}, 'fp16': {'enabled': True}, 'loss_scale': 0, 'loss_scale_window': 400, 'hysteresis': 2, 'min_loss_scale': 1, 'activation_checkpointing': {'partition_activations': False, 'contiguous_memory_optimization': False}, 'wall_clock_breakdown': False}
[2024-09-09 18:22:07,584] [INFO] [RANK 0]   deepscale .................... False
[2024-09-09 18:22:07,584] [INFO] [RANK 0]   deepscale_config ............. None
[2024-09-09 18:22:07,585] [INFO] [RANK 0]   model_config ................. {'scale_factor': 1.15258426, 'disable_first_stage_autocast': True, 'not_trainable_prefixes': ['all'], 'log_keys': ['txt'], 'denoiser_config': {'target': 'sgm.modules.diffusionmodules.denoiser.DiscreteDenoiser', 'params': {'num_idx': 1000, 'quantize_c_noise': False, 'weighting_config': {'target': 'sgm.modules.diffusionmodules.denoiser_weighting.EpsWeighting'}, 'scaling_config': {'target': 'sgm.modules.diffusionmodules.denoiser_scaling.VideoScaling'}, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 3.0}}}}, 'network_config': {'target': 'dit_video_concat.DiffusionTransformer', 'params': {'time_embed_dim': 512, 'elementwise_affine': True, 'num_frames': 49, 'time_compressed_rate': 4, 'latent_width': 90, 'latent_height': 60, 'num_layers': 30, 'patch_size': 2, 'in_channels': 16, 'out_channels': 16, 'hidden_size': 1920, 'adm_in_channels': 256, 'num_attention_heads': 30, 'transformer_args': {'checkpoint_activations': True, 'vocab_size': 1, 'max_sequence_length': 64, 'layernorm_order': 'pre', 'skip_init': False, 'model_parallel_size': 1, 'is_decoder': False, 'num_layers': 30, 'hidden_size': 1920, 'num_attention_heads': 30, 'parallel_output': True}, 'modules': {'pos_embed_config': {'target': 'dit_video_concat.Basic3DPositionEmbeddingMixin', 'params': {'text_length': 2048, 'height_interpolation': 1.875, 'width_interpolation': 1.875}}, 'lora_config': {'target': 'sat.model.finetune.lora2.LoraMixin', 'params': {'r': 128}}, 'patch_embed_config': {'target': 'dit_video_concat.ImagePatchEmbeddingMixin', 'params': {'text_hidden_size': 4096}}, 'adaln_layer_config': {'target': 'dit_video_concat.AdaLNMixin', 'params': {'qk_ln': True}}, 'final_layer_config': {'target': 'dit_video_concat.FinalLayerMixin'}}, 'dtype': 'fp16'}}, 'conditioner_config': {'target': 'sgm.modules.GeneralConditioner', 'params': {'emb_models': [{'is_trainable': False, 'input_key': 'txt', 'ucg_rate': 0.1, 'target': 'sgm.modules.encoders.modules.FrozenT5Embedder', 'params': {'model_dir': '/mnt/ceph/develop/jiawei/model_checkpoint/t5-v1_1-xxl', 'max_length': 2048}}]}}, 'first_stage_config': {'target': 'vae_modules.autoencoder.VideoAutoencoderInferenceWrapper', 'params': {'cp_size': 1, 'ckpt_path': '/mnt/ceph/develop/jiawei/model_checkpoint/CogVideoX-2b-sat/vae/3d-vae.pt', 'ignore_keys': ['loss'], 'loss_config': {'target': 'torch.nn.Identity'}, 'regularizer_config': {'target': 'vae_modules.regularizers.DiagonalGaussianRegularizer'}, 'encoder_config': {'target': 'vae_modules.cp_enc_dec.ContextParallelEncoder3D', 'params': {'double_z': True, 'z_channels': 16, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 2, 4], 'attn_resolutions': [], 'num_res_blocks': 3, 'dropout': 0.0, 'gather_norm': True}}, 'decoder_config': {'target': 'vae_modules.cp_enc_dec.ContextParallelDecoder3D', 'params': {'double_z': True, 'z_channels': 16, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 2, 4], 'attn_resolutions': [], 'num_res_blocks': 3, 'dropout': 0.0, 'gather_norm': False}}}}, 'loss_fn_config': {'target': 'sgm.modules.diffusionmodules.loss.VideoDiffusionLoss', 'params': {'offset_noise_level': 0, 'sigma_sampler_config': {'target': 'sgm.modules.diffusionmodules.sigma_sampling.DiscreteSampling', 'params': {'uniform_sampling': True, 'num_idx': 1000, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 3.0}}}}}}, 'sampler_config': {'target': 'sgm.modules.diffusionmodules.sampling.VPSDEDPMPP2MSampler', 'params': {'num_steps': 50, 'verbose': True, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 3.0}}, 'guider_config': {'target': 'sgm.modules.diffusionmodules.guiders.DynamicCFG', 'params': {'scale': 6, 'exp': 5, 'num_steps': 50}}}}}
[2024-09-09 18:22:07,586] [INFO] [RANK 0]   data_config .................. {'target': 'data_video.SFTDataset', 'params': {'video_size': [480, 720], 'fps': 8, 'max_num_frames': 49, 'skip_frms_num': 3.0}}
[2024-09-09 18:22:07,586] [INFO] [RANK 0]   cuda ......................... True
[2024-09-09 18:22:07,586] [INFO] [RANK 0]   rank ......................... 0
[2024-09-09 18:22:07,586] [INFO] [RANK 0]   world_size ................... 1
[2024-09-09 18:22:07,586] [INFO] [RANK 0]   deepspeed_activation_checkpointing  True
[2024-09-09 18:22:07,586] [INFO] [RANK 0]   master_ip .................... nm04-a800-node083
[2024-09-09 18:22:07,586] [INFO] [RANK 0]   master_port .................. 32771
[2024-09-09 18:22:07,587] [INFO] [RANK 0]   log_config ................... [{'model': {'scale_factor': 1.15258426, 'disable_first_stage_autocast': True, 'not_trainable_prefixes': ['all'], 'log_keys': ['txt'], 'denoiser_config': {'target': 'sgm.modules.diffusionmodules.denoiser.DiscreteDenoiser', 'params': {'num_idx': 1000, 'quantize_c_noise': False, 'weighting_config': {'target': 'sgm.modules.diffusionmodules.denoiser_weighting.EpsWeighting'}, 'scaling_config': {'target': 'sgm.modules.diffusionmodules.denoiser_scaling.VideoScaling'}, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 3.0}}}}, 'network_config': {'target': 'dit_video_concat.DiffusionTransformer', 'params': {'time_embed_dim': 512, 'elementwise_affine': True, 'num_frames': 49, 'time_compressed_rate': 4, 'latent_width': 90, 'latent_height': 60, 'num_layers': 30, 'patch_size': 2, 'in_channels': 16, 'out_channels': 16, 'hidden_size': 1920, 'adm_in_channels': 256, 'num_attention_heads': 30, 'transformer_args': {'checkpoint_activations': True, 'vocab_size': 1, 'max_sequence_length': 64, 'layernorm_order': 'pre', 'skip_init': False, 'model_parallel_size': 1, 'is_decoder': False}, 'modules': {'pos_embed_config': {'target': 'dit_video_concat.Basic3DPositionEmbeddingMixin', 'params': {'text_length': 2048, 'height_interpolation': 1.875, 'width_interpolation': 1.875}}, 'lora_config': {'target': 'sat.model.finetune.lora2.LoraMixin', 'params': {'r': 128}}, 'patch_embed_config': {'target': 'dit_video_concat.ImagePatchEmbeddingMixin', 'params': {'text_hidden_size': 4096}}, 'adaln_layer_config': {'target': 'dit_video_concat.AdaLNMixin', 'params': {'qk_ln': True}}, 'final_layer_config': {'target': 'dit_video_concat.FinalLayerMixin'}}}}, 'conditioner_config': {'target': 'sgm.modules.GeneralConditioner', 'params': {'emb_models': [{'is_trainable': False, 'input_key': 'txt', 'ucg_rate': 0.1, 'target': 'sgm.modules.encoders.modules.FrozenT5Embedder', 'params': {'model_dir': '/mnt/ceph/develop/jiawei/model_checkpoint/t5-v1_1-xxl', 'max_length': 2048}}]}}, 'first_stage_config': {'target': 'vae_modules.autoencoder.VideoAutoencoderInferenceWrapper', 'params': {'cp_size': 1, 'ckpt_path': '/mnt/ceph/develop/jiawei/model_checkpoint/CogVideoX-2b-sat/vae/3d-vae.pt', 'ignore_keys': ['loss'], 'loss_config': {'target': 'torch.nn.Identity'}, 'regularizer_config': {'target': 'vae_modules.regularizers.DiagonalGaussianRegularizer'}, 'encoder_config': {'target': 'vae_modules.cp_enc_dec.ContextParallelEncoder3D', 'params': {'double_z': True, 'z_channels': 16, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 2, 4], 'attn_resolutions': [], 'num_res_blocks': 3, 'dropout': 0.0, 'gather_norm': True}}, 'decoder_config': {'target': 'vae_modules.cp_enc_dec.ContextParallelDecoder3D', 'params': {'double_z': True, 'z_channels': 16, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 2, 4], 'attn_resolutions': [], 'num_res_blocks': 3, 'dropout': 0.0, 'gather_norm': False}}}}, 'loss_fn_config': {'target': 'sgm.modules.diffusionmodules.loss.VideoDiffusionLoss', 'params': {'offset_noise_level': 0, 'sigma_sampler_config': {'target': 'sgm.modules.diffusionmodules.sigma_sampling.DiscreteSampling', 'params': {'uniform_sampling': True, 'num_idx': 1000, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 3.0}}}}}}, 'sampler_config': {'target': 'sgm.modules.diffusionmodules.sampling.VPSDEDPMPP2MSampler', 'params': {'num_steps': 50, 'verbose': True, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 3.0}}, 'guider_config': {'target': 'sgm.modules.diffusionmodules.guiders.DynamicCFG', 'params': {'scale': 6, 'exp': 5, 'num_steps': 50}}}}}}, {'args': {'checkpoint_activations': True, 'model_parallel_size': 1, 'experiment_name': 'lora-disney', 'mode': 'finetune', 'load': '/mnt/ceph/develop/jiawei/model_checkpoint/CogVideoX-2b-sat/transformer', 'no_load_rng': True, 'train_iters': 1000, 'eval_iters': 1, 'eval_interval': 100, 'eval_batch_size': 1, 'save': '/mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora', 'save_interval': 100, 'log_interval': 20, 'train_data': ['/mnt/ceph/develop/jiawei/lora_dataset/cogvideo_lora_dataset'], 'valid_data': ['/mnt/ceph/develop/jiawei/lora_dataset/cogvideo_lora_dataset'], 'split': '1,0,0', 'num_workers': 8, 'force_train': True, 'only_log_video_latents': False, 'wandb': True, 'wandb_project_name': 'cogv_2b_lora_base_002', 'summary_dir': '/mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora'}, 'data': {'target': 'data_video.SFTDataset', 'params': {'video_size': [480, 720], 'fps': 8, 'max_num_frames': 49, 'skip_frms_num': 3.0}}, 'deepspeed': {'train_micro_batch_size_per_gpu': 2, 'gradient_accumulation_steps': 1, 'steps_per_print': 50, 'gradient_clipping': 0.1, 'zero_optimization': {'stage': 2, 'cpu_offload': False, 'contiguous_gradients': False, 'overlap_comm': True, 'reduce_scatter': True, 'reduce_bucket_size': 1000000000, 'allgather_bucket_size': 1000000000, 'load_from_fp32_weights': False}, 'zero_allow_untested_optimizer': True, 'bf16': {'enabled': False}, 'fp16': {'enabled': True}, 'loss_scale': 0, 'loss_scale_window': 400, 'hysteresis': 2, 'min_loss_scale': 1, 'optimizer': {'type': 'sat.ops.FusedEmaAdam', 'params': {'lr': '5E-4', 'betas': [0.9, 0.95], 'eps': '5E-8', 'weight_decay': '5E-6'}}, 'activation_checkpointing': {'partition_activations': False, 'contiguous_memory_optimization': False}, 'wall_clock_breakdown': False}}]
[2024-09-09 18:22:07,589] [INFO] [RANK 0]   do_train ..................... True
[2024-09-09 18:22:07,590] [INFO] [RANK 0]   val_last_shape ............... []
[2024-09-09 18:22:07,590] [INFO] [RANK 0]   val_drop_number .............. 0
[2024-09-09 18:22:07,590] [INFO] [RANK 0]   do_valid ..................... True
[2024-09-09 18:22:07,590] [INFO] [RANK 0]   do_test ...................... False
[2024-09-09 18:22:07,590] [INFO] [RANK 0]   iteration .................... 0
wandb: Currently logged in as: dmeck. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora/wandb/run-20240909_182211-i5j8rtq2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lora-disney-09-09-18-21
wandb: â­ï¸ View project at https://wandb.ai/dmeck/cogv_2b_lora_base_002
wandb: ðŸš€ View run at https://wandb.ai/dmeck/cogv_2b_lora_base_002/runs/i5j8rtq2
[2024-09-09 18:22:48,878] [INFO] [checkpointing.py:541:forward] Activation Checkpointing Information
[2024-09-09 18:22:48,878] [INFO] [checkpointing.py:542:forward] ----Partition Activations False, CPU CHECKPOINTING False
[2024-09-09 18:22:48,881] [INFO] [checkpointing.py:543:forward] ----contiguous Memory Checkpointing False with None total layers
[2024-09-09 18:22:48,881] [INFO] [checkpointing.py:545:forward] ----Synchronization False
[2024-09-09 18:22:48,881] [INFO] [checkpointing.py:546:forward] ----Profiling time in checkpointing False
[2024-09-09 18:22:56,963] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4294967296, reducing to 2147483648
E0909 18:23:01.050000 139655399475008 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: -9) local_rank: 0 (pid: 2044454) of binary: /mnt/ceph/develop/jiawei/conda_env/cogvideo/bin/python
Traceback (most recent call last):
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
========================================================
train_video.py FAILED
--------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
--------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-09-09_18:23:01
  host      : nm04-a800-node083
  rank      : 0 (local_rank: 0)
  exitcode  : -9 (pid: 2044454)
  error_file: <N/A>
  traceback : Signal 9 (SIGKILL) received by PID 2044454
========================================================
DONE on nm04-a800-node083
RUN on nm04-a800-node083, CUDA_VISIBLE_DEVICES=0
torchrun --standalone --nproc_per_node=1 train_video.py --base configs/cogvideox_2b_lora.yaml configs/sft.yaml --seed 42
[2024-09-09 18:26:00,531] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
[93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, input, weight, bias=None):
/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/kornia/feature/lightglue.py:44: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)
no module 'xformers'. Processing without...
no module 'xformers'. Processing without...
[2024-09-09 18:26:20,191] [INFO] using world size: 1
[2024-09-09 18:26:20,191] [INFO] Will override arguments with manually specified deepspeed_config!
[2024-09-09 18:26:20,197] [INFO] [RANK 0] > initializing model parallel with size 1
[2024-09-09 18:26:20,198] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-09-09 18:26:21,410] [INFO] [RANK 0] building SATVideoDiffusionEngine model ...
[2024-09-09 18:26:31,628] [WARNING] [RANK 0] Failed to load bitsandbytes:No module named 'bitsandbytes'
[2024-09-09 18:26:31,628] [INFO] [RANK 0] replacing layer 0 attention with lora
[2024-09-09 18:26:31,656] [INFO] [RANK 0] replacing layer 1 attention with lora
[2024-09-09 18:26:31,679] [INFO] [RANK 0] replacing layer 2 attention with lora
[2024-09-09 18:26:31,702] [INFO] [RANK 0] replacing layer 3 attention with lora
[2024-09-09 18:26:31,726] [INFO] [RANK 0] replacing layer 4 attention with lora
[2024-09-09 18:26:31,749] [INFO] [RANK 0] replacing layer 5 attention with lora
[2024-09-09 18:26:31,772] [INFO] [RANK 0] replacing layer 6 attention with lora
[2024-09-09 18:26:31,795] [INFO] [RANK 0] replacing layer 7 attention with lora
[2024-09-09 18:26:31,817] [INFO] [RANK 0] replacing layer 8 attention with lora
[2024-09-09 18:26:31,839] [INFO] [RANK 0] replacing layer 9 attention with lora
[2024-09-09 18:26:31,862] [INFO] [RANK 0] replacing layer 10 attention with lora
[2024-09-09 18:26:31,884] [INFO] [RANK 0] replacing layer 11 attention with lora
[2024-09-09 18:26:31,907] [INFO] [RANK 0] replacing layer 12 attention with lora
[2024-09-09 18:26:31,931] [INFO] [RANK 0] replacing layer 13 attention with lora
[2024-09-09 18:26:31,953] [INFO] [RANK 0] replacing layer 14 attention with lora
[2024-09-09 18:26:31,975] [INFO] [RANK 0] replacing layer 15 attention with lora
[2024-09-09 18:26:31,997] [INFO] [RANK 0] replacing layer 16 attention with lora
[2024-09-09 18:26:32,019] [INFO] [RANK 0] replacing layer 17 attention with lora
[2024-09-09 18:26:32,038] [INFO] [RANK 0] replacing layer 18 attention with lora
[2024-09-09 18:26:32,057] [INFO] [RANK 0] replacing layer 19 attention with lora
[2024-09-09 18:26:32,076] [INFO] [RANK 0] replacing layer 20 attention with lora
[2024-09-09 18:26:32,095] [INFO] [RANK 0] replacing layer 21 attention with lora
[2024-09-09 18:26:32,115] [INFO] [RANK 0] replacing layer 22 attention with lora
[2024-09-09 18:26:32,134] [INFO] [RANK 0] replacing layer 23 attention with lora
[2024-09-09 18:26:32,153] [INFO] [RANK 0] replacing layer 24 attention with lora
[2024-09-09 18:26:32,172] [INFO] [RANK 0] replacing layer 25 attention with lora
[2024-09-09 18:26:32,194] [INFO] [RANK 0] replacing layer 26 attention with lora
[2024-09-09 18:26:32,215] [INFO] [RANK 0] replacing layer 27 attention with lora
[2024-09-09 18:26:32,238] [INFO] [RANK 0] replacing layer 28 attention with lora
[2024-09-09 18:26:32,260] [INFO] [RANK 0] replacing layer 29 attention with lora
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  1.48it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.55it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.54it/s]
Initialized embedder #0: FrozenT5Embedder with 4762310656 params. Trainable: False
Working with z of shape (1, 16, 32, 32) = 16384 dimensions.
/mnt/ceph/develop/jiawei/CogVideo/sat/vae_modules/autoencoder.py:565: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  sd = torch.load(path, map_location="cpu")["state_dict"]
Deleting key loss.logvar from state_dict.
Deleting key loss.perceptual_loss.scaling_layer.shift from state_dict.
Deleting key loss.perceptual_loss.scaling_layer.scale from state_dict.
Deleting key loss.perceptual_loss.net.slice1.0.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice1.0.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice1.2.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice1.2.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice2.5.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice2.5.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice2.7.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice2.7.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice3.10.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice3.10.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice3.12.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice3.12.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice3.14.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice3.14.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice4.17.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice4.17.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice4.19.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice4.19.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice4.21.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice4.21.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice5.24.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice5.24.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice5.26.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice5.26.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice5.28.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice5.28.bias from state_dict.
Deleting key loss.perceptual_loss.lin0.model.1.weight from state_dict.
Deleting key loss.perceptual_loss.lin1.model.1.weight from state_dict.
Deleting key loss.perceptual_loss.lin2.model.1.weight from state_dict.
Deleting key loss.perceptual_loss.lin3.model.1.weight from state_dict.
Deleting key loss.perceptual_loss.lin4.model.1.weight from state_dict.
Deleting key loss.discriminator.blocks.0.downsample_res.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.0.downsample_res.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.0.net.0.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.0.net.0.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.0.net.2.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.0.net.2.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.0.downsample.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.0.downsample.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.1.downsample_res.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.1.downsample_res.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.1.net.0.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.1.net.0.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.1.net.2.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.1.net.2.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.1.downsample.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.1.downsample.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.2.downsample_res.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.2.downsample_res.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.2.net.0.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.2.net.0.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.2.net.2.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.2.net.2.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.2.downsample.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.2.downsample.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.3.downsample_res.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.3.downsample_res.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.3.net.0.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.3.net.0.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.3.net.2.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.3.net.2.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.3.downsample.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.3.downsample.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.4.0.conv_res.weight from state_dict.
Deleting key loss.discriminator.blocks.4.0.conv_res.bias from state_dict.
Deleting key loss.discriminator.blocks.4.0.net.0.weight from state_dict.
Deleting key loss.discriminator.blocks.4.0.net.0.bias from state_dict.
Deleting key loss.discriminator.blocks.4.0.net.2.weight from state_dict.
Deleting key loss.discriminator.blocks.4.0.net.2.bias from state_dict.
Deleting key loss.discriminator.blocks.4.0.downsample.1.weight from state_dict.
Deleting key loss.discriminator.blocks.4.0.downsample.1.bias from state_dict.
Deleting key loss.discriminator.blocks.4.1.0.fn.norm.gamma from state_dict.
Deleting key loss.discriminator.blocks.4.1.0.fn.attn.to_q.0.weight from state_dict.
Deleting key loss.discriminator.blocks.4.1.0.fn.attn.to_kv.0.weight from state_dict.
Deleting key loss.discriminator.blocks.4.1.0.fn.attn.to_out.0.weight from state_dict.
Deleting key loss.discriminator.blocks.4.1.1.fn.norm.gamma from state_dict.
Deleting key loss.discriminator.blocks.4.1.1.fn.net.0.weight from state_dict.
Deleting key loss.discriminator.blocks.4.1.1.fn.net.0.bias from state_dict.
Deleting key loss.discriminator.blocks.4.1.1.fn.net.2.weight from state_dict.
Deleting key loss.discriminator.blocks.4.1.1.fn.net.2.bias from state_dict.
Deleting key loss.discriminator.blocks.5.0.conv_res.weight from state_dict.
Deleting key loss.discriminator.blocks.5.0.conv_res.bias from state_dict.
Deleting key loss.discriminator.blocks.5.0.net.0.weight from state_dict.
Deleting key loss.discriminator.blocks.5.0.net.0.bias from state_dict.
Deleting key loss.discriminator.blocks.5.0.net.2.weight from state_dict.
Deleting key loss.discriminator.blocks.5.0.net.2.bias from state_dict.
Deleting key loss.discriminator.blocks.5.0.downsample.1.weight from state_dict.
Deleting key loss.discriminator.blocks.5.0.downsample.1.bias from state_dict.
Deleting key loss.discriminator.blocks.5.1.0.fn.norm.gamma from state_dict.
Deleting key loss.discriminator.blocks.5.1.0.fn.attn.to_q.0.weight from state_dict.
Deleting key loss.discriminator.blocks.5.1.0.fn.attn.to_kv.0.weight from state_dict.
Deleting key loss.discriminator.blocks.5.1.0.fn.attn.to_out.0.weight from state_dict.
Deleting key loss.discriminator.blocks.5.1.1.fn.norm.gamma from state_dict.
Deleting key loss.discriminator.blocks.5.1.1.fn.net.0.weight from state_dict.
Deleting key loss.discriminator.blocks.5.1.1.fn.net.0.bias from state_dict.
Deleting key loss.discriminator.blocks.5.1.1.fn.net.2.weight from state_dict.
Deleting key loss.discriminator.blocks.5.1.1.fn.net.2.bias from state_dict.
Deleting key loss.discriminator.blocks.6.0.conv_res.weight from state_dict.
Deleting key loss.discriminator.blocks.6.0.conv_res.bias from state_dict.
Deleting key loss.discriminator.blocks.6.0.net.0.weight from state_dict.
Deleting key loss.discriminator.blocks.6.0.net.0.bias from state_dict.
Deleting key loss.discriminator.blocks.6.0.net.2.weight from state_dict.
Deleting key loss.discriminator.blocks.6.0.net.2.bias from state_dict.
Deleting key loss.discriminator.blocks.6.1.0.fn.norm.gamma from state_dict.
Deleting key loss.discriminator.blocks.6.1.0.fn.attn.to_q.0.weight from state_dict.
Deleting key loss.discriminator.blocks.6.1.0.fn.attn.to_kv.0.weight from state_dict.
Deleting key loss.discriminator.blocks.6.1.0.fn.attn.to_out.0.weight from state_dict.
Deleting key loss.discriminator.blocks.6.1.1.fn.norm.gamma from state_dict.
Deleting key loss.discriminator.blocks.6.1.1.fn.net.0.weight from state_dict.
Deleting key loss.discriminator.blocks.6.1.1.fn.net.0.bias from state_dict.
Deleting key loss.discriminator.blocks.6.1.1.fn.net.2.weight from state_dict.
Deleting key loss.discriminator.blocks.6.1.1.fn.net.2.bias from state_dict.
Deleting key loss.discriminator.to_logits.0.weight from state_dict.
Deleting key loss.discriminator.to_logits.0.bias from state_dict.
Deleting key loss.discriminator.to_logits.3.weight from state_dict.
Deleting key loss.discriminator.to_logits.3.bias from state_dict.
Missing keys:  []
Unexpected keys:  []
Restored from /mnt/ceph/develop/jiawei/model_checkpoint/CogVideoX-2b-sat/vae/3d-vae.pt
[2024-09-09 18:26:36,578] [INFO] [RANK 0]  > number of parameters on model parallel rank 0: 6768288995
[2024-09-09 18:26:42,966] [INFO] [RANK 0] global rank 0 is loading checkpoint /mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora/lora-disney-09-09-14-22/500/mp_rank_00_model_states.pt
/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/sat/training/model_io.py:286: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  sd = torch.load(checkpoint_name, map_location='cpu')
[rank0]: Traceback (most recent call last):
[rank0]:   File "/mnt/ceph/develop/jiawei/CogVideo/sat/train_video.py", line 223, in <module>
[rank0]:     training_main(
[rank0]:   File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/sat/training/deepspeed_training.py", line 98, in training_main
[rank0]:     args.iteration = load_checkpoint(model, args)
[rank0]:   File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/sat/training/model_io.py", line 304, in load_checkpoint
[rank0]:     missing_keys, unexpected_keys = module.load_state_dict(sd['module'], strict=False)
[rank0]:   File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/nn/modules/module.py", line 2215, in load_state_dict
[rank0]:     raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
[rank0]: RuntimeError: Error(s) in loading state_dict for SATVideoDiffusionEngine:
[rank0]: 	size mismatch for model.diffusion_model.mixins.pos_embed.pos_embedding: copying a param with shape torch.Size([1, 17776, 1920]) from checkpoint, the shape in current model is torch.Size([1, 19598, 1920]).
E0909 18:28:58.650000 140349962180416 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 0 (pid: 2049966) of binary: /mnt/ceph/develop/jiawei/conda_env/cogvideo/bin/python
Traceback (most recent call last):
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train_video.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-09-09_18:28:58
  host      : nm04-a800-node083
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 2049966)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
DONE on nm04-a800-node083
RUN on nm04-a800-node083, CUDA_VISIBLE_DEVICES=0
torchrun --standalone --nproc_per_node=1 train_video.py --base configs/cogvideox_2b_lora.yaml configs/sft.yaml --seed 42
[2024-09-09 18:35:57,496] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
[93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, input, weight, bias=None):
/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/kornia/feature/lightglue.py:44: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)
no module 'xformers'. Processing without...
no module 'xformers'. Processing without...
[2024-09-09 18:36:19,611] [INFO] using world size: 1
[2024-09-09 18:36:19,611] [INFO] Will override arguments with manually specified deepspeed_config!
[2024-09-09 18:36:19,618] [INFO] [RANK 0] > initializing model parallel with size 1
[2024-09-09 18:36:19,621] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-09-09 18:36:21,097] [INFO] [RANK 0] building SATVideoDiffusionEngine model ...
[2024-09-09 18:36:31,393] [WARNING] [RANK 0] Failed to load bitsandbytes:No module named 'bitsandbytes'
[2024-09-09 18:36:31,394] [INFO] [RANK 0] replacing layer 0 attention with lora
[2024-09-09 18:36:31,421] [INFO] [RANK 0] replacing layer 1 attention with lora
[2024-09-09 18:36:31,446] [INFO] [RANK 0] replacing layer 2 attention with lora
[2024-09-09 18:36:31,470] [INFO] [RANK 0] replacing layer 3 attention with lora
[2024-09-09 18:36:31,494] [INFO] [RANK 0] replacing layer 4 attention with lora
[2024-09-09 18:36:31,517] [INFO] [RANK 0] replacing layer 5 attention with lora
[2024-09-09 18:36:31,541] [INFO] [RANK 0] replacing layer 6 attention with lora
[2024-09-09 18:36:31,567] [INFO] [RANK 0] replacing layer 7 attention with lora
[2024-09-09 18:36:31,590] [INFO] [RANK 0] replacing layer 8 attention with lora
[2024-09-09 18:36:31,614] [INFO] [RANK 0] replacing layer 9 attention with lora
[2024-09-09 18:36:31,638] [INFO] [RANK 0] replacing layer 10 attention with lora
[2024-09-09 18:36:31,661] [INFO] [RANK 0] replacing layer 11 attention with lora
[2024-09-09 18:36:31,684] [INFO] [RANK 0] replacing layer 12 attention with lora
[2024-09-09 18:36:31,707] [INFO] [RANK 0] replacing layer 13 attention with lora
[2024-09-09 18:36:31,731] [INFO] [RANK 0] replacing layer 14 attention with lora
[2024-09-09 18:36:31,753] [INFO] [RANK 0] replacing layer 15 attention with lora
[2024-09-09 18:36:31,777] [INFO] [RANK 0] replacing layer 16 attention with lora
[2024-09-09 18:36:31,800] [INFO] [RANK 0] replacing layer 17 attention with lora
[2024-09-09 18:36:31,823] [INFO] [RANK 0] replacing layer 18 attention with lora
[2024-09-09 18:36:31,842] [INFO] [RANK 0] replacing layer 19 attention with lora
[2024-09-09 18:36:31,861] [INFO] [RANK 0] replacing layer 20 attention with lora
[2024-09-09 18:36:31,880] [INFO] [RANK 0] replacing layer 21 attention with lora
[2024-09-09 18:36:31,902] [INFO] [RANK 0] replacing layer 22 attention with lora
[2024-09-09 18:36:31,925] [INFO] [RANK 0] replacing layer 23 attention with lora
[2024-09-09 18:36:31,947] [INFO] [RANK 0] replacing layer 24 attention with lora
[2024-09-09 18:36:31,969] [INFO] [RANK 0] replacing layer 25 attention with lora
[2024-09-09 18:36:31,992] [INFO] [RANK 0] replacing layer 26 attention with lora
[2024-09-09 18:36:32,015] [INFO] [RANK 0] replacing layer 27 attention with lora
[2024-09-09 18:36:32,037] [INFO] [RANK 0] replacing layer 28 attention with lora
[2024-09-09 18:36:32,059] [INFO] [RANK 0] replacing layer 29 attention with lora
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  1.54it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.60it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.59it/s]
Initialized embedder #0: FrozenT5Embedder with 4762310656 params. Trainable: False
Working with z of shape (1, 16, 32, 32) = 16384 dimensions.
/mnt/ceph/develop/jiawei/CogVideo/sat/vae_modules/autoencoder.py:565: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  sd = torch.load(path, map_location="cpu")["state_dict"]
Deleting key loss.logvar from state_dict.
Deleting key loss.perceptual_loss.scaling_layer.shift from state_dict.
Deleting key loss.perceptual_loss.scaling_layer.scale from state_dict.
Deleting key loss.perceptual_loss.net.slice1.0.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice1.0.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice1.2.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice1.2.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice2.5.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice2.5.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice2.7.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice2.7.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice3.10.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice3.10.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice3.12.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice3.12.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice3.14.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice3.14.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice4.17.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice4.17.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice4.19.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice4.19.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice4.21.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice4.21.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice5.24.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice5.24.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice5.26.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice5.26.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice5.28.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice5.28.bias from state_dict.
Deleting key loss.perceptual_loss.lin0.model.1.weight from state_dict.
Deleting key loss.perceptual_loss.lin1.model.1.weight from state_dict.
Deleting key loss.perceptual_loss.lin2.model.1.weight from state_dict.
Deleting key loss.perceptual_loss.lin3.model.1.weight from state_dict.
Deleting key loss.perceptual_loss.lin4.model.1.weight from state_dict.
Deleting key loss.discriminator.blocks.0.downsample_res.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.0.downsample_res.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.0.net.0.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.0.net.0.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.0.net.2.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.0.net.2.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.0.downsample.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.0.downsample.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.1.downsample_res.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.1.downsample_res.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.1.net.0.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.1.net.0.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.1.net.2.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.1.net.2.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.1.downsample.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.1.downsample.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.2.downsample_res.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.2.downsample_res.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.2.net.0.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.2.net.0.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.2.net.2.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.2.net.2.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.2.downsample.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.2.downsample.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.3.downsample_res.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.3.downsample_res.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.3.net.0.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.3.net.0.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.3.net.2.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.3.net.2.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.3.downsample.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.3.downsample.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.4.0.conv_res.weight from state_dict.
Deleting key loss.discriminator.blocks.4.0.conv_res.bias from state_dict.
Deleting key loss.discriminator.blocks.4.0.net.0.weight from state_dict.
Deleting key loss.discriminator.blocks.4.0.net.0.bias from state_dict.
Deleting key loss.discriminator.blocks.4.0.net.2.weight from state_dict.
Deleting key loss.discriminator.blocks.4.0.net.2.bias from state_dict.
Deleting key loss.discriminator.blocks.4.0.downsample.1.weight from state_dict.
Deleting key loss.discriminator.blocks.4.0.downsample.1.bias from state_dict.
Deleting key loss.discriminator.blocks.4.1.0.fn.norm.gamma from state_dict.
Deleting key loss.discriminator.blocks.4.1.0.fn.attn.to_q.0.weight from state_dict.
Deleting key loss.discriminator.blocks.4.1.0.fn.attn.to_kv.0.weight from state_dict.
Deleting key loss.discriminator.blocks.4.1.0.fn.attn.to_out.0.weight from state_dict.
Deleting key loss.discriminator.blocks.4.1.1.fn.norm.gamma from state_dict.
Deleting key loss.discriminator.blocks.4.1.1.fn.net.0.weight from state_dict.
Deleting key loss.discriminator.blocks.4.1.1.fn.net.0.bias from state_dict.
Deleting key loss.discriminator.blocks.4.1.1.fn.net.2.weight from state_dict.
Deleting key loss.discriminator.blocks.4.1.1.fn.net.2.bias from state_dict.
Deleting key loss.discriminator.blocks.5.0.conv_res.weight from state_dict.
Deleting key loss.discriminator.blocks.5.0.conv_res.bias from state_dict.
Deleting key loss.discriminator.blocks.5.0.net.0.weight from state_dict.
Deleting key loss.discriminator.blocks.5.0.net.0.bias from state_dict.
Deleting key loss.discriminator.blocks.5.0.net.2.weight from state_dict.
Deleting key loss.discriminator.blocks.5.0.net.2.bias from state_dict.
Deleting key loss.discriminator.blocks.5.0.downsample.1.weight from state_dict.
Deleting key loss.discriminator.blocks.5.0.downsample.1.bias from state_dict.
Deleting key loss.discriminator.blocks.5.1.0.fn.norm.gamma from state_dict.
Deleting key loss.discriminator.blocks.5.1.0.fn.attn.to_q.0.weight from state_dict.
Deleting key loss.discriminator.blocks.5.1.0.fn.attn.to_kv.0.weight from state_dict.
Deleting key loss.discriminator.blocks.5.1.0.fn.attn.to_out.0.weight from state_dict.
Deleting key loss.discriminator.blocks.5.1.1.fn.norm.gamma from state_dict.
Deleting key loss.discriminator.blocks.5.1.1.fn.net.0.weight from state_dict.
Deleting key loss.discriminator.blocks.5.1.1.fn.net.0.bias from state_dict.
Deleting key loss.discriminator.blocks.5.1.1.fn.net.2.weight from state_dict.
Deleting key loss.discriminator.blocks.5.1.1.fn.net.2.bias from state_dict.
Deleting key loss.discriminator.blocks.6.0.conv_res.weight from state_dict.
Deleting key loss.discriminator.blocks.6.0.conv_res.bias from state_dict.
Deleting key loss.discriminator.blocks.6.0.net.0.weight from state_dict.
Deleting key loss.discriminator.blocks.6.0.net.0.bias from state_dict.
Deleting key loss.discriminator.blocks.6.0.net.2.weight from state_dict.
Deleting key loss.discriminator.blocks.6.0.net.2.bias from state_dict.
Deleting key loss.discriminator.blocks.6.1.0.fn.norm.gamma from state_dict.
Deleting key loss.discriminator.blocks.6.1.0.fn.attn.to_q.0.weight from state_dict.
Deleting key loss.discriminator.blocks.6.1.0.fn.attn.to_kv.0.weight from state_dict.
Deleting key loss.discriminator.blocks.6.1.0.fn.attn.to_out.0.weight from state_dict.
Deleting key loss.discriminator.blocks.6.1.1.fn.norm.gamma from state_dict.
Deleting key loss.discriminator.blocks.6.1.1.fn.net.0.weight from state_dict.
Deleting key loss.discriminator.blocks.6.1.1.fn.net.0.bias from state_dict.
Deleting key loss.discriminator.blocks.6.1.1.fn.net.2.weight from state_dict.
Deleting key loss.discriminator.blocks.6.1.1.fn.net.2.bias from state_dict.
Deleting key loss.discriminator.to_logits.0.weight from state_dict.
Deleting key loss.discriminator.to_logits.0.bias from state_dict.
Deleting key loss.discriminator.to_logits.3.weight from state_dict.
Deleting key loss.discriminator.to_logits.3.bias from state_dict.
Missing keys:  []
Unexpected keys:  []
Restored from /mnt/ceph/develop/jiawei/model_checkpoint/CogVideoX-2b-sat/vae/3d-vae.pt
[2024-09-09 18:36:36,241] [INFO] [RANK 0]  > number of parameters on model parallel rank 0: 6768288995
[2024-09-09 18:36:42,927] [INFO] [RANK 0] global rank 0 is loading checkpoint /mnt/ceph/develop/jiawei/model_checkpoint/CogVideoX-2b-sat/transformer/1000/mp_rank_00_model_states.pt
/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/sat/training/model_io.py:286: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  sd = torch.load(checkpoint_name, map_location='cpu')
[2024-09-09 18:36:45,104] [INFO] [RANK 0] > successfully loaded /mnt/ceph/develop/jiawei/model_checkpoint/CogVideoX-2b-sat/transformer/1000/mp_rank_00_model_states.pt
[2024-09-09 18:36:45,430] [INFO] [RANK 0] ***** Total trainable parameters: 58982400 *****
[2024-09-09 18:36:45,430] [INFO] [RANK 0] [<class 'sat.ops.layernorm.LayerNorm'>, <class 'torch.nn.modules.normalization.LayerNorm'>, <class 'sat.ops.layernorm.RMSNorm'>] is set to no_weight_decay
[2024-09-09 18:36:45,435] [INFO] [RANK 0] Syncing initialized parameters...
[2024-09-09 18:36:45,497] [INFO] [RANK 0] Finished syncing initialized parameters.
[2024-09-09 18:36:45,497] [INFO] [RANK 0] Using optimizer sat.ops.FusedEmaAdam from sat.
[2024-09-09 18:36:45,499] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.4, git-hash=unknown, git-branch=unknown
[2024-09-09 18:36:45,500] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
[2024-09-09 18:36:45,584] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /root/.cache/torch_extensions/py310_cu121/fused_ema_adam/build.ninja...
/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1965: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module fused_ema_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_ema_adam...
Time to load fused_ema_adam op: 4.735092878341675 seconds
[2024-09-09 18:36:50,357] [INFO] [logging.py:96:log_dist] [Rank 0] Using client callable to create basic optimizer
[2024-09-09 18:36:50,358] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-09-09 18:36:50,386] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedEmaAdam
[2024-09-09 18:36:50,386] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedEmaAdam type=<class 'sat.ops.fused_ema_adam.FusedEmaAdam'>
[2024-09-09 18:36:50,391] [WARNING] [engine.py:1179:_do_optimizer_sanity_check] **** You are using ZeRO with an untested optimizer, proceed with caution *****
[2024-09-09 18:36:50,391] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2024-09-09 18:36:50,391] [INFO] [stage_1_and_2.py:148:__init__] Reduce bucket size 1000000000
[2024-09-09 18:36:50,391] [INFO] [stage_1_and_2.py:149:__init__] Allgather bucket size 1000000000
[2024-09-09 18:36:50,391] [INFO] [stage_1_and_2.py:150:__init__] CPU Offload: False
[2024-09-09 18:36:50,393] [INFO] [stage_1_and_2.py:151:__init__] Round robin gradient partitioning: False
[2024-09-09 18:36:50,646] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2024-09-09 18:36:50,647] [INFO] [utils.py:782:see_memory_usage] MA 12.87 GB         Max_MA 12.98 GB         CA 13.23 GB         Max_CA 13 GB 
[2024-09-09 18:36:50,650] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 121.55 GB, percent = 6.0%
[2024-09-09 18:36:50,792] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2024-09-09 18:36:50,793] [INFO] [utils.py:782:see_memory_usage] MA 12.87 GB         Max_MA 13.09 GB         CA 13.45 GB         Max_CA 13 GB 
[2024-09-09 18:36:50,795] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 121.55 GB, percent = 6.0%
[2024-09-09 18:36:50,795] [INFO] [stage_1_and_2.py:543:__init__] optimizer state initialized
[2024-09-09 18:36:50,944] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2024-09-09 18:36:50,945] [INFO] [utils.py:782:see_memory_usage] MA 12.87 GB         Max_MA 12.87 GB         CA 13.45 GB         Max_CA 13 GB 
[2024-09-09 18:36:50,949] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 121.55 GB, percent = 6.0%
[2024-09-09 18:36:50,953] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer
[2024-09-09 18:36:50,955] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2024-09-09 18:36:50,955] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2024-09-09 18:36:50,955] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[1.0], mom=[[0.9, 0.95]]
[2024-09-09 18:36:50,958] [INFO] [config.py:997:print] DeepSpeedEngine configuration:
[2024-09-09 18:36:50,958] [INFO] [config.py:1001:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-09-09 18:36:50,958] [INFO] [config.py:1001:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-09-09 18:36:50,958] [INFO] [config.py:1001:print]   amp_enabled .................. False
[2024-09-09 18:36:50,958] [INFO] [config.py:1001:print]   amp_params ................... False
[2024-09-09 18:36:50,958] [INFO] [config.py:1001:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-09-09 18:36:50,964] [INFO] [config.py:1001:print]   bfloat16_enabled ............. False
[2024-09-09 18:36:50,964] [INFO] [config.py:1001:print]   bfloat16_immediate_grad_update  False
[2024-09-09 18:36:50,964] [INFO] [config.py:1001:print]   checkpoint_parallel_write_pipeline  False
[2024-09-09 18:36:50,964] [INFO] [config.py:1001:print]   checkpoint_tag_validation_enabled  True
[2024-09-09 18:36:50,964] [INFO] [config.py:1001:print]   checkpoint_tag_validation_fail  False
[2024-09-09 18:36:50,965] [INFO] [config.py:1001:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f7a28f6faf0>
[2024-09-09 18:36:50,965] [INFO] [config.py:1001:print]   communication_data_type ...... None
[2024-09-09 18:36:50,965] [INFO] [config.py:1001:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-09-09 18:36:50,965] [INFO] [config.py:1001:print]   curriculum_enabled_legacy .... False
[2024-09-09 18:36:50,965] [INFO] [config.py:1001:print]   curriculum_params_legacy ..... False
[2024-09-09 18:36:50,965] [INFO] [config.py:1001:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-09-09 18:36:50,965] [INFO] [config.py:1001:print]   data_efficiency_enabled ...... False
[2024-09-09 18:36:50,965] [INFO] [config.py:1001:print]   dataloader_drop_last ......... False
[2024-09-09 18:36:50,965] [INFO] [config.py:1001:print]   disable_allgather ............ False
[2024-09-09 18:36:50,965] [INFO] [config.py:1001:print]   dump_state ................... False
[2024-09-09 18:36:50,965] [INFO] [config.py:1001:print]   dynamic_loss_scale_args ...... None
[2024-09-09 18:36:50,965] [INFO] [config.py:1001:print]   eigenvalue_enabled ........... False
[2024-09-09 18:36:50,965] [INFO] [config.py:1001:print]   eigenvalue_gas_boundary_resolution  1
[2024-09-09 18:36:50,965] [INFO] [config.py:1001:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-09-09 18:36:50,965] [INFO] [config.py:1001:print]   eigenvalue_layer_num ......... 0
[2024-09-09 18:36:50,965] [INFO] [config.py:1001:print]   eigenvalue_max_iter .......... 100
[2024-09-09 18:36:50,965] [INFO] [config.py:1001:print]   eigenvalue_stability ......... 1e-06
[2024-09-09 18:36:50,965] [INFO] [config.py:1001:print]   eigenvalue_tol ............... 0.01
[2024-09-09 18:36:50,965] [INFO] [config.py:1001:print]   eigenvalue_verbose ........... False
[2024-09-09 18:36:50,965] [INFO] [config.py:1001:print]   elasticity_enabled ........... False
[2024-09-09 18:36:50,968] [INFO] [config.py:1001:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-09-09 18:36:50,968] [INFO] [config.py:1001:print]   fp16_auto_cast ............... False
[2024-09-09 18:36:50,968] [INFO] [config.py:1001:print]   fp16_enabled ................. True
[2024-09-09 18:36:50,968] [INFO] [config.py:1001:print]   fp16_master_weights_and_gradients  False
[2024-09-09 18:36:50,968] [INFO] [config.py:1001:print]   global_rank .................. 0
[2024-09-09 18:36:50,968] [INFO] [config.py:1001:print]   grad_accum_dtype ............. None
[2024-09-09 18:36:50,968] [INFO] [config.py:1001:print]   gradient_accumulation_steps .. 1
[2024-09-09 18:36:50,968] [INFO] [config.py:1001:print]   gradient_clipping ............ 0.1
[2024-09-09 18:36:50,968] [INFO] [config.py:1001:print]   gradient_predivide_factor .... 1.0
[2024-09-09 18:36:50,968] [INFO] [config.py:1001:print]   graph_harvesting ............. False
[2024-09-09 18:36:50,968] [INFO] [config.py:1001:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-09-09 18:36:50,968] [INFO] [config.py:1001:print]   initial_dynamic_scale ........ 65536
[2024-09-09 18:36:50,968] [INFO] [config.py:1001:print]   load_universal_checkpoint .... False
[2024-09-09 18:36:50,968] [INFO] [config.py:1001:print]   loss_scale ................... 0
[2024-09-09 18:36:50,968] [INFO] [config.py:1001:print]   memory_breakdown ............. False
[2024-09-09 18:36:50,968] [INFO] [config.py:1001:print]   mics_hierarchial_params_gather  False
[2024-09-09 18:36:50,968] [INFO] [config.py:1001:print]   mics_shard_size .............. -1
[2024-09-09 18:36:50,968] [INFO] [config.py:1001:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-09-09 18:36:50,969] [INFO] [config.py:1001:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-09-09 18:36:50,971] [INFO] [config.py:1001:print]   optimizer_legacy_fusion ...... False
[2024-09-09 18:36:50,971] [INFO] [config.py:1001:print]   optimizer_name ............... None
[2024-09-09 18:36:50,971] [INFO] [config.py:1001:print]   optimizer_params ............. None
[2024-09-09 18:36:50,971] [INFO] [config.py:1001:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-09-09 18:36:50,971] [INFO] [config.py:1001:print]   pld_enabled .................. False
[2024-09-09 18:36:50,971] [INFO] [config.py:1001:print]   pld_params ................... False
[2024-09-09 18:36:50,971] [INFO] [config.py:1001:print]   prescale_gradients ........... False
[2024-09-09 18:36:50,971] [INFO] [config.py:1001:print]   scheduler_name ............... None
[2024-09-09 18:36:50,971] [INFO] [config.py:1001:print]   scheduler_params ............. None
[2024-09-09 18:36:50,971] [INFO] [config.py:1001:print]   seq_parallel_communication_data_type  torch.float32
[2024-09-09 18:36:50,971] [INFO] [config.py:1001:print]   sparse_attention ............. None
[2024-09-09 18:36:50,971] [INFO] [config.py:1001:print]   sparse_gradients_enabled ..... False
[2024-09-09 18:36:50,971] [INFO] [config.py:1001:print]   steps_per_print .............. 50
[2024-09-09 18:36:50,971] [INFO] [config.py:1001:print]   timers_config ................ enabled=True synchronized=True
[2024-09-09 18:36:50,971] [INFO] [config.py:1001:print]   train_batch_size ............. 2
[2024-09-09 18:36:50,971] [INFO] [config.py:1001:print]   train_micro_batch_size_per_gpu  2
[2024-09-09 18:36:50,971] [INFO] [config.py:1001:print]   use_data_before_expert_parallel_  False
[2024-09-09 18:36:50,971] [INFO] [config.py:1001:print]   use_node_local_storage ....... False
[2024-09-09 18:36:50,971] [INFO] [config.py:1001:print]   wall_clock_breakdown ......... False
[2024-09-09 18:36:50,971] [INFO] [config.py:1001:print]   weight_quantization_config ... None
[2024-09-09 18:36:50,971] [INFO] [config.py:1001:print]   world_size ................... 1
[2024-09-09 18:36:50,971] [INFO] [config.py:1001:print]   zero_allow_untested_optimizer  True
[2024-09-09 18:36:50,971] [INFO] [config.py:1001:print]   zero_config .................. stage=2 contiguous_gradients=False reduce_scatter=True reduce_bucket_size=1000000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=1000000000 overlap_comm=True load_from_fp32_weights=False elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-09-09 18:36:50,971] [INFO] [config.py:1001:print]   zero_enabled ................. True
[2024-09-09 18:36:50,971] [INFO] [config.py:1001:print]   zero_force_ds_cpu_optimizer .. True
[2024-09-09 18:36:50,971] [INFO] [config.py:1001:print]   zero_optimization_stage ...... 2
[2024-09-09 18:36:50,971] [INFO] [config.py:987:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 2, 
    "gradient_accumulation_steps": 1, 
    "steps_per_print": 50, 
    "gradient_clipping": 0.1, 
    "zero_optimization": {
        "stage": 2, 
        "cpu_offload": false, 
        "contiguous_gradients": false, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 1.000000e+09, 
        "allgather_bucket_size": 1.000000e+09, 
        "load_from_fp32_weights": false
    }, 
    "zero_allow_untested_optimizer": true, 
    "bf16": {
        "enabled": false
    }, 
    "fp16": {
        "enabled": true
    }, 
    "loss_scale": 0, 
    "loss_scale_window": 400, 
    "hysteresis": 2, 
    "min_loss_scale": 1, 
    "activation_checkpointing": {
        "partition_activations": false, 
        "contiguous_memory_optimization": false
    }, 
    "wall_clock_breakdown": false
}
[2024-09-09 18:36:50,972] [INFO] [RANK 0] learning rate decaying style linear, ratio 10.0
[2024-09-09 18:36:50,975] [INFO] [RANK 0] Finetuning Model...
[2024-09-09 18:36:50,975] [INFO] [RANK 0] arguments:
[2024-09-09 18:36:50,975] [INFO] [RANK 0]   base ......................... ['configs/cogvideox_2b_lora.yaml', 'configs/sft.yaml']
[2024-09-09 18:36:50,975] [INFO] [RANK 0]   model_parallel_size .......... 1
[2024-09-09 18:36:50,975] [INFO] [RANK 0]   force_pretrain ............... False
[2024-09-09 18:36:50,975] [INFO] [RANK 0]   device ....................... 0
[2024-09-09 18:36:50,975] [INFO] [RANK 0]   debug ........................ False
[2024-09-09 18:36:50,975] [INFO] [RANK 0]   log_image .................... True
[2024-09-09 18:36:50,975] [INFO] [RANK 0]   output_dir ................... samples
[2024-09-09 18:36:50,975] [INFO] [RANK 0]   input_dir .................... None
[2024-09-09 18:36:50,975] [INFO] [RANK 0]   input_type ................... cli
[2024-09-09 18:36:50,975] [INFO] [RANK 0]   input_file ................... input.txt
[2024-09-09 18:36:50,975] [INFO] [RANK 0]   final_size ................... 2048
[2024-09-09 18:36:50,975] [INFO] [RANK 0]   sdedit ....................... False
[2024-09-09 18:36:50,975] [INFO] [RANK 0]   grid_num_rows ................ 1
[2024-09-09 18:36:50,975] [INFO] [RANK 0]   force_inference .............. False
[2024-09-09 18:36:50,976] [INFO] [RANK 0]   lcm_steps .................... None
[2024-09-09 18:36:50,976] [INFO] [RANK 0]   sampling_num_frames .......... 32
[2024-09-09 18:36:50,976] [INFO] [RANK 0]   sampling_fps ................. 8
[2024-09-09 18:36:50,979] [INFO] [RANK 0]   only_save_latents ............ False
[2024-09-09 18:36:50,979] [INFO] [RANK 0]   only_log_video_latents ....... False
[2024-09-09 18:36:50,979] [INFO] [RANK 0]   latent_channels .............. 32
[2024-09-09 18:36:50,979] [INFO] [RANK 0]   image2video .................. False
[2024-09-09 18:36:50,979] [INFO] [RANK 0]   experiment_name .............. lora-disney-09-09-18-36
[2024-09-09 18:36:50,979] [INFO] [RANK 0]   train_iters .................. 1000
[2024-09-09 18:36:50,979] [INFO] [RANK 0]   batch_size ................... 2
[2024-09-09 18:36:50,979] [INFO] [RANK 0]   lr ........................... 0.0005
[2024-09-09 18:36:50,979] [INFO] [RANK 0]   mode ......................... finetune
[2024-09-09 18:36:50,979] [INFO] [RANK 0]   seed ......................... 42
[2024-09-09 18:36:50,979] [INFO] [RANK 0]   zero_stage ................... 0
[2024-09-09 18:36:50,979] [INFO] [RANK 0]   checkpoint_activations ....... True
[2024-09-09 18:36:50,979] [INFO] [RANK 0]   checkpoint_num_layers ........ 1
[2024-09-09 18:36:50,979] [INFO] [RANK 0]   checkpoint_skip_layers ....... 0
[2024-09-09 18:36:50,979] [INFO] [RANK 0]   fp16 ......................... True
[2024-09-09 18:36:50,979] [INFO] [RANK 0]   bf16 ......................... False
[2024-09-09 18:36:50,979] [INFO] [RANK 0]   gradient_accumulation_steps .. 1
[2024-09-09 18:36:50,979] [INFO] [RANK 0]   profiling .................... -1
[2024-09-09 18:36:50,979] [INFO] [RANK 0]   epochs ....................... None
[2024-09-09 18:36:50,979] [INFO] [RANK 0]   log_interval ................. 20
[2024-09-09 18:36:50,979] [INFO] [RANK 0]   summary_dir .................. /mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora
[2024-09-09 18:36:50,979] [INFO] [RANK 0]   save_args .................... False
[2024-09-09 18:36:50,979] [INFO] [RANK 0]   lr_decay_iters ............... None
[2024-09-09 18:36:50,979] [INFO] [RANK 0]   lr_decay_style ............... linear
[2024-09-09 18:36:50,979] [INFO] [RANK 0]   lr_decay_ratio ............... 0.1
[2024-09-09 18:36:50,979] [INFO] [RANK 0]   warmup ....................... 0.01
[2024-09-09 18:36:50,979] [INFO] [RANK 0]   weight_decay ................. 5e-06
[2024-09-09 18:36:50,979] [INFO] [RANK 0]   save ......................... /mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora/lora-disney-09-09-18-36
[2024-09-09 18:36:50,979] [INFO] [RANK 0]   load ......................... /mnt/ceph/develop/jiawei/model_checkpoint/CogVideoX-2b-sat/transformer
[2024-09-09 18:36:50,979] [INFO] [RANK 0]   force_train .................. True
[2024-09-09 18:36:50,979] [INFO] [RANK 0]   save_interval ................ 100
[2024-09-09 18:36:50,979] [INFO] [RANK 0]   no_save_rng .................. False
[2024-09-09 18:36:50,979] [INFO] [RANK 0]   no_load_rng .................. True
[2024-09-09 18:36:50,979] [INFO] [RANK 0]   resume_dataloader ............ False
[2024-09-09 18:36:50,979] [INFO] [RANK 0]   distributed_backend .......... nccl
[2024-09-09 18:36:50,979] [INFO] [RANK 0]   local_rank ................... 0
[2024-09-09 18:36:50,979] [INFO] [RANK 0]   exit_interval ................ None
[2024-09-09 18:36:50,979] [INFO] [RANK 0]   wandb ........................ True
[2024-09-09 18:36:50,979] [INFO] [RANK 0]   wandb_project_name ........... cogv_2b_lora_base_002
[2024-09-09 18:36:50,979] [INFO] [RANK 0]   eval_batch_size .............. 1
[2024-09-09 18:36:50,979] [INFO] [RANK 0]   eval_iters ................... 1
[2024-09-09 18:36:50,980] [INFO] [RANK 0]   eval_interval ................ 100
[2024-09-09 18:36:50,980] [INFO] [RANK 0]   strict_eval .................. False
[2024-09-09 18:36:50,980] [INFO] [RANK 0]   train_data ................... ['/mnt/ceph/develop/jiawei/lora_dataset/cogvideo_lora_dataset']
[2024-09-09 18:36:50,980] [INFO] [RANK 0]   train_data_weights ........... None
[2024-09-09 18:36:50,980] [INFO] [RANK 0]   iterable_dataset ............. False
[2024-09-09 18:36:50,980] [INFO] [RANK 0]   iterable_dataset_eval ........ 
[2024-09-09 18:36:50,980] [INFO] [RANK 0]   batch_from_same_dataset ...... False
[2024-09-09 18:36:50,980] [INFO] [RANK 0]   valid_data ................... ['/mnt/ceph/develop/jiawei/lora_dataset/cogvideo_lora_dataset']
[2024-09-09 18:36:50,980] [INFO] [RANK 0]   test_data .................... None
[2024-09-09 18:36:50,980] [INFO] [RANK 0]   split ........................ 1,0,0
[2024-09-09 18:36:50,980] [INFO] [RANK 0]   num_workers .................. 8
[2024-09-09 18:36:50,980] [INFO] [RANK 0]   block_size ................... 10000
[2024-09-09 18:36:50,980] [INFO] [RANK 0]   prefetch_factor .............. 4
[2024-09-09 18:36:50,980] [INFO] [RANK 0]   deepspeed .................... True
[2024-09-09 18:36:50,980] [INFO] [RANK 0]   deepspeed_config ............. {'train_micro_batch_size_per_gpu': 2, 'gradient_accumulation_steps': 1, 'steps_per_print': 50, 'gradient_clipping': 0.1, 'zero_optimization': {'stage': 2, 'cpu_offload': False, 'contiguous_gradients': False, 'overlap_comm': True, 'reduce_scatter': True, 'reduce_bucket_size': 1000000000, 'allgather_bucket_size': 1000000000, 'load_from_fp32_weights': False}, 'zero_allow_untested_optimizer': True, 'bf16': {'enabled': False}, 'fp16': {'enabled': True}, 'loss_scale': 0, 'loss_scale_window': 400, 'hysteresis': 2, 'min_loss_scale': 1, 'activation_checkpointing': {'partition_activations': False, 'contiguous_memory_optimization': False}, 'wall_clock_breakdown': False}
[2024-09-09 18:36:50,980] [INFO] [RANK 0]   deepscale .................... False
[2024-09-09 18:36:50,980] [INFO] [RANK 0]   deepscale_config ............. None
[2024-09-09 18:36:50,980] [INFO] [RANK 0]   model_config ................. {'scale_factor': 1.15258426, 'disable_first_stage_autocast': True, 'not_trainable_prefixes': ['all'], 'log_keys': ['txt'], 'denoiser_config': {'target': 'sgm.modules.diffusionmodules.denoiser.DiscreteDenoiser', 'params': {'num_idx': 1000, 'quantize_c_noise': False, 'weighting_config': {'target': 'sgm.modules.diffusionmodules.denoiser_weighting.EpsWeighting'}, 'scaling_config': {'target': 'sgm.modules.diffusionmodules.denoiser_scaling.VideoScaling'}, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 3.0}}}}, 'network_config': {'target': 'dit_video_concat.DiffusionTransformer', 'params': {'time_embed_dim': 512, 'elementwise_affine': True, 'num_frames': 49, 'time_compressed_rate': 4, 'latent_width': 90, 'latent_height': 60, 'num_layers': 30, 'patch_size': 2, 'in_channels': 16, 'out_channels': 16, 'hidden_size': 1920, 'adm_in_channels': 256, 'num_attention_heads': 30, 'transformer_args': {'checkpoint_activations': True, 'vocab_size': 1, 'max_sequence_length': 64, 'layernorm_order': 'pre', 'skip_init': False, 'model_parallel_size': 1, 'is_decoder': False, 'num_layers': 30, 'hidden_size': 1920, 'num_attention_heads': 30, 'parallel_output': True}, 'modules': {'pos_embed_config': {'target': 'dit_video_concat.Basic3DPositionEmbeddingMixin', 'params': {'text_length': 2048, 'height_interpolation': 1.875, 'width_interpolation': 1.875}}, 'lora_config': {'target': 'sat.model.finetune.lora2.LoraMixin', 'params': {'r': 128}}, 'patch_embed_config': {'target': 'dit_video_concat.ImagePatchEmbeddingMixin', 'params': {'text_hidden_size': 4096}}, 'adaln_layer_config': {'target': 'dit_video_concat.AdaLNMixin', 'params': {'qk_ln': True}}, 'final_layer_config': {'target': 'dit_video_concat.FinalLayerMixin'}}, 'dtype': 'fp16'}}, 'conditioner_config': {'target': 'sgm.modules.GeneralConditioner', 'params': {'emb_models': [{'is_trainable': False, 'input_key': 'txt', 'ucg_rate': 0.1, 'target': 'sgm.modules.encoders.modules.FrozenT5Embedder', 'params': {'model_dir': '/mnt/ceph/develop/jiawei/model_checkpoint/t5-v1_1-xxl', 'max_length': 2048}}]}}, 'first_stage_config': {'target': 'vae_modules.autoencoder.VideoAutoencoderInferenceWrapper', 'params': {'cp_size': 1, 'ckpt_path': '/mnt/ceph/develop/jiawei/model_checkpoint/CogVideoX-2b-sat/vae/3d-vae.pt', 'ignore_keys': ['loss'], 'loss_config': {'target': 'torch.nn.Identity'}, 'regularizer_config': {'target': 'vae_modules.regularizers.DiagonalGaussianRegularizer'}, 'encoder_config': {'target': 'vae_modules.cp_enc_dec.ContextParallelEncoder3D', 'params': {'double_z': True, 'z_channels': 16, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 2, 4], 'attn_resolutions': [], 'num_res_blocks': 3, 'dropout': 0.0, 'gather_norm': True}}, 'decoder_config': {'target': 'vae_modules.cp_enc_dec.ContextParallelDecoder3D', 'params': {'double_z': True, 'z_channels': 16, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 2, 4], 'attn_resolutions': [], 'num_res_blocks': 3, 'dropout': 0.0, 'gather_norm': False}}}}, 'loss_fn_config': {'target': 'sgm.modules.diffusionmodules.loss.VideoDiffusionLoss', 'params': {'offset_noise_level': 0, 'sigma_sampler_config': {'target': 'sgm.modules.diffusionmodules.sigma_sampling.DiscreteSampling', 'params': {'uniform_sampling': True, 'num_idx': 1000, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 3.0}}}}}}, 'sampler_config': {'target': 'sgm.modules.diffusionmodules.sampling.VPSDEDPMPP2MSampler', 'params': {'num_steps': 50, 'verbose': True, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 3.0}}, 'guider_config': {'target': 'sgm.modules.diffusionmodules.guiders.DynamicCFG', 'params': {'scale': 6, 'exp': 5, 'num_steps': 50}}}}}
[2024-09-09 18:36:50,980] [INFO] [RANK 0]   data_config .................. {'target': 'data_video.SFTDataset', 'params': {'video_size': [480, 720], 'fps': 8, 'max_num_frames': 49, 'skip_frms_num': 3.0}}
[2024-09-09 18:36:50,980] [INFO] [RANK 0]   cuda ......................... True
[2024-09-09 18:36:50,980] [INFO] [RANK 0]   rank ......................... 0
[2024-09-09 18:36:50,980] [INFO] [RANK 0]   world_size ................... 1
[2024-09-09 18:36:50,980] [INFO] [RANK 0]   deepspeed_activation_checkpointing  True
[2024-09-09 18:36:50,980] [INFO] [RANK 0]   master_ip .................... nm04-a800-node083
[2024-09-09 18:36:50,980] [INFO] [RANK 0]   master_port .................. 63839
[2024-09-09 18:36:50,980] [INFO] [RANK 0]   log_config ................... [{'model': {'scale_factor': 1.15258426, 'disable_first_stage_autocast': True, 'not_trainable_prefixes': ['all'], 'log_keys': ['txt'], 'denoiser_config': {'target': 'sgm.modules.diffusionmodules.denoiser.DiscreteDenoiser', 'params': {'num_idx': 1000, 'quantize_c_noise': False, 'weighting_config': {'target': 'sgm.modules.diffusionmodules.denoiser_weighting.EpsWeighting'}, 'scaling_config': {'target': 'sgm.modules.diffusionmodules.denoiser_scaling.VideoScaling'}, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 3.0}}}}, 'network_config': {'target': 'dit_video_concat.DiffusionTransformer', 'params': {'time_embed_dim': 512, 'elementwise_affine': True, 'num_frames': 49, 'time_compressed_rate': 4, 'latent_width': 90, 'latent_height': 60, 'num_layers': 30, 'patch_size': 2, 'in_channels': 16, 'out_channels': 16, 'hidden_size': 1920, 'adm_in_channels': 256, 'num_attention_heads': 30, 'transformer_args': {'checkpoint_activations': True, 'vocab_size': 1, 'max_sequence_length': 64, 'layernorm_order': 'pre', 'skip_init': False, 'model_parallel_size': 1, 'is_decoder': False}, 'modules': {'pos_embed_config': {'target': 'dit_video_concat.Basic3DPositionEmbeddingMixin', 'params': {'text_length': 2048, 'height_interpolation': 1.875, 'width_interpolation': 1.875}}, 'lora_config': {'target': 'sat.model.finetune.lora2.LoraMixin', 'params': {'r': 128}}, 'patch_embed_config': {'target': 'dit_video_concat.ImagePatchEmbeddingMixin', 'params': {'text_hidden_size': 4096}}, 'adaln_layer_config': {'target': 'dit_video_concat.AdaLNMixin', 'params': {'qk_ln': True}}, 'final_layer_config': {'target': 'dit_video_concat.FinalLayerMixin'}}}}, 'conditioner_config': {'target': 'sgm.modules.GeneralConditioner', 'params': {'emb_models': [{'is_trainable': False, 'input_key': 'txt', 'ucg_rate': 0.1, 'target': 'sgm.modules.encoders.modules.FrozenT5Embedder', 'params': {'model_dir': '/mnt/ceph/develop/jiawei/model_checkpoint/t5-v1_1-xxl', 'max_length': 2048}}]}}, 'first_stage_config': {'target': 'vae_modules.autoencoder.VideoAutoencoderInferenceWrapper', 'params': {'cp_size': 1, 'ckpt_path': '/mnt/ceph/develop/jiawei/model_checkpoint/CogVideoX-2b-sat/vae/3d-vae.pt', 'ignore_keys': ['loss'], 'loss_config': {'target': 'torch.nn.Identity'}, 'regularizer_config': {'target': 'vae_modules.regularizers.DiagonalGaussianRegularizer'}, 'encoder_config': {'target': 'vae_modules.cp_enc_dec.ContextParallelEncoder3D', 'params': {'double_z': True, 'z_channels': 16, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 2, 4], 'attn_resolutions': [], 'num_res_blocks': 3, 'dropout': 0.0, 'gather_norm': True}}, 'decoder_config': {'target': 'vae_modules.cp_enc_dec.ContextParallelDecoder3D', 'params': {'double_z': True, 'z_channels': 16, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 2, 4], 'attn_resolutions': [], 'num_res_blocks': 3, 'dropout': 0.0, 'gather_norm': False}}}}, 'loss_fn_config': {'target': 'sgm.modules.diffusionmodules.loss.VideoDiffusionLoss', 'params': {'offset_noise_level': 0, 'sigma_sampler_config': {'target': 'sgm.modules.diffusionmodules.sigma_sampling.DiscreteSampling', 'params': {'uniform_sampling': True, 'num_idx': 1000, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 3.0}}}}}}, 'sampler_config': {'target': 'sgm.modules.diffusionmodules.sampling.VPSDEDPMPP2MSampler', 'params': {'num_steps': 50, 'verbose': True, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 3.0}}, 'guider_config': {'target': 'sgm.modules.diffusionmodules.guiders.DynamicCFG', 'params': {'scale': 6, 'exp': 5, 'num_steps': 50}}}}}}, {'args': {'checkpoint_activations': True, 'model_parallel_size': 1, 'experiment_name': 'lora-disney', 'mode': 'finetune', 'load': '/mnt/ceph/develop/jiawei/model_checkpoint/CogVideoX-2b-sat/transformer', 'no_load_rng': True, 'train_iters': 1000, 'eval_iters': 1, 'eval_interval': 100, 'eval_batch_size': 1, 'save': '/mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora', 'save_interval': 100, 'log_interval': 20, 'train_data': ['/mnt/ceph/develop/jiawei/lora_dataset/cogvideo_lora_dataset'], 'valid_data': ['/mnt/ceph/develop/jiawei/lora_dataset/cogvideo_lora_dataset'], 'split': '1,0,0', 'num_workers': 8, 'force_train': True, 'only_log_video_latents': False, 'wandb': True, 'wandb_project_name': 'cogv_2b_lora_base_002', 'summary_dir': '/mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora'}, 'data': {'target': 'data_video.SFTDataset', 'params': {'video_size': [480, 720], 'fps': 8, 'max_num_frames': 49, 'skip_frms_num': 3.0}}, 'deepspeed': {'train_micro_batch_size_per_gpu': 2, 'gradient_accumulation_steps': 1, 'steps_per_print': 50, 'gradient_clipping': 0.1, 'zero_optimization': {'stage': 2, 'cpu_offload': False, 'contiguous_gradients': False, 'overlap_comm': True, 'reduce_scatter': True, 'reduce_bucket_size': 1000000000, 'allgather_bucket_size': 1000000000, 'load_from_fp32_weights': False}, 'zero_allow_untested_optimizer': True, 'bf16': {'enabled': False}, 'fp16': {'enabled': True}, 'loss_scale': 0, 'loss_scale_window': 400, 'hysteresis': 2, 'min_loss_scale': 1, 'optimizer': {'type': 'sat.ops.FusedEmaAdam', 'params': {'lr': '5E-4', 'betas': [0.9, 0.95], 'eps': '5E-8', 'weight_decay': '5E-6'}}, 'activation_checkpointing': {'partition_activations': False, 'contiguous_memory_optimization': False}, 'wall_clock_breakdown': False}}]
[2024-09-09 18:36:50,980] [INFO] [RANK 0]   do_train ..................... True
[2024-09-09 18:36:50,980] [INFO] [RANK 0]   val_last_shape ............... []
[2024-09-09 18:36:50,984] [INFO] [RANK 0]   val_drop_number .............. 0
[2024-09-09 18:36:50,984] [INFO] [RANK 0]   do_valid ..................... True
[2024-09-09 18:36:50,984] [INFO] [RANK 0]   do_test ...................... False
[2024-09-09 18:36:50,984] [INFO] [RANK 0]   iteration .................... 0
wandb: Currently logged in as: dmeck. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora/wandb/run-20240909_183655-gxrmwyct
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lora-disney-09-09-18-36
wandb: â­ï¸ View project at https://wandb.ai/dmeck/cogv_2b_lora_base_002
wandb: ðŸš€ View run at https://wandb.ai/dmeck/cogv_2b_lora_base_002/runs/gxrmwyct
[2024-09-09 18:37:30,996] [INFO] [checkpointing.py:541:forward] Activation Checkpointing Information
[2024-09-09 18:37:30,997] [INFO] [checkpointing.py:542:forward] ----Partition Activations False, CPU CHECKPOINTING False
[2024-09-09 18:37:30,999] [INFO] [checkpointing.py:543:forward] ----contiguous Memory Checkpointing False with None total layers
[2024-09-09 18:37:30,999] [INFO] [checkpointing.py:545:forward] ----Synchronization False
[2024-09-09 18:37:31,000] [INFO] [checkpointing.py:546:forward] ----Profiling time in checkpointing False
[2024-09-09 18:37:38,600] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4294967296, reducing to 2147483648
[2024-09-09 18:37:55,978] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2147483648, reducing to 1073741824
[2024-09-09 18:38:13,340] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1073741824, reducing to 536870912
[2024-09-09 18:38:30,691] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 536870912, reducing to 268435456
[2024-09-09 18:38:48,015] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 268435456, reducing to 134217728
[2024-09-09 18:39:22,956] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 134217728, reducing to 67108864
[2024-09-09 18:43:09,232] [INFO] [RANK 0]  iteration       20/    1000 | elapsed time per iteration (ms): 18379.1 | learning rate 2.500E-05 | total loss 2.418396E-01 | loss 2.418396E-01 | loss scale 67108864.0 |speed 6.53 samples/(min*GPU)
[2024-09-09 18:43:09,234] [INFO] [RANK 0] after 20 iterations memory (MB) | allocated: 13980.6455078125 | max allocated: 64459.90478515625 | cached: 24182.0 | max cached: 77972.0
[2024-09-09 18:43:09,237] [INFO] [RANK 0] time (ms) | forward: 12632.55 | backward: 5720.79 | allreduce: 0.00 | optimizer: 24.56 | data loader: 75.37
[2024-09-09 18:44:01,669] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 67108864, reducing to 33554432
[2024-09-09 18:44:36,343] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 33554432, reducing to 16777216
[2024-09-09 18:48:56,969] [INFO] [RANK 0]  iteration       40/    1000 | elapsed time per iteration (ms): 17386.9 | learning rate 2.500E-05 | total loss 2.100303E-01 | loss 2.100303E-01 | loss scale 16777216.0 |speed 6.90 samples/(min*GPU)
[2024-09-09 18:48:56,970] [INFO] [RANK 0] time (ms) | forward: 11632.32 | backward: 5726.07 | allreduce: 0.00 | optimizer: 27.07 | data loader: 0.32
[2024-09-09 18:51:50,992] [INFO] [logging.py:96:log_dist] [Rank 0] step=50, skipped=8, lr=[2.5e-05], mom=[[0.9, 0.95]]
[2024-09-09 18:54:45,280] [INFO] [RANK 0]  iteration       60/    1000 | elapsed time per iteration (ms): 17415.6 | learning rate 2.500E-05 | total loss 2.085692E-01 | loss 2.085692E-01 | loss scale 16777216.0 |speed 6.89 samples/(min*GPU)
[2024-09-09 18:54:45,281] [INFO] [RANK 0] time (ms) | forward: 11658.38 | backward: 5726.60 | allreduce: 0.00 | optimizer: 28.99 | data loader: 0.33
[2024-09-09 19:00:33,632] [INFO] [RANK 0]  iteration       80/    1000 | elapsed time per iteration (ms): 17417.6 | learning rate 2.500E-05 | total loss 2.619407E-01 | loss 2.619407E-01 | loss scale 16777216.0 |speed 6.89 samples/(min*GPU)
[2024-09-09 19:00:33,633] [INFO] [RANK 0] time (ms) | forward: 11662.99 | backward: 5725.86 | allreduce: 0.00 | optimizer: 27.60 | data loader: 0.30
[2024-09-09 19:06:21,284] [INFO] [logging.py:96:log_dist] [Rank 0] step=100, skipped=8, lr=[2.5e-05], mom=[[0.9, 0.95]]
[2024-09-09 19:06:21,285] [INFO] [RANK 0]  iteration      100/    1000 | elapsed time per iteration (ms): 17382.6 | learning rate 2.500E-05 | total loss 2.419559E-01 | loss 2.419559E-01 | loss scale 16777216.0 |speed 6.90 samples/(min*GPU)
[2024-09-09 19:06:21,286] [INFO] [RANK 0] time (ms) | forward: 11624.10 | backward: 5728.74 | allreduce: 0.00 | optimizer: 28.63 | data loader: 0.32
[2024-09-09 19:06:21,286] [INFO] [RANK 0] Saving Model...
[2024-09-09 19:06:27,732] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora/lora-disney-09-09-18-36/100/mp_rank_00_model_states.pt
[2024-09-09 19:06:27,732] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora/lora-disney-09-09-18-36/100/mp_rank_00_model_states.pt...
[2024-09-09 19:07:24,774] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora/lora-disney-09-09-18-36/100/mp_rank_00_model_states.pt.
[2024-09-09 19:07:25,154] [INFO] [RANK 0] Saving Ema Model...
[2024-09-09 19:07:31,028] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora/lora-disney-09-09-18-36/100-ema/mp_rank_00_model_states.pt
[2024-09-09 19:07:31,028] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora/lora-disney-09-09-18-36/100-ema/mp_rank_00_model_states.pt...
[2024-09-09 19:08:27,233] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora/lora-disney-09-09-18-36/100-ema/mp_rank_00_model_states.pt.
/mnt/ceph/develop/jiawei/CogVideo/sat/train_video.py:67: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at ../torch/csrc/autograd/init.cpp:733.)
  "dtype": torch.get_autocast_gpu_dtype(),
/mnt/ceph/develop/jiawei/CogVideo/sat/train_video.py:70: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.no_grad(), torch.cuda.amp.autocast(**gpu_autocast_kwargs):
##############################  Sampling setting  ##############################
Sampler: VPSDEDPMPP2MSampler
Discretization: ZeroSNRDDPMDiscretization
Guider: DynamicCFG
Sampling with VPSDEDPMPP2MSampler for 51 steps:   0%|          | 0/51 [00:00<?, ?it/s]Sampling with VPSDEDPMPP2MSampler for 51 steps:   2%|â–         | 1/51 [00:01<01:32,  1.85s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:   4%|â–         | 2/51 [00:03<01:33,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:   6%|â–Œ         | 3/51 [00:05<01:31,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:   8%|â–Š         | 4/51 [00:07<01:29,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  10%|â–‰         | 5/51 [00:09<01:27,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  12%|â–ˆâ–        | 6/51 [00:11<01:25,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  14%|â–ˆâ–Ž        | 7/51 [00:13<01:24,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  16%|â–ˆâ–Œ        | 8/51 [00:15<01:22,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  18%|â–ˆâ–Š        | 9/51 [00:17<01:20,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  20%|â–ˆâ–‰        | 10/51 [00:19<01:18,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  22%|â–ˆâ–ˆâ–       | 11/51 [00:20<01:16,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  24%|â–ˆâ–ˆâ–Ž       | 12/51 [00:22<01:14,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  25%|â–ˆâ–ˆâ–Œ       | 13/51 [00:24<01:12,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  27%|â–ˆâ–ˆâ–‹       | 14/51 [00:26<01:10,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  29%|â–ˆâ–ˆâ–‰       | 15/51 [00:28<01:08,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  31%|â–ˆâ–ˆâ–ˆâ–      | 16/51 [00:30<01:06,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 17/51 [00:32<01:05,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 18/51 [00:34<01:03,  1.93s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 19/51 [00:36<01:01,  1.93s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 20/51 [00:38<00:59,  1.92s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 21/51 [00:40<00:57,  1.92s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 22/51 [00:42<00:55,  1.92s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 23/51 [00:44<00:53,  1.92s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 24/51 [00:45<00:51,  1.92s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 25/51 [00:47<00:49,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 26/51 [00:49<00:47,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 27/51 [00:51<00:45,  1.92s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 28/51 [00:53<00:44,  1.92s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 29/51 [00:55<00:42,  1.92s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 30/51 [00:57<00:40,  1.92s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 31/51 [00:59<00:38,  1.92s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 32/51 [01:01<00:36,  1.92s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33/51 [01:03<00:34,  1.92s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 34/51 [01:05<00:32,  1.92s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 35/51 [01:07<00:30,  1.92s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 36/51 [01:08<00:28,  1.92s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 37/51 [01:10<00:26,  1.92s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/51 [01:12<00:24,  1.92s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 39/51 [01:14<00:23,  1.92s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 40/51 [01:16<00:21,  1.92s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 41/51 [01:18<00:19,  1.92s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 42/51 [01:20<00:17,  1.92s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 43/51 [01:22<00:15,  1.92s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 44/51 [01:24<00:13,  1.92s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 45/51 [01:26<00:11,  1.92s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 46/51 [01:28<00:09,  1.92s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 47/51 [01:30<00:07,  1.92s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 48/51 [01:31<00:05,  1.92s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 49/51 [01:33<00:03,  1.92s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 50/51 [01:35<00:01,  1.90s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 50/51 [01:35<00:01,  1.92s/it]
[2024-09-09 19:10:39,993] [INFO] [RANK 0] ----------------------------------------------------------------------------------------------------
[2024-09-09 19:10:39,993] [INFO] [RANK 0] -----------------------------------------------------------------------------------------------
[2024-09-09 19:10:39,999] [INFO] [RANK 0]  validation loss at iteration 100 | loss: 3.936713E-01 | PPL: 1.482413E+00 loss 3.936713E-01 |
[2024-09-09 19:10:39,999] [INFO] [RANK 0] -----------------------------------------------------------------------------------------------
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 100 is less than current step: 101. Dropping entry: {'Train/valid_ppl': 1.4824131610964224, 'Train/valid_loss': 0.39367127418518066, '_timestamp': 1725880239.999711}).
[2024-09-09 19:16:24,376] [INFO] [RANK 0]  iteration      120/    1000 | elapsed time per iteration (ms): 30154.5 | learning rate 4.485E-04 | total loss 2.564652E-01 | loss 2.564652E-01 | loss scale 16777216.0 |speed 3.98 samples/(min*GPU)
[2024-09-09 19:16:24,377] [INFO] [RANK 0] time (ms) | forward: 11459.65 | backward: 5728.96 | allreduce: 0.00 | optimizer: 29.06 | data loader: 0.39
[2024-09-09 19:22:12,764] [INFO] [RANK 0]  iteration      140/    1000 | elapsed time per iteration (ms): 17419.4 | learning rate 4.385E-04 | total loss 2.143668E-01 | loss 2.143668E-01 | loss scale 16777216.0 |speed 6.89 samples/(min*GPU)
[2024-09-09 19:22:12,766] [INFO] [RANK 0] time (ms) | forward: 11662.60 | backward: 5728.05 | allreduce: 0.00 | optimizer: 27.57 | data loader: 0.32
[2024-09-09 19:25:06,438] [INFO] [logging.py:96:log_dist] [Rank 0] step=150, skipped=8, lr=[0.00043400000000000003], mom=[[0.9, 0.95]]
[2024-09-09 19:28:00,366] [INFO] [RANK 0]  iteration      160/    1000 | elapsed time per iteration (ms): 17380.1 | learning rate 4.285E-04 | total loss 2.535489E-01 | loss 2.535489E-01 | loss scale 16777216.0 |speed 6.90 samples/(min*GPU)
[2024-09-09 19:28:00,368] [INFO] [RANK 0] time (ms) | forward: 11624.26 | backward: 5726.91 | allreduce: 0.00 | optimizer: 27.53 | data loader: 0.33
[2024-09-09 19:33:47,728] [INFO] [RANK 0]  iteration      180/    1000 | elapsed time per iteration (ms): 17368.1 | learning rate 4.185E-04 | total loss 2.144062E-01 | loss 2.144062E-01 | loss scale 16777216.0 |speed 6.91 samples/(min*GPU)
[2024-09-09 19:33:47,729] [INFO] [RANK 0] time (ms) | forward: 11610.55 | backward: 5728.74 | allreduce: 0.00 | optimizer: 27.55 | data loader: 0.33
[2024-09-09 19:39:35,194] [INFO] [logging.py:96:log_dist] [Rank 0] step=200, skipped=8, lr=[0.00040899999999999997], mom=[[0.9, 0.95]]
[2024-09-09 19:39:35,195] [INFO] [RANK 0]  iteration      200/    1000 | elapsed time per iteration (ms): 17373.4 | learning rate 4.085E-04 | total loss 2.172302E-01 | loss 2.172302E-01 | loss scale 16777216.0 |speed 6.91 samples/(min*GPU)
[2024-09-09 19:39:35,196] [INFO] [RANK 0] time (ms) | forward: 11616.97 | backward: 5727.85 | allreduce: 0.00 | optimizer: 27.37 | data loader: 0.32
[2024-09-09 19:39:35,201] [INFO] [RANK 0] Saving Model...
[2024-09-09 19:39:41,659] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora/lora-disney-09-09-18-36/200/mp_rank_00_model_states.pt
[2024-09-09 19:39:41,659] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora/lora-disney-09-09-18-36/200/mp_rank_00_model_states.pt...
[2024-09-09 19:40:37,583] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora/lora-disney-09-09-18-36/200/mp_rank_00_model_states.pt.
[2024-09-09 19:40:37,972] [INFO] [RANK 0] Saving Ema Model...
[2024-09-09 19:40:44,068] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora/lora-disney-09-09-18-36/200-ema/mp_rank_00_model_states.pt
[2024-09-09 19:40:44,068] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora/lora-disney-09-09-18-36/200-ema/mp_rank_00_model_states.pt...
[2024-09-09 19:41:39,782] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora/lora-disney-09-09-18-36/200-ema/mp_rank_00_model_states.pt.
##############################  Sampling setting  ##############################
Sampler: VPSDEDPMPP2MSampler
Discretization: ZeroSNRDDPMDiscretization
Guider: DynamicCFG
Sampling with VPSDEDPMPP2MSampler for 51 steps:   0%|          | 0/51 [00:00<?, ?it/s]Sampling with VPSDEDPMPP2MSampler for 51 steps:   2%|â–         | 1/51 [00:01<01:32,  1.85s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:   4%|â–         | 2/51 [00:03<01:33,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:   6%|â–Œ         | 3/51 [00:05<01:32,  1.93s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:   8%|â–Š         | 4/51 [00:07<01:30,  1.92s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  10%|â–‰         | 5/51 [00:09<01:28,  1.92s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  12%|â–ˆâ–        | 6/51 [00:11<01:26,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  14%|â–ˆâ–Ž        | 7/51 [00:13<01:24,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  16%|â–ˆâ–Œ        | 8/51 [00:15<01:22,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  18%|â–ˆâ–Š        | 9/51 [00:17<01:20,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  20%|â–ˆâ–‰        | 10/51 [00:19<01:18,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  22%|â–ˆâ–ˆâ–       | 11/51 [00:21<01:16,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  24%|â–ˆâ–ˆâ–Ž       | 12/51 [00:22<01:14,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  25%|â–ˆâ–ˆâ–Œ       | 13/51 [00:24<01:12,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  27%|â–ˆâ–ˆâ–‹       | 14/51 [00:26<01:10,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  29%|â–ˆâ–ˆâ–‰       | 15/51 [00:28<01:08,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  31%|â–ˆâ–ˆâ–ˆâ–      | 16/51 [00:30<01:06,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 17/51 [00:32<01:05,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 18/51 [00:34<01:03,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 19/51 [00:36<01:01,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 20/51 [00:38<00:59,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 21/51 [00:40<00:57,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 22/51 [00:42<00:55,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 23/51 [00:44<00:53,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 24/51 [00:45<00:51,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 25/51 [00:47<00:49,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 26/51 [00:49<00:47,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 27/51 [00:51<00:45,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 28/51 [00:53<00:44,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 29/51 [00:55<00:42,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 30/51 [00:57<00:40,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 31/51 [00:59<00:38,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 32/51 [01:01<00:36,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33/51 [01:03<00:34,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 34/51 [01:05<00:32,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 35/51 [01:06<00:30,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 36/51 [01:08<00:28,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 37/51 [01:10<00:26,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/51 [01:12<00:24,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 39/51 [01:14<00:22,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 40/51 [01:16<00:21,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 41/51 [01:18<00:19,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 42/51 [01:20<00:17,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 43/51 [01:22<00:15,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 44/51 [01:24<00:13,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 45/51 [01:26<00:11,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 46/51 [01:27<00:09,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 47/51 [01:29<00:07,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 48/51 [01:31<00:05,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 49/51 [01:33<00:03,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 50/51 [01:35<00:01,  1.90s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 50/51 [01:35<00:01,  1.91s/it]
[2024-09-09 19:43:37,426] [INFO] [RANK 0] ----------------------------------------------------------------------------------------------------
[2024-09-09 19:43:37,427] [INFO] [RANK 0] -----------------------------------------------------------------------------------------------
[2024-09-09 19:43:37,435] [INFO] [RANK 0]  validation loss at iteration 200 | loss: 1.867729E-01 | PPL: 1.205353E+00 loss 1.867729E-01 |
[2024-09-09 19:43:37,438] [INFO] [RANK 0] -----------------------------------------------------------------------------------------------
wandb: WARNING (User provided step: 200 is less than current step: 201. Dropping entry: {'Train/valid_ppl': 1.205353497615617, 'Train/valid_loss': 0.186772882938385, '_timestamp': 1725882217.4388273}).
[2024-09-09 19:49:25,430] [INFO] [RANK 0]  iteration      220/    1000 | elapsed time per iteration (ms): 29511.7 | learning rate 3.985E-04 | total loss 2.197954E-01 | loss 2.197954E-01 | loss scale 16777216.0 |speed 4.07 samples/(min*GPU)
[2024-09-09 19:49:25,431] [INFO] [RANK 0] time (ms) | forward: 11628.98 | backward: 5741.93 | allreduce: 0.00 | optimizer: 27.50 | data loader: 0.36
[2024-09-09 19:55:31,342] [INFO] [RANK 0]  iteration      240/    1000 | elapsed time per iteration (ms): 18295.6 | learning rate 3.885E-04 | total loss 2.059690E-01 | loss 2.059690E-01 | loss scale 16777216.0 |speed 6.56 samples/(min*GPU)
[2024-09-09 19:55:31,343] [INFO] [RANK 0] time (ms) | forward: 12479.08 | backward: 5787.92 | allreduce: 0.00 | optimizer: 27.43 | data loader: 0.32
[2024-09-09 19:58:33,627] [INFO] [logging.py:96:log_dist] [Rank 0] step=250, skipped=8, lr=[0.000384], mom=[[0.9, 0.95]]
[2024-09-09 20:01:35,751] [INFO] [RANK 0]  iteration      260/    1000 | elapsed time per iteration (ms): 18220.5 | learning rate 3.785E-04 | total loss 2.341353E-01 | loss 2.341353E-01 | loss scale 16777216.0 |speed 6.59 samples/(min*GPU)
[2024-09-09 20:01:35,752] [INFO] [RANK 0] time (ms) | forward: 12462.02 | backward: 5729.14 | allreduce: 0.00 | optimizer: 28.06 | data loader: 0.30
[2024-09-09 20:07:40,372] [INFO] [RANK 0]  iteration      280/    1000 | elapsed time per iteration (ms): 18231.1 | learning rate 3.685E-04 | total loss 2.363755E-01 | loss 2.363754E-01 | loss scale 16777216.0 |speed 6.58 samples/(min*GPU)
[2024-09-09 20:07:40,374] [INFO] [RANK 0] time (ms) | forward: 12474.07 | backward: 5727.73 | allreduce: 0.00 | optimizer: 27.76 | data loader: 0.33
[2024-09-09 20:13:44,906] [INFO] [logging.py:96:log_dist] [Rank 0] step=300, skipped=8, lr=[0.000359], mom=[[0.9, 0.95]]
[2024-09-09 20:13:44,907] [INFO] [RANK 0]  iteration      300/    1000 | elapsed time per iteration (ms): 18226.7 | learning rate 3.585E-04 | total loss 2.036849E-01 | loss 2.036849E-01 | loss scale 16777216.0 |speed 6.58 samples/(min*GPU)
[2024-09-09 20:13:44,910] [INFO] [RANK 0] time (ms) | forward: 12467.87 | backward: 5728.91 | allreduce: 0.00 | optimizer: 28.78 | data loader: 0.33
[2024-09-09 20:13:44,912] [INFO] [RANK 0] Saving Model...
[2024-09-09 20:13:51,290] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora/lora-disney-09-09-18-36/300/mp_rank_00_model_states.pt
[2024-09-09 20:13:51,290] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora/lora-disney-09-09-18-36/300/mp_rank_00_model_states.pt...
[2024-09-09 20:14:46,456] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora/lora-disney-09-09-18-36/300/mp_rank_00_model_states.pt.
[2024-09-09 20:14:46,901] [INFO] [RANK 0] Saving Ema Model...
[2024-09-09 20:14:53,137] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora/lora-disney-09-09-18-36/300-ema/mp_rank_00_model_states.pt
[2024-09-09 20:14:53,137] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora/lora-disney-09-09-18-36/300-ema/mp_rank_00_model_states.pt...
[2024-09-09 20:15:49,080] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora/lora-disney-09-09-18-36/300-ema/mp_rank_00_model_states.pt.
##############################  Sampling setting  ##############################
Sampler: VPSDEDPMPP2MSampler
Discretization: ZeroSNRDDPMDiscretization
Guider: DynamicCFG
Sampling with VPSDEDPMPP2MSampler for 51 steps:   0%|          | 0/51 [00:00<?, ?it/s]Sampling with VPSDEDPMPP2MSampler for 51 steps:   2%|â–         | 1/51 [00:01<01:32,  1.86s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:   4%|â–         | 2/51 [00:03<01:33,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:   6%|â–Œ         | 3/51 [00:05<01:31,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:   8%|â–Š         | 4/51 [00:07<01:29,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  10%|â–‰         | 5/51 [00:09<01:27,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  12%|â–ˆâ–        | 6/51 [00:11<01:25,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  14%|â–ˆâ–Ž        | 7/51 [00:13<01:24,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  16%|â–ˆâ–Œ        | 8/51 [00:15<01:22,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  18%|â–ˆâ–Š        | 9/51 [00:17<01:20,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  20%|â–ˆâ–‰        | 10/51 [00:19<01:18,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  22%|â–ˆâ–ˆâ–       | 11/51 [00:21<01:16,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  24%|â–ˆâ–ˆâ–Ž       | 12/51 [00:22<01:14,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  25%|â–ˆâ–ˆâ–Œ       | 13/51 [00:24<01:12,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  27%|â–ˆâ–ˆâ–‹       | 14/51 [00:26<01:10,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  29%|â–ˆâ–ˆâ–‰       | 15/51 [00:28<01:08,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  31%|â–ˆâ–ˆâ–ˆâ–      | 16/51 [00:30<01:06,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 17/51 [00:32<01:05,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 18/51 [00:34<01:03,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 19/51 [00:36<01:01,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 20/51 [00:38<00:59,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 21/51 [00:40<00:57,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 22/51 [00:42<00:55,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 23/51 [00:43<00:53,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 24/51 [00:45<00:51,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 25/51 [00:47<00:49,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 26/51 [00:49<00:47,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 27/51 [00:51<00:45,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 28/51 [00:53<00:44,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 29/51 [00:55<00:42,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 30/51 [00:57<00:40,  1.92s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 31/51 [00:59<00:38,  1.92s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 32/51 [01:01<00:36,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33/51 [01:03<00:34,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 34/51 [01:05<00:32,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 35/51 [01:06<00:30,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 36/51 [01:08<00:28,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 37/51 [01:10<00:26,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/51 [01:12<00:24,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 39/51 [01:14<00:22,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 40/51 [01:16<00:21,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 41/51 [01:18<00:19,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 42/51 [01:20<00:17,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 43/51 [01:22<00:15,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 44/51 [01:24<00:13,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 45/51 [01:26<00:11,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 46/51 [01:27<00:09,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 47/51 [01:29<00:07,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 48/51 [01:31<00:05,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 49/51 [01:33<00:03,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 50/51 [01:35<00:01,  1.90s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 50/51 [01:35<00:01,  1.91s/it]
[2024-09-09 20:17:46,675] [INFO] [RANK 0] ----------------------------------------------------------------------------------------------------
[2024-09-09 20:17:46,675] [INFO] [RANK 0] -----------------------------------------------------------------------------------------------
[2024-09-09 20:17:46,683] [INFO] [RANK 0]  validation loss at iteration 300 | loss: 2.396625E-01 | PPL: 1.270820E+00 loss 2.396625E-01 |
[2024-09-09 20:17:46,684] [INFO] [RANK 0] -----------------------------------------------------------------------------------------------
wandb: WARNING (User provided step: 300 is less than current step: 301. Dropping entry: {'Train/valid_ppl': 1.2708201360111417, 'Train/valid_loss': 0.23966246843338013, '_timestamp': 1725884266.68442}).
[2024-09-09 20:23:51,101] [INFO] [RANK 0]  iteration      320/    1000 | elapsed time per iteration (ms): 30309.7 | learning rate 3.485E-04 | total loss 2.394571E-01 | loss 2.394571E-01 | loss scale 16777216.0 |speed 3.96 samples/(min*GPU)
[2024-09-09 20:23:51,102] [INFO] [RANK 0] time (ms) | forward: 12464.10 | backward: 5727.66 | allreduce: 0.00 | optimizer: 27.93 | data loader: 0.36
[2024-09-09 20:29:55,774] [INFO] [RANK 0]  iteration      340/    1000 | elapsed time per iteration (ms): 18233.7 | learning rate 3.385E-04 | total loss 1.996131E-01 | loss 1.996131E-01 | loss scale 16777216.0 |speed 6.58 samples/(min*GPU)
[2024-09-09 20:29:55,775] [INFO] [RANK 0] time (ms) | forward: 12476.34 | backward: 5727.49 | allreduce: 0.00 | optimizer: 28.63 | data loader: 0.36
[2024-09-09 20:32:58,146] [INFO] [logging.py:96:log_dist] [Rank 0] step=350, skipped=8, lr=[0.00033400000000000004], mom=[[0.9, 0.95]]
[2024-09-09 20:36:00,521] [INFO] [RANK 0]  iteration      360/    1000 | elapsed time per iteration (ms): 18237.3 | learning rate 3.285E-04 | total loss 2.171316E-01 | loss 2.171317E-01 | loss scale 16777216.0 |speed 6.58 samples/(min*GPU)
[2024-09-09 20:36:00,522] [INFO] [RANK 0] time (ms) | forward: 12479.63 | backward: 5728.89 | allreduce: 0.00 | optimizer: 27.61 | data loader: 0.33
[2024-09-09 20:42:27,897] [INFO] [RANK 0]  iteration      380/    1000 | elapsed time per iteration (ms): 19368.8 | learning rate 3.185E-04 | total loss 2.263744E-01 | loss 2.263744E-01 | loss scale 16777216.0 |speed 6.20 samples/(min*GPU)
[2024-09-09 20:42:27,899] [INFO] [RANK 0] time (ms) | forward: 12973.74 | backward: 6363.01 | allreduce: 0.00 | optimizer: 30.77 | data loader: 0.38
[2024-09-09 20:48:43,568] [INFO] [logging.py:96:log_dist] [Rank 0] step=400, skipped=8, lr=[0.00030900000000000003], mom=[[0.9, 0.95]]
[2024-09-09 20:48:43,569] [INFO] [RANK 0]  iteration      400/    1000 | elapsed time per iteration (ms): 18783.6 | learning rate 3.085E-04 | total loss 2.410881E-01 | loss 2.410881E-01 | loss scale 16777216.0 |speed 6.39 samples/(min*GPU)
[2024-09-09 20:48:43,571] [INFO] [RANK 0] time (ms) | forward: 12782.00 | backward: 5971.42 | allreduce: 0.00 | optimizer: 28.54 | data loader: 0.33
[2024-09-09 20:48:43,571] [INFO] [RANK 0] Saving Model...
[2024-09-09 20:48:49,923] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora/lora-disney-09-09-18-36/400/mp_rank_00_model_states.pt
[2024-09-09 20:48:49,923] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora/lora-disney-09-09-18-36/400/mp_rank_00_model_states.pt...
[2024-09-09 20:49:43,797] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora/lora-disney-09-09-18-36/400/mp_rank_00_model_states.pt.
[2024-09-09 20:49:44,223] [INFO] [RANK 0] Saving Ema Model...
[2024-09-09 20:49:50,332] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora/lora-disney-09-09-18-36/400-ema/mp_rank_00_model_states.pt
[2024-09-09 20:49:50,332] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora/lora-disney-09-09-18-36/400-ema/mp_rank_00_model_states.pt...
[2024-09-09 20:50:48,837] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /mnt/ceph/develop/jiawei/model_checkpoint/ckpts_2b_lora/lora-disney-09-09-18-36/400-ema/mp_rank_00_model_states.pt.
##############################  Sampling setting  ##############################
Sampler: VPSDEDPMPP2MSampler
Discretization: ZeroSNRDDPMDiscretization
Guider: DynamicCFG
Sampling with VPSDEDPMPP2MSampler for 51 steps:   0%|          | 0/51 [00:00<?, ?it/s]Sampling with VPSDEDPMPP2MSampler for 51 steps:   2%|â–         | 1/51 [00:01<01:32,  1.86s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:   4%|â–         | 2/51 [00:03<01:33,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:   6%|â–Œ         | 3/51 [00:05<01:31,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:   8%|â–Š         | 4/51 [00:07<01:29,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  10%|â–‰         | 5/51 [00:09<01:27,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  12%|â–ˆâ–        | 6/51 [00:11<01:25,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  14%|â–ˆâ–Ž        | 7/51 [00:13<01:24,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  16%|â–ˆâ–Œ        | 8/51 [00:15<01:22,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  18%|â–ˆâ–Š        | 9/51 [00:17<01:20,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  20%|â–ˆâ–‰        | 10/51 [00:19<01:18,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  22%|â–ˆâ–ˆâ–       | 11/51 [00:21<01:16,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  24%|â–ˆâ–ˆâ–Ž       | 12/51 [00:22<01:14,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  25%|â–ˆâ–ˆâ–Œ       | 13/51 [00:24<01:12,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  27%|â–ˆâ–ˆâ–‹       | 14/51 [00:26<01:10,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  29%|â–ˆâ–ˆâ–‰       | 15/51 [00:28<01:08,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  31%|â–ˆâ–ˆâ–ˆâ–      | 16/51 [00:30<01:06,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 17/51 [00:32<01:05,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 18/51 [00:34<01:03,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 19/51 [00:36<01:01,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 20/51 [00:38<00:59,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 21/51 [00:40<00:57,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 22/51 [00:42<00:55,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 23/51 [00:43<00:53,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 24/51 [00:45<00:51,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 25/51 [00:47<00:49,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 26/51 [00:49<00:47,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 27/51 [00:51<00:45,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 28/51 [00:53<00:44,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 29/51 [00:55<00:42,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 30/51 [00:57<00:40,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 31/51 [00:59<00:38,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 32/51 [01:01<00:36,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33/51 [01:03<00:34,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 34/51 [01:05<00:32,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 35/51 [01:06<00:30,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 36/51 [01:08<00:28,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 37/51 [01:10<00:26,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/51 [01:12<00:24,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 39/51 [01:14<00:22,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 40/51 [01:16<00:21,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 41/51 [01:18<00:19,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 42/51 [01:20<00:17,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 43/51 [01:22<00:15,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 44/51 [01:24<00:13,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 45/51 [01:26<00:11,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 46/51 [01:27<00:09,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 47/51 [01:29<00:07,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 48/51 [01:31<00:05,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 49/51 [01:33<00:03,  1.91s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 50/51 [01:35<00:01,  1.90s/it]Sampling with VPSDEDPMPP2MSampler for 51 steps:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 50/51 [01:35<00:01,  1.91s/it]
[2024-09-09 20:52:46,321] [INFO] [RANK 0] ----------------------------------------------------------------------------------------------------
[2024-09-09 20:52:46,321] [INFO] [RANK 0] -----------------------------------------------------------------------------------------------
[2024-09-09 20:52:46,327] [INFO] [RANK 0]  validation loss at iteration 400 | loss: 1.466303E-01 | PPL: 1.157926E+00 loss 1.466303E-01 |
[2024-09-09 20:52:46,327] [INFO] [RANK 0] -----------------------------------------------------------------------------------------------
wandb: WARNING (User provided step: 400 is less than current step: 401. Dropping entry: {'Train/valid_ppl': 1.1579257838650594, 'Train/valid_loss': 0.14663028717041016, '_timestamp': 1725886366.3282437}).
W0909 20:55:54.009000 139804893615936 torch/distributed/elastic/agent/server/api.py:688] Received Signals.SIGHUP death signal, shutting down workers
W0909 20:55:54.010000 139804893615936 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2055321 closing signal SIGHUP
Traceback (most recent call last):
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 680, in run
    result = self._invoke_run(role)
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 835, in _invoke_run
    time.sleep(monitor_interval)
  File "/mnt/ceph/develop/jiawei/conda_env/cogvideo/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 79, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2055161 got signal: 1
DONE on nm04-a800-node083
